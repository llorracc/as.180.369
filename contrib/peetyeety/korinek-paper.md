# Reflections on Korinek's AI Productivity for Economists

After reading Korinek's paper on AI productivity for economists, I found several of his ideas both interesting and relatable. His work demonstrates how AI agents—autonomous systems based on large language models—are transforming the landscape of economic research by automating complex, multi-step tasks that traditionally consumed substantial researcher time and cognitive resources.

## Human Shift from Generators to Evaluators

One key insight that resonated with me is the notion that humans are shifting from generators to evaluators. Since ChatGPT's emergence in late 2022, I've been gradually trained to assess its performance when reviewing its output for my own use. This evaluative skill seems to be acquired primarily through hands-on usage, which suggests that to excel in today's environment, everyone needs substantial exposure to AI tools. As AI increasingly generates content, our primary role may become that of curators and assessors of quality rather than original creators. 

This shift is fundamental to understanding how research workflows will evolve—we must become adept at judging AI-generated literature reviews, econometric code, and data analyses rather than producing everything from scratch ourselves. Yet this transformation raises critical questions about the nature of expertise itself. When I evaluate AI output, what exactly am I evaluating against? My own prior knowledge, accumulated through the very generative process that AI now threatens to bypass. This creates a potential paradox: if the next generation of economists primarily learns through evaluation rather than generation, will they develop the deep tacit knowledge necessary to be effective evaluators? 

The evaluative stance also demands a different cognitive skill set than generative work. Where generation requires wrestling with blank pages and building arguments from first principles, evaluation requires pattern recognition, error detection, and the ability to distinguish plausible-sounding but incorrect claims from genuinely insightful analysis. In economics specifically, this means recognizing when an AI-generated model specification is theoretically sound versus when it merely mimics the surface features of econometric practice. The question becomes: can we develop robust evaluation capabilities without first building them on a foundation of generative struggle? Or put differently, what minimum threshold of generative experience is necessary before one can reliably evaluate AI outputs in economic research?

## Modularizing the Research Process

Korinek's breakdown of the research paper process into small, seemingly trivial pieces that collectively consume substantial time was particularly insightful. He identifies specific tasks like conducting literature reviews, writing and debugging code, fetching economic data, and coordinating complex workflows—each individually manageable but collectively overwhelming. By identifying these discrete cognitive tasks, we can leverage AI to replace significant cognitive power and time resources that researchers traditionally expend. 

However, this modularization reveals something deeper about the nature of economic research that perhaps we've been reluctant to acknowledge: much of what economists spend time on is indeed modular and procedural rather than genuinely creative. Korinek's taxonomy implicitly challenges the romantic notion that research is primarily an act of inspired synthesis. Instead, it suggests that research productivity has long been bottlenecked by numerous time-consuming but cognitively routine tasks—fetching data from FRED, reformatting citations, debugging pandas syntax errors, summarizing the last decade of literature on a specific mechanism.

This raises an important question about what constitutes the actual intellectual contribution in economic research. If these modular tasks can be effectively automated, what remains? Korinek suggests we'll focus on "higher-level theoretical insights and creative problem-solving," but this optimistic framing may underestimate how much of our current sense of research intuition is built precisely through the grinding engagement with these "trivial" tasks. When I debug code, I often stumble upon data patterns I wouldn't have noticed otherwise. When I manually read through literature, I make unexpected connections between papers that keyword searches might miss. The modularization is efficient, but it may also inadvertently streamline away the productive friction that generates serendipitous insight. The risk is not that AI will fail to handle these modular tasks, but that in succeeding, it will compress the research process in ways that eliminate important forms of tacit learning and accidental discovery.

## AI as Cognitive Partner

Perhaps most interestingly, I found value in Korinek's framing of AI as "brains" rather than just calculators. This perspective shift suggests treating AI tools as cognitive partners capable of reasoning and generation, rather than merely computational aids. Korinek demonstrates this through his discussion of agentic frameworks and what he calls "natural language programming," where economists can create sophisticated research assistants without traditional coding expertise. 

This reframing from "calculator" to "brain" is more radical than it initially appears, because it fundamentally alters the attribution of intellectual work. When we use a calculator, there's no ambiguity about authorship—the tool performs a bounded computational task while the human provides all the intellectual framing. But when AI acts as a "brain," capable of reasoning through economic problems, generating hypotheses, and making methodological choices, the lines of intellectual contribution blur considerably. Korinek's "natural language programming" exemplifies this tension: I can now instruct an AI agent to "fetch relevant macroeconomic indicators and run a panel regression analyzing their relationship to labor market outcomes," and the agent will make dozens of implicit decisions—which variables to include, how to handle missing data, which fixed effects to add, how to cluster standard errors. Each of these choices reflects a form of economic judgment.

The collaborative framing that Korinek adopts—positioning this as partnership rather than replacement—is appealing but perhaps too comfortable. In practice, as AI systems become more capable cognitive partners, there's a risk of asymmetric collaboration where the human's role diminishes not through explicit replacement but through gradual atrophy of skills. If I can always delegate literature synthesis to an AI agent, will I maintain the capacity to critically read economic papers? If "natural language programming" allows me to create complex research pipelines without understanding Python or econometric software, what happens when the AI makes a subtle methodological error that I lack the technical depth to catch? The "partnership" metaphor suggests equal footing, but the actual dynamics may resemble an increasingly dependent relationship where the human provides high-level direction while losing the capacity for deep technical engagement. This is perhaps the most unsettling aspect of Korinek's vision: not that AI will replace economists, but that it will transform us into managers of cognitive processes we no longer fully understand.

## Applying These Reflections to My Own Research: The Film Tax Credit Methodology

My current research on California's film tax credit expansions provides a concrete case study for examining these tensions between AI-assisted and traditional research approaches. The methodology draft I developed with AI assistance (documented in `methodology-draft.ipynb`) illustrates both the productive capabilities and concerning implications that Korinek identifies.

### The Modularization in Practice

The film tax credit project required exactly the kind of modular tasks Korinek describes: constructing panel datasets from BLS QCEW files, writing difference-in-differences specifications, generating parallel trends visualizations, and synthesizing four disparate literatures (Thom 2018, Rickman & Wang 2020, Bradbury 2020, Owens & Rennhoff 2024). Each task is procedurally well-defined—load CSV files, merge on state-quarter identifiers, calculate quarterly employment averages, specify two-way fixed effects models—yet collectively they represent hundreds of hours of traditional research labor.

AI assistance compressed this timeline dramatically. Within minutes, I had working Python code that properly constructed a state-quarter panel, created treatment indicators for California's 2015 expansion, and estimated three progressive DiD specifications (basic, time fixed effects, two-way fixed effects). The code included appropriate logging transformations, handled the timing of AB 1839's Q2 2015 implementation, and even generated publication-quality parallel trends visualizations. This is precisely the efficiency gain Korinek celebrates.

But this acceleration raises the serendipity concern Korinek identifies. When I manually explored the 2013-2018 QCEW data in my initial simple-exploratory notebook, I noticed something unexpected: Georgia's employment trajectory diverged sharply from California and New York around 2016—well after California's 2015 expansion. This observation wasn't part of my original research plan, but it prompted me to investigate Georgia's competing tax credit program and ultimately influenced my control state selection criteria. Would AI-generated code, optimized for my stated research question, have surfaced this pattern? Or would efficient modularization have streamlined away this productive friction?

### The Generator-to-Evaluator Shift in Methodology Design

The methodology draft demonstrates the evaluator role Korinek describes. When AI proposed the synthetic control section (Part 4), it correctly specified the optimization problem, identified appropriate donor pool exclusions (Georgia, Louisiana, New Mexico due to policy changes), and outlined permutation inference procedures. This output is technically sound—any applied econometrician would recognize it as proper synthetic control methodology following Abadie & Gardeazabal (2003).

Yet my evaluation relied entirely on knowledge I accumulated through previous generative struggle. I recognized that the donor pool exclusions were correct because I had previously read state-by-state film incentive program timelines. I understood why permutation tests matter for inference because I once manually coded placebo synthetic controls and saw firsthand how pre-treatment fit quality affects post-treatment gap interpretation. The AI produced methodologically correct specifications, but I could only evaluate their correctness because of prior hands-on experience that AI now threatens to make unnecessary.

This creates Korinek's paradox acutely: if the next cohort of graduate students can have AI generate their entire empirical strategy—complete with proper specifications, appropriate robustness checks, and theoretically grounded interpretations—will they develop the tacit knowledge necessary to catch subtle errors? When the AI proposed clustering standard errors "at the state level" for my DiD regressions, I recognized this was correct for my 4-state panel but would be inadequate if I expanded to more states, potentially requiring wild cluster bootstrap (Cameron et al. 2008). But this recognition came from previous painful experience with small-cluster inference problems. Without that generative foundation, would I even know what to evaluate?

### The Cognitive Partnership's Asymmetry

The most unsettling aspect appears in Part 5 of my methodology draft—the ACS migration validation analysis. This represents my claimed novel contribution: using American Community Survey residential migration data to validate whether QCEW employment gains reflect genuine worker relocation versus administrative reclassification. The idea originated from my own theoretical insight about the measurement problem. But the implementation strategy—which SOC occupation codes to use (27-2012, 27-4031, 27-4032), how to structure the validation logic table, how to design placebo tests with other professional occupations—emerged through dialogue with AI.

Is this collaboration or dependency? Korinek's "cognitive partner" framing suggests equal intellectual contribution, but the actual relationship feels more asymmetric. I provided high-level conceptual direction ("I want to distinguish real job creation from statistical reclassification"), and the AI operationalized this into concrete empirical strategies with specific data sources, regression specifications, and interpretation frameworks. The AI made dozens of implicit methodological choices: using in-migration flows rather than net migration, focusing on occupation-specific patterns, comparing to professional occupation placebos rather than total migration. Each choice reflects economic judgment.

I evaluated these choices as appropriate, but could I have generated them myself? Probably, given enough time and literature review. But Korinek's concern is precisely this: as I increasingly rely on AI to operationalize my conceptual insights, do I gradually lose the capacity for that operationalization? If I can always delegate the translation from idea to implementation, will I maintain the technical depth to recognize when the translation introduces subtle errors?

### The Evaluation Paradox: Meta-Cognitive Blind Spots

Perhaps most troubling is what Korinek doesn't explicitly address: the problem of evaluating what we cannot generate. My methodology draft includes formal mathematical specifications for the DiD model, event study framework, and synthetic control optimization problem. These equations look correct—they match the textbook formulations I learned in econometrics courses. But if an AI had generated a *slightly different* specification—say, clustering standard errors at the state-quarter level rather than just state level, or using different weights in the synthetic control optimization—would I catch the error? Or would the plausible-sounding justification the AI provides ("clustering at state-quarter level accounts for both spatial and temporal correlation") persuade me despite being subtly wrong?

The evaluative stance Korinek describes assumes we can reliably distinguish correct from plausible-but-incorrect AI outputs. But evaluation is itself a learned skill, built on a foundation of generative experience. When evaluation becomes our primary mode before we've developed that foundation, we risk systematic blind spots: errors that are sophisticated enough to pass surface evaluation but wrong in ways we lack the generative experience to detect.

### Concluding Tension: Productive but Concerning

My film tax credit research would not exist in its current form without AI assistance. The methodology draft, with its integration of four distinct literatures, multi-method empirical strategy, and novel ACS validation approach, represents a scope of work that would have taken weeks or months to develop manually. AI compressed this to hours, enabling research ambition that would otherwise be infeasible.

Yet Korinek's concerns manifest throughout. The modularization that made this efficiency possible also eliminated much of the exploratory meandering that yields unexpected insights. My role as evaluator relies on generative experience I'm fortunate to have accumulated before AI's emergence, but I cannot be confident that evaluation alone would build that tacit knowledge. And the cognitive partnership, while productive, creates subtle dependencies I'm increasingly aware of but uncertain how to address.

This isn't an argument against AI assistance in economic research—the efficiency gains are too substantial and Korinek's optimism too compelling. But my concrete experience validates his cautions. We are indeed shifting from generators to evaluators, research is indeed being modularized, and AI is indeed becoming a cognitive partner rather than mere calculator. Whether this transformation produces better economists or merely more productive research managers remains an open question that my own experience cannot yet resolve.
