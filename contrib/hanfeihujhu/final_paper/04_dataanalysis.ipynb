{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e18e32d",
   "metadata": {},
   "source": [
    "# Data and Empirical Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2066cea-7c72-4d86-9c94-1c805d070f53",
   "metadata": {},
   "source": [
    "## Data Sources and Sample Construction\n",
    "To construct a continuous panel of bank balance sheets spanning 1984 to 2019, we had to reconcile a major structural break in Federal Reserve reporting formats. Prior to 2001, FFIEC Call Report data were distributed as SAS Transport (XPT) files with non-standard date indexing and cryptic variable codes. Post-2001, the format shifted to flat text/CSV files with standardized naming conventions. We developed a dual-pipeline ingestion strategy to harmonize these eras. A custom Python parser processed the legacy XPT files, recovering reporting dates from filename metadata (e.g., `call8403.xpt`), while a separate recursive scraper aggregated the modern directory structures. These streams were standardized into a unified schema (matching `RCFD` and `RCON` prefixes) and concatenated to form a seamless 140-quarter time series. (See Appendix for technical details and replication code).\n",
    "\n",
    "![Alt text](graphs/datatransformation.png)\n",
    "\n",
    "We applied strict filtering criteria to ensure panel stability. We excluded entities with fewer than 8 quarters of continuous data to mitigate noise from short-lived institutions and to ensure sufficient observations for valid dynamic lag construction. The final dataset contains approximately 740,000 bank-quarter observations.\n",
    "\n",
    "\n",
    "To measure the stance of monetary policy communication, we construct a sentiment index based on the full corpus of Federal Open Market Committee (FOMC) transcripts. We collected the raw transcript data directly from the **Federal Reserve Board of Governors'** historical archives. Using a custom web scraper, we retrieved the meeting transcripts for every FOMC meeting from 1976 to 2019. Unlike the concise \"Post-Meeting Statements,\" transcripts provide a verbatim record of the deliberations, offering a richer dataset for sentiment extraction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed2849",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "We employ a dictionary-based textual analysis using the Loughran-McDonald (2011) financial sentiment lexicon, implemented via the `pysentiment2` Python library. For each meeting $t$, we tokenize the full transcript and calculate a \"Net Tone\" score based on the relative frequency of positive and negative financial terms. The polarity score is defined as:\n",
    "\n",
    "$$\n",
    "NetSentiment_t = \\frac{Positive_{t} - Negative_{t}}{Positive_{t} + Negative_{t}}\n",
    "$$\n",
    "\n",
    "To isolate the \"Pure Sentiment\" channel, we must distinguish between communication and policy action. We retrieve the Daily Effective Federal Funds Rate (Series: DFF) directly from the Federal Reserve Bank of St. Louis (FRED) API. We then orthogonalize the sentiment score against the effective rate using the regression described below. The resulting residual represents the \"Sentiment Shock\"â€”variation in Fed communication that cannot be explained by the current interest rate level. Finally, we align these shocks with bank reporting periods using the \"Late-Quarter Shift\" protocol (see Appendix B) to ensure banks had access to the information prior to filing their Call Reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a431756-9fc5-4d47-a17b-60f33aec00aa",
   "metadata": {},
   "source": [
    "# Econometric Strategy\n",
    "\n",
    "## Model Specification\n",
    "To identify the causal effect of sentiment on risk-taking, we estimate a dynamic panel model including bank-fixed effects. While dynamic panels with fixed effects can suffer from Nickell bias (Nickell, 1981) in short time frames, our sample covers a long time dimension ($T = 92$ quarters). As the bias is of order $O(1/T)$, it is negligible in our context (Judson & Owen, 1999). We therefore employ a standard Fixed Effects (Within) estimator rather than Difference-GMM or Anderson-Hsiao, allowing for more efficient use of the data.\n",
    "\n",
    "The primary specification is as follows:\n",
    "\n",
    "$$\n",
    "Risk_{i,t} = \\alpha_i + \\rho Risk_{i,t-1} + \\beta_1 Shock_{t-1} + \\beta_2 (Shock_{t-1} \\times Size_{i,t-1}) + \\beta_3 Controls_{i,t-1} + \\epsilon_{i,t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $Risk_{i,t}$ is the Loans-to-Assets ratio.\n",
    "* $\\alpha_i$ represents bank-specific fixed effects (time-invariant heterogeneity).\n",
    "* $Shock_{t-1}$ is the \"Purged\" Sentiment Shock, standardized to unit variance.\n",
    "* $Size_{i,t-1}$ is the natural log of total assets, centered at the sample mean to facilitate interpretation of the main effect.\n",
    "\n",
    "We compute standard errors clustered by both **Entity (Bank)** and **Time (Date)** to account for serial correlation within banks and common shocks affecting all banks simultaneously.\n",
    "\n",
    "Regression code can be found in Appendix C\n",
    "\n",
    "![risk taking pre post 2008](graphs/prepost2008.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066cc9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econark",
   "language": "python",
   "name": "econark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
