{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1114c872-732e-4b0e-a1cb-5b69e35458d1",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96c0f8-5df0-4ad2-9cd2-e14bc267c6bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix A: Data Construction & Harmonization\n",
    "\n",
    "### A.1 The \"Dual-Pipeline\" Ingestion Strategy\n",
    "A significant barrier to longitudinal analysis of US banking data is the format change that occurred in 2001. Standard merging procedures often fail to bridge this gap, resulting in \"short panels\" that drop the 1980s and 90s. To resolve this, we employed two distinct extraction algorithms:\n",
    "\n",
    "1.  **The Legacy Pipeline (1984–2000):** This pipeline handles SAS Transport (`.xpt`) files. The primary challenge in this era is inconsistent date recording; dates are often omitted from the file content or stored in non-standard \"days since 1960\" formats. Our algorithm recovers the valid reporting quarter by parsing the original filenames (e.g., identifying `8206` as June 1982) and cross-validating with internal file metadata.\n",
    "    \n",
    "2.  **The Modern Pipeline (2001–2019):** This pipeline handles the modern bulk repository, which is organized into nested directories of variable-delimited text files. The algorithm recursively scans these directories, extracts the `Schedule RC` (Balance Sheet) and `Schedule RI` (Income Statement) files, and merges them on the unique bank identifier (`IDRSSD`).\n",
    "\n",
    "### A.2 Variable Harmonization\n",
    "Both pipelines standardize variables to a common dictionary before merging. Specifically, we prioritize \"Consolidated Bank\" reported values (Prefix `RCFD`) and use \"Domestic Office\" values (Prefix `RCON`) as a fallback for smaller banks that do not report consolidated figures. This ensures that the definition of \"Total Assets\" and \"Commercial & Industrial Loans\" remains consistent across the 35-year sample.\n",
    "\n",
    "---\n",
    "\n",
    "# Appendix B: Replication Code\n",
    "\n",
    "To replicate the bank dataset construction, place the raw FFIEC data folders in the root directory and execute the scripts in the following order.\n",
    "\n",
    "**Step 1: Process Legacy Data (1984–2000)**\n",
    "Run `old_bank_data.py` to parse the SAS Transport files.\n",
    "* **Input:** Raw `.xpt` files (recursive search).\n",
    "* **Output:** `OLD_RAW_BANK_DATA.csv`\n",
    "\n",
    "**Step 2: Process Modern Data (2001–2019)**\n",
    "Run `new_bank_scrape.py` to parse the modern text repositories.\n",
    "* **Input:** FFIEC folders containing `Schedule RC.txt` and `Schedule CI.txt`.\n",
    "* **Output:** `NEW_RAW_BANK_DATA.csv`\n",
    "\n",
    "**Step 3: Merge**\n",
    "Run `merge_bank.py` Combine the two outputs to form final file `ALL_BANKS_MERGED.py`\n",
    "\n",
    "---\n",
    "\n",
    "To replicate the macro dataset construction, place the raw excel spreadsheet in the same directory the following scripts and execute them in the following order.\n",
    "\n",
    "**Step 1: Clean Transcripts**\n",
    "Run `fomc_data.py` to parse the transcripts.\n",
    "* **Input:** transcript `.xlsx` file.\n",
    "* **Output:** `fomc_data` folder, containing text files with transcripts of each date.\n",
    "\n",
    "**Step 2: Calculate Sentiment**\n",
    "Run `sentiment.py` to parse the txt files in the previously created fomc_data file.\n",
    "* **Input:** fomc_data folder containing `meeting_yyyy-mm-dd.txt`.\n",
    "* **Output:** `fomc_sentiment_data.csv`\n",
    "\n",
    "**Step 3: Scrape Effective Funds Rate**\n",
    "Run `rate.py` to pull latest fomc data from the St. Louis Fed.\n",
    "* **Input:** N/A\n",
    "* **Output:** `fed_funds.csv`\n",
    "\n",
    "---\n",
    "\n",
    "**Merge Data Into Final Dataset**\n",
    "Run `merge.py` to pull latest fomc data from the St. Louis Fed.\n",
    "* **Input:** `fed_funds.csv`, `ALL_BANKS_MERGED.csv`, `fomc_sentiment_data.csv`\n",
    "* **Output:** `MACRO_DATA_CLEAN.csv`, `master_merged_dataset.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dcc0d2",
   "metadata": {},
   "source": [
    "### B.1 Legacy Parser (`old_bank_data.py`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c82270-f1f8-49eb-8bca-b679a4e16aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pandas.tseries.offsets import QuarterEnd\n",
    "\n",
    "\n",
    "def build_xpt_dataset_smart():\n",
    "    print(\"--- STARTING OLD ERA (XPT) DATA BUILDER V2 (CORRECTED) ---\")\n",
    "\n",
    "    # 1. Define SAS Variable Names (Old Era)\n",
    "    # RSSD9001: Entity ID\n",
    "    # RSSD9999: Report Date (Standard internal date variable)\n",
    "\n",
    "    # ASSETS:\n",
    "    # RCFD2170: Total Assets (Consolidated)\n",
    "    # RCON2170: Total Assets (Domestic)\n",
    "\n",
    "    # LOANS:\n",
    "    # RCFD1763: Total C&I Loans (Consolidated) - CRITICAL for large banks\n",
    "    # RCFD1766: C&I Loans to U.S. Addressees (Consolidated)\n",
    "    # RCON1766: C&I Loans (Domestic)\n",
    "\n",
    "    id_col = 'RSSD9001'\n",
    "    date_col_internal = 'RSSD9999'\n",
    "\n",
    "    # We define the columns we WANT to find.\n",
    "    # Note: We don't filter strictly on read because read_sas reads the whole file anyway.\n",
    "    target_cols = [\n",
    "        'RSSD9001', 'RSSD9999',\n",
    "        'RCFD2170', 'RCON2170',\n",
    "        'RCFD1763', 'RCFD1766', 'RCON1766'\n",
    "    ]\n",
    "\n",
    "    root_dir = \".\"\n",
    "    xpt_files = glob.glob(os.path.join(\n",
    "        root_dir, \"**\", \"*.xpt\"), recursive=True)\n",
    "\n",
    "    print(f\"Found {len(xpt_files)} .xpt files. Processing...\")\n",
    "\n",
    "    master_list = []\n",
    "\n",
    "    for file_path in xpt_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        date_obj = None\n",
    "\n",
    "        # --- STRATEGY 1: PARSE FILENAME (YYMM) ---\n",
    "        match = re.search(\n",
    "            r'([0-9]{2})([0-9]{2})\\.xpt$', filename, re.IGNORECASE)\n",
    "\n",
    "        if match:\n",
    "            yy = int(match.group(1))\n",
    "            mm = int(match.group(2))\n",
    "\n",
    "            # Year Pivot Logic (Window: 1950-2049)\n",
    "            if yy > 50:\n",
    "                full_year = 1900 + yy\n",
    "            else:\n",
    "                full_year = 2000 + yy\n",
    "\n",
    "            try:\n",
    "                # Snap to QuarterEnd\n",
    "                date_obj = pd.Timestamp(\n",
    "                    year=full_year, month=mm, day=1) + QuarterEnd(0)\n",
    "            except:\n",
    "                print(f\"  Warning: Invalid date in filename {filename}\")\n",
    "\n",
    "        # --- STRATEGY 2: READ AND EXTRACT ---\n",
    "        try:\n",
    "            # Read SAS file\n",
    "            df = pd.read_sas(file_path, format='xport')\n",
    "\n",
    "            # Convert columns to Uppercase\n",
    "            df.columns = [c.upper() for c in df.columns]\n",
    "\n",
    "            # If date not found in filename, try internal SAS date\n",
    "            if date_obj is None:\n",
    "                if date_col_internal in df.columns:\n",
    "                    # SAS Dates are days since 1960-01-01\n",
    "                    sas_date_val = df[date_col_internal].mode()[0]\n",
    "                    date_obj = pd.to_datetime(\n",
    "                        '1960-01-01') + pd.to_timedelta(sas_date_val, unit='D')\n",
    "                    print(\n",
    "                        f\"  Extracted internal date {date_obj.date()} from {filename}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {filename}: Could not determine date.\")\n",
    "                    continue\n",
    "\n",
    "            # Standardize ID\n",
    "            if id_col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            df[id_col] = pd.to_numeric(df[id_col], errors='coerce')\n",
    "            df.rename(columns={id_col: 'bank_id'}, inplace=True)\n",
    "\n",
    "            # --- STANDARDIZE ASSETS ---\n",
    "            # Priority: RCFD (Consolidated) > RCON (Domestic)\n",
    "            # Create a base series of NaNs\n",
    "            df['assets'] = pd.NA\n",
    "\n",
    "            if 'RCON2170' in df.columns:\n",
    "                df['assets'] = df['RCON2170']\n",
    "\n",
    "            if 'RCFD2170' in df.columns:\n",
    "                # combine_first: if 'RCFD' is present, use it; else keep 'RCON'\n",
    "                df['assets'] = df['RCFD2170'].combine_first(df['assets'])\n",
    "\n",
    "            # --- STANDARDIZE C&I LOANS ---\n",
    "            # Priority: RCFD1763 (Total Global) > RCFD1766 (US Global) > RCON1766 (Domestic)\n",
    "\n",
    "            # Start with Domestic as baseline\n",
    "            loan_series = df.get('RCON1766', pd.Series([pd.NA]*len(df)))\n",
    "\n",
    "            # Update with US Global if available (fills gaps or overwrites depending on preference,\n",
    "            # here we want RCFD to supersede RCON if the bank reports RCFD)\n",
    "            if 'RCFD1766' in df.columns:\n",
    "                loan_series = df['RCFD1766'].combine_first(loan_series)\n",
    "\n",
    "            # Update with Total Global (Best Metric)\n",
    "            if 'RCFD1763' in df.columns:\n",
    "                loan_series = df['RCFD1763'].combine_first(loan_series)\n",
    "\n",
    "            df['ci_loans'] = loan_series\n",
    "\n",
    "            # --- FINAL DATA PREP ---\n",
    "            df['date'] = date_obj\n",
    "\n",
    "            # Keep only valid data columns\n",
    "            df = df[['bank_id', 'date', 'assets', 'ci_loans']]\n",
    "            df = df.dropna(subset=['bank_id'])\n",
    "\n",
    "            master_list.append(df)\n",
    "            print(\n",
    "                f\"Processed {filename}: {len(df)} banks. Date: {date_obj.date()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {filename}: {e}\")\n",
    "\n",
    "    # --- COMPILE ---\n",
    "    if not master_list:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Compiling Old Era Master Dataset...\")\n",
    "    full_old_df = pd.concat(master_list, ignore_index=True)\n",
    "\n",
    "    # Final Polish\n",
    "    full_old_df['assets'] = pd.to_numeric(\n",
    "        full_old_df['assets'], errors='coerce')\n",
    "    full_old_df['ci_loans'] = pd.to_numeric(\n",
    "        full_old_df['ci_loans'], errors='coerce')\n",
    "\n",
    "    # Remove rows with 0 assets to avoid DivideByZero\n",
    "    full_old_df = full_old_df[full_old_df['assets'] > 0]\n",
    "\n",
    "    # Calculate Risk Taking\n",
    "    full_old_df['risk_taking'] = full_old_df['ci_loans'] / \\\n",
    "        full_old_df['assets']\n",
    "\n",
    "    # Final cleanup of NaNs created by risk calc\n",
    "    full_old_df = full_old_df.dropna(subset=['risk_taking'])\n",
    "\n",
    "    output_filename = 'OLD_RAW_BANK_DATA.csv'\n",
    "    full_old_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nSUCCESS! Processed {len(full_old_df)} rows from Old Era.\")\n",
    "    print(f\"Saved to: {output_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_xpt_dataset_smart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45173e",
   "metadata": {},
   "source": [
    "### B.2 Modern Parser (`new_bank_data.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "def build_bank_dataset_recursive():\n",
    "    print(\"--- STARTING BANK DATA BUILDER ---\")\n",
    "\n",
    "    # 1. Setup Columns\n",
    "    # IDRSSD: Bank ID\n",
    "    # RCFD2170: Total Assets (Consolidated)\n",
    "    # RCON2170: Total Assets (Domestic - fallback)\n",
    "\n",
    "    # LOANS:\n",
    "    # RCFD1763: Total C&I Loans (Consolidated - Includes Foreign & Domestic)\n",
    "    # RCFD1766: C&I Loans to U.S. Addressees (Consolidated - Partial)\n",
    "    # RCON1766: C&I Loans (Domestic)\n",
    "\n",
    "    asset_cols = ['IDRSSD', 'RCFD2170', 'RCON2170']\n",
    "    # Added RCFD1763 for accurate Total C&I on global banks\n",
    "    loan_cols = ['IDRSSD', 'RCFD1763', 'RCFD1766', 'RCON1766']\n",
    "\n",
    "    all_quarters = []\n",
    "\n",
    "    root_dir = \".\"\n",
    "    subfolders = [f.path for f in os.scandir(root_dir) if f.is_dir()]\n",
    "\n",
    "    print(f\"Found {len(subfolders)} folders. Scanning for data files...\")\n",
    "\n",
    "    for folder in subfolders:\n",
    "        # 2. Find Schedule RC (Assets) and Schedule RCCI (Loans)\n",
    "        # CRITICAL FIX: The FFIEC file for loans is named \"Schedule RCCI\", not \"Schedule CI\"\n",
    "        rc_files = glob.glob(os.path.join(folder, \"*Schedule RC *.txt\"))\n",
    "        ci_files = glob.glob(os.path.join(folder, \"*Schedule RCCI *.txt\"))\n",
    "\n",
    "        if not rc_files or not ci_files:\n",
    "            # Silent skip or debug print if needed\n",
    "            continue\n",
    "\n",
    "        path_rc = rc_files[0]\n",
    "        path_ci = ci_files[0]\n",
    "\n",
    "        # Extract Date\n",
    "        date_match = re.search(r'(\\d{8})', os.path.basename(path_rc))\n",
    "        if not date_match:\n",
    "            print(f\"Skipping {folder}: Could not determine date.\")\n",
    "            continue\n",
    "\n",
    "        date_str = date_match.group(1)\n",
    "\n",
    "        try:\n",
    "            # 3. Read ASSETS (Schedule RC)\n",
    "            df_rc = pd.read_csv(path_rc, sep='\\t',\n",
    "                                skiprows=0, low_memory=False)\n",
    "            existing_asset_cols = [c for c in asset_cols if c in df_rc.columns]\n",
    "            df_rc = df_rc[existing_asset_cols]\n",
    "\n",
    "            # 4. Read LOANS (Schedule RC-C Part I)\n",
    "            df_ci = pd.read_csv(path_ci, sep='\\t',\n",
    "                                skiprows=0, low_memory=False)\n",
    "            existing_loan_cols = [c for c in loan_cols if c in df_ci.columns]\n",
    "            df_ci = df_ci[existing_loan_cols]\n",
    "\n",
    "            # 5. Merge\n",
    "            if 'IDRSSD' not in df_rc.columns or 'IDRSSD' not in df_ci.columns:\n",
    "                print(f\"Skipping {date_str}: Missing IDRSSD column.\")\n",
    "                continue\n",
    "\n",
    "            df_quarter = pd.merge(df_rc, df_ci, on='IDRSSD', how='inner')\n",
    "            df_quarter['date'] = pd.to_datetime(date_str, format='%m%d%Y')\n",
    "\n",
    "            # 6. Standardize Assets\n",
    "            if 'RCFD2170' in df_quarter.columns:\n",
    "                df_quarter['assets'] = df_quarter['RCFD2170'].fillna(\n",
    "                    df_quarter.get('RCON2170', 0))\n",
    "            elif 'RCON2170' in df_quarter.columns:\n",
    "                df_quarter['assets'] = df_quarter['RCON2170']\n",
    "            else:\n",
    "                df_quarter['assets'] = pd.NA\n",
    "\n",
    "            # 7. Standardize C&I Loans (Prioritize Total Global -> US Global -> Domestic)\n",
    "            # RCFD1763 = Total C&I (Consolidated)\n",
    "            # RCFD1766 = C&I to US Addressees (Consolidated)\n",
    "            # RCON1766 = Total C&I (Domestic)\n",
    "\n",
    "            df_quarter['ci_loans'] = 0  # Default\n",
    "\n",
    "            if 'RCFD1763' in df_quarter.columns:\n",
    "                # Primary for large banks\n",
    "                df_quarter['ci_loans'] = df_quarter['RCFD1763']\n",
    "\n",
    "            # Fill gaps where RCFD1763 might be missing but RCFD1766 exists\n",
    "            if 'RCFD1766' in df_quarter.columns:\n",
    "                df_quarter['ci_loans'] = df_quarter['ci_loans'].fillna(\n",
    "                    df_quarter['RCFD1766'])\n",
    "                # If we initialized with 0, fillna won't work on 0s, so we use logic:\n",
    "                # If 1763 was missing, column is 0 (or NaN if we didn't init).\n",
    "                # Better approach: Coalesce.\n",
    "\n",
    "            # Re-doing clean coalescing logic:\n",
    "            # Create a temporary series for the best available C&I data\n",
    "\n",
    "            # Start with Domestic (RCON1766) as baseline\n",
    "            temp_loans = df_quarter.get(\n",
    "                'RCON1766', pd.Series([pd.NA]*len(df_quarter)))\n",
    "\n",
    "            # Overwrite with RCFD1766 (US Addressees) if available\n",
    "            if 'RCFD1766' in df_quarter.columns:\n",
    "                temp_loans = df_quarter['RCFD1766'].combine_first(temp_loans)\n",
    "\n",
    "            # Overwrite with RCFD1763 (Total Consolidated) if available - Best Metric\n",
    "            if 'RCFD1763' in df_quarter.columns:\n",
    "                temp_loans = df_quarter['RCFD1763'].combine_first(temp_loans)\n",
    "\n",
    "            df_quarter['ci_loans'] = temp_loans\n",
    "\n",
    "            # 8. Clean up\n",
    "            df_quarter.rename(columns={'IDRSSD': 'bank_id'}, inplace=True)\n",
    "            df_quarter = df_quarter[['bank_id', 'date', 'assets', 'ci_loans']]\n",
    "\n",
    "            # Ensure numeric\n",
    "            df_quarter['assets'] = pd.to_numeric(\n",
    "                df_quarter['assets'], errors='coerce')\n",
    "            df_quarter['ci_loans'] = pd.to_numeric(\n",
    "                df_quarter['ci_loans'], errors='coerce')\n",
    "\n",
    "            all_quarters.append(df_quarter)\n",
    "            print(f\"Processed {date_str}: {len(df_quarter)} banks.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {date_str} in {folder}: {e}\")\n",
    "\n",
    "    # 9. Compile and Save\n",
    "    if not all_quarters:\n",
    "        print(\"CRITICAL: No data found. Ensure folders are named correctly (e.g. '03312008') and contain 'Schedule RC' and 'Schedule RCCI' files.\")\n",
    "        return\n",
    "\n",
    "    print(\"Compiling Master Dataset...\")\n",
    "    master_df = pd.concat(all_quarters, ignore_index=True)\n",
    "\n",
    "    # Calculate Risk Taking\n",
    "    master_df['risk_taking'] = master_df['ci_loans'] / master_df['assets']\n",
    "\n",
    "    # Filter\n",
    "    master_df = master_df.dropna(subset=['assets', 'risk_taking'])\n",
    "    master_df = master_df[master_df['assets'] > 0]\n",
    "\n",
    "    output_filename = 'NEW_RAW_BANK_DATA.csv'\n",
    "    master_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nSUCCESS! Built dataset with {len(master_df)} rows.\")\n",
    "    print(f\"Saved to: {output_filename}\")\n",
    "\n",
    "    # Diagnostics\n",
    "    count_2008 = len(master_df[master_df['date'].dt.year == 2008])\n",
    "    print(f\"Diagnostics: Rows found for 2008: {count_2008}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_bank_dataset_recursive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a822b4",
   "metadata": {},
   "source": [
    "### B.3 Bank Merge (`merge_bank.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24520f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def find_file(filename, search_path=\".\"):\n",
    "    \"\"\"Recursively searches for a file in the directory tree.\"\"\"\n",
    "    print(f\"  Searching for '{filename}'...\")\n",
    "    # Recursive glob search\n",
    "    matches = glob.glob(os.path.join(\n",
    "        search_path, \"**\", filename), recursive=True)\n",
    "    if matches:\n",
    "        print(f\"  -> Found at: {matches[0]}\")\n",
    "        return matches[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_smart_merge():\n",
    "    print(\"==================================================\")\n",
    "    print(\"       SMART BANK DATA MERGER                     \")\n",
    "    print(\"==================================================\")\n",
    "    print(f\"Current Working Directory: {os.getcwd()}\\n\")\n",
    "\n",
    "    # --- STEP 1: LOCATE FILES ---\n",
    "    print(\"Step 1: Locating Input Files...\")\n",
    "\n",
    "    path_new = find_file(\"NEW_RAW_BANK_DATA.csv\")\n",
    "    path_old = find_file(\"OLD_RAW_BANK_DATA.csv\")\n",
    "\n",
    "    if not path_new and not path_old:\n",
    "        print(\"\\nCRITICAL ERROR: Could not find ANY bank data files.\")\n",
    "        print(\"You must run the builder scripts first:\")\n",
    "        print(\"  1. Run 'build_bank_data.py' (for Modern data)\")\n",
    "        print(\"  2. Run 'build_old_data_v2.py' (for Old data)\")\n",
    "        return\n",
    "\n",
    "    # --- STEP 2: LOAD DATA ---\n",
    "    print(\"\\nStep 2: Loading Data...\")\n",
    "    dfs = []\n",
    "\n",
    "    if path_new:\n",
    "        try:\n",
    "            df_new = pd.read_csv(path_new)\n",
    "            print(f\"  -> Loaded Modern Data: {len(df_new):,} rows\")\n",
    "            dfs.append(df_new)\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error reading Modern Data: {e}\")\n",
    "\n",
    "    if path_old:\n",
    "        try:\n",
    "            df_old = pd.read_csv(path_old)\n",
    "            print(f\"  -> Loaded Old Data: {len(df_old):,} rows\")\n",
    "            dfs.append(df_old)\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error reading Old Data: {e}\")\n",
    "\n",
    "    # --- STEP 3: MERGE ---\n",
    "    print(\"\\nStep 3: Merging...\")\n",
    "    if not dfs:\n",
    "        print(\"  -> No data loaded. Exiting.\")\n",
    "        return\n",
    "\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Standardize Date\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "    # Deduplicate (Priority to Modern Data if overlaps exist)\n",
    "    merged_df = merged_df.sort_values(['date', 'bank_id'])\n",
    "    before_dedup = len(merged_df)\n",
    "    merged_df = merged_df.drop_duplicates(\n",
    "        subset=['bank_id', 'date'], keep='last')\n",
    "    print(\n",
    "        f\"  -> Deduplication removed {before_dedup - len(merged_df):,} duplicates.\")\n",
    "\n",
    "    # --- STEP 4: SAVE ---\n",
    "    output_filename = \"ALL_BANKS_MERGED.csv\"\n",
    "    merged_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"SUCCESS! Created '{output_filename}'\")\n",
    "    print(f\"Total Rows: {len(merged_df):,}\")\n",
    "    print(f\"Location: {os.path.abspath(output_filename)}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_smart_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fac177",
   "metadata": {},
   "source": [
    "### B.4 FOMC Data Retrieval (`data_fomc.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2966755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Download and prepare FOMC transcripts for sentiment analysis.\n",
    "\n",
    "- Source: Miguel Acosta, \"FOMC Communications Data\"\n",
    "  https://www.acostamiguel.com/data/fomc_data.html\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "TRANSCRIPTS_URL = \"https://www.acostamiguel.com/data/FOMC/transcripts.xlsx\"\n",
    "\n",
    "\n",
    "def download_file(url: str, dest: pathlib.Path, force: bool = False) -> pathlib.Path:\n",
    "    \"\"\"Download file with simple resume logic (skip if exists unless --force).\"\"\"\n",
    "    dest = pathlib.Path(dest)\n",
    "    if dest.exists() and not force:\n",
    "        logging.info(\n",
    "            \"File %s already exists; skipping download. Use --force to re-download.\",\n",
    "            dest,\n",
    "        )\n",
    "        return dest\n",
    "\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(\"Downloading %s -> %s\", url, dest)\n",
    "\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        tmp = dest.with_suffix(dest.suffix + \".part\")\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        tmp.replace(dest)\n",
    "\n",
    "    logging.info(\"Finished download.\")\n",
    "    return dest\n",
    "\n",
    "\n",
    "def split_meetings_to_txt(df: pd.DataFrame, out_dir: pathlib.Path) -> None:\n",
    "    \"\"\"Group utterances by meeting date and write one text file per meeting.\"\"\"\n",
    "    out_dir = pathlib.Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Choose a date column and normalize to datetime (handles raw YYYYMMDD ints).\n",
    "    date_col = \"meeting_date\" if \"meeting_date\" in df.columns else \"date\"\n",
    "    if date_col not in df.columns:\n",
    "        raise KeyError(\"Expected a 'date' or 'meeting_date' column in the transcript data.\")\n",
    "\n",
    "    dates = df[date_col]\n",
    "    if pd.api.types.is_integer_dtype(dates):\n",
    "        df[\"meeting_date\"] = pd.to_datetime(dates.astype(str), format=\"%Y%m%d\")\n",
    "    else:\n",
    "        df[\"meeting_date\"] = pd.to_datetime(dates)\n",
    "\n",
    "    # Ensure we have ordering columns for reproducible text output.\n",
    "    sort_cols = [c for c in [\"sequence\", \"n_utterance\"] if c in df.columns]\n",
    "\n",
    "    for date, g in df.groupby(\"meeting_date\"):\n",
    "        date_str = pd.to_datetime(date).strftime(\"%Y-%m-%d\")\n",
    "        g_sorted = g.sort_values(sort_cols)\n",
    "\n",
    "        text = \"\\n\\n\".join(g_sorted[\"text\"].astype(str))\n",
    "\n",
    "        out_path = out_dir / f\"meeting_{date_str}.txt\"\n",
    "        out_path.write_text(text, encoding=\"utf-8\")\n",
    "        logging.info(\"Wrote %s (%d utterances)\", out_path.name, len(g_sorted))\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Download and preprocess FOMC transcripts for sentiment analysis.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out-dir\",\n",
    "        default=\"fomc_data\",\n",
    "        help=\"Root output directory (default: %(default)s)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--force\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Force re-download of source files even if they exist.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--xlsx\",\n",
    "        help=\"Use an existing transcripts.xlsx instead of downloading.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--meetings-dir\",\n",
    "        default=\"meetings_txt_all\",\n",
    "        help=\"Subdirectory under processed/ for per-meeting text files (default: %(default)s).\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(argv)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    )\n",
    "\n",
    "    out_root = pathlib.Path(args.out_dir)\n",
    "    raw_dir = out_root / \"raw\"\n",
    "    processed_dir = out_root / \"processed\"\n",
    "\n",
    "    # 1. Download (or reuse) the transcripts Excel\n",
    "    if args.xlsx:\n",
    "        xlsx_path = pathlib.Path(args.xlsx)\n",
    "        if not xlsx_path.exists():\n",
    "            raise FileNotFoundError(f\"Provided --xlsx path does not exist: {xlsx_path}\")\n",
    "        logging.info(\"Using existing transcripts file at %s\", xlsx_path)\n",
    "    else:\n",
    "        xlsx_path = download_file(\n",
    "            TRANSCRIPTS_URL,\n",
    "            raw_dir / \"fomc_transcripts.xlsx\",\n",
    "            force=args.force,\n",
    "        )\n",
    "\n",
    "    # 2. Read into pandas and save a CSV version\n",
    "    logging.info(\"Reading %s\", xlsx_path)\n",
    "    df = pd.read_excel(xlsx_path)\n",
    "\n",
    "    tables_dir = processed_dir / \"tables\"\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    csv_path = tables_dir / \"fomc_transcripts.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logging.info(\"Saved full transcripts table to %s\", csv_path)\n",
    "\n",
    "    # 3. Write one text file per meeting\n",
    "    meetings_txt_dir = processed_dir / args.meetings_dir\n",
    "    split_meetings_to_txt(df, meetings_txt_dir)\n",
    "\n",
    "    logging.info(\"All done. Meeting-level text lives in %s\", meetings_txt_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad061e9e",
   "metadata": {},
   "source": [
    "### B.5 Sentiment Analysis (`sentiment.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pysentiment2 as ps\n",
    "\n",
    "# 1. Setup\n",
    "# Folder containing the text meetings. We try common locations so the script works\n",
    "# whether you generated files with fomc3.py (meetings_txt_all) or an earlier step (meetings_txt).\n",
    "candidate_dirs = [\n",
    "    'fomc_data/processed/meetings_txt_all',\n",
    "    'fomc_data/processed/meetings_txt',\n",
    "    # fallback to absolute path if used elsewhere\n",
    "    '/Users/hfh/Downloads/meetings_txt',\n",
    "]\n",
    "folder_path = next(\n",
    "    (d for d in candidate_dirs if os.path.isdir(d)), candidate_dirs[0])\n",
    "lm = ps.LM()  # Initialize the Financial Dictionary\n",
    "results = []\n",
    "\n",
    "# 2. The Loop\n",
    "# glob.glob grabs all .txt files in the folder\n",
    "file_list = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "if not file_list:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No .txt files found in '{folder_path}'. \"\n",
    "        \"Check the path or run fomc3.py to generate the meeting text files.\"\n",
    "    )\n",
    "\n",
    "print(f\"Found {len(file_list)} files. Starting analysis...\")\n",
    "\n",
    "for file_path in file_list:\n",
    "    try:\n",
    "        # Get filename (e.g., \"meeting_1976-03-29.txt\")\n",
    "        filename = os.path.basename(file_path)\n",
    "\n",
    "        # Extract Date from filename\n",
    "        # Assumes format \"meeting_YYYY-MM-DD.txt\"\n",
    "        date_str = filename.replace('meeting_', '').replace('.txt', '')\n",
    "\n",
    "        # Read the text file\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # 3. Analyze Sentiment\n",
    "        tokens = lm.tokenize(text)\n",
    "        score = lm.get_score(tokens)\n",
    "\n",
    "        # Calculate Net Sentiment: (Pos - Neg) / (Pos + Neg)\n",
    "        net_sentiment = score['Polarity']\n",
    "\n",
    "        # Store data\n",
    "        results.append({\n",
    "            'date': date_str,\n",
    "            'net_sentiment': net_sentiment,\n",
    "            'positive_count': score['Positive'],\n",
    "            'negative_count': score['Negative'],\n",
    "            'word_count': len(tokens),\n",
    "            'filename': filename\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "# 4. Save Results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Convert 'date' column to actual datetime objects for sorting/merging later\n",
    "if df.empty:\n",
    "    raise ValueError(\"No sentiment results produced; verify the input files.\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date')\n",
    "\n",
    "print(\"Analysis Complete.\")\n",
    "print(df.head())\n",
    "\n",
    "# Save to CSV for the next step of your paper\n",
    "output_csv = 'fomc_sentiment_data.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Saved results to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d89825",
   "metadata": {},
   "source": [
    "### B.6 Funds Rate Retrieval (`rate.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ec1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrape_fed_funds():\n",
    "    # URL for Daily Federal Funds Rate (DFF) from St. Louis Fed\n",
    "    # Use 'FEDFUNDS' instead of 'DFF' if you want monthly averages\n",
    "    url = \"https://fred.stlouisfed.org/graph/fredgraph.csv?id=DFF\"\n",
    "\n",
    "    print(f\"Downloading data from {url}...\")\n",
    "\n",
    "    try:\n",
    "        # Read CSV directly from the URL\n",
    "        df = pd.read_csv(url)\n",
    "\n",
    "        # Rename columns as requested\n",
    "        df.columns = ['date', 'rate']\n",
    "\n",
    "        # Convert date column to datetime objects for filtering\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # Filter for dates starting from 1982-01-01\n",
    "        df = df[df['date'] >= '1976-01-01']\n",
    "\n",
    "        # Save to CSV without the index number\n",
    "        output_file = 'fed_funds.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Success! Saved {len(df)} rows to {output_file}\")\n",
    "        print(f\"Range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_fed_funds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0de66d",
   "metadata": {},
   "source": [
    "### B.7 Final Dataset Merge (`merge.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pick_file(csvs, label):\n",
    "    \"\"\"Tiny helper to pick a CSV by index with a friendly label.\"\"\"\n",
    "    print(f\"\\nWhich file contains the {label}? (Enter the number)\")\n",
    "    try:\n",
    "        idx = int(input(\"Selection: \"))\n",
    "        return csvs[idx]\n",
    "    except Exception:\n",
    "        print(\"Invalid selection.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_macro_golden_source():\n",
    "    print(\"--- MACRO + BANK DATA BUILDER ---\")\n",
    "    print(\"We need to identify your raw source files.\")\n",
    "\n",
    "    # 1. List all CSVs in the folder (and parent folder)\n",
    "    csvs = glob.glob(\"*.csv\") + glob.glob(\"../*.csv\")\n",
    "\n",
    "    if not csvs:\n",
    "        print(\"CRITICAL: No CSV files found. Please move your raw Sentiment, Fed Funds, and Bank CSVs into this folder.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nAvailable CSV files:\")\n",
    "    for i, f in enumerate(csvs):\n",
    "        print(f\"[{i}] {f}\")\n",
    "\n",
    "    # 2. Ask User for Sentiment File\n",
    "    sent_file = pick_file(csvs, \"SENTIMENT data\")\n",
    "    if not sent_file:\n",
    "        return\n",
    "\n",
    "    # 3. Ask User for Fed Funds File\n",
    "    print(\"(Enter same number if they are in one file)\")\n",
    "    fed_file = pick_file(csvs, \"FED FUNDS RATE\")\n",
    "    if not fed_file:\n",
    "        return\n",
    "\n",
    "    # 4. Ask User for Bank Panel File\n",
    "    bank_file = pick_file(csvs, \"BANK PANEL (assets, risk_taking, etc.)\")\n",
    "    if not bank_file:\n",
    "        return\n",
    "\n",
    "    # 5. Process Sentiment\n",
    "    print(f\"\\nProcessing Sentiment from {sent_file}...\")\n",
    "    df_sent = pd.read_csv(sent_file)\n",
    "\n",
    "    # Auto-detect columns (Looking for 'sentiment' or similar)\n",
    "    sent_col = next((c for c in df_sent.columns if 'sentiment' in c.lower()), None)\n",
    "    date_col_s = next((c for c in df_sent.columns if 'date' in c.lower()), None)\n",
    "\n",
    "    if not sent_col or not date_col_s:\n",
    "        print(f\"Error: Could not find 'date' or 'sentiment' column in {sent_file}\")\n",
    "        print(f\"Columns found: {list(df_sent.columns)}\")\n",
    "        return\n",
    "\n",
    "    df_sent = df_sent[[date_col_s, sent_col]].rename(\n",
    "        columns={date_col_s: 'date', sent_col: 'fed_sentiment'}\n",
    "    )\n",
    "    df_sent['date'] = pd.to_datetime(df_sent['date'])\n",
    "\n",
    "    # 6. Process Fed Funds\n",
    "    print(f\"Processing Fed Funds from {fed_file}...\")\n",
    "    df_fed = pd.read_csv(fed_file)\n",
    "\n",
    "    # Auto-detect columns\n",
    "    fed_col = next((c for c in df_fed.columns if 'fund' in c.lower() or 'rate' in c.lower()), None)\n",
    "    date_col_f = next((c for c in df_fed.columns if 'date' in c.lower()), None)\n",
    "\n",
    "    if not fed_col or not date_col_f:\n",
    "        print(f\"Error: Could not find 'date' or 'rate' column in {fed_file}\")\n",
    "        return\n",
    "\n",
    "    df_fed = df_fed[[date_col_f, fed_col]].rename(\n",
    "        columns={date_col_f: 'date', fed_col: 'fed_funds_rate'}\n",
    "    )\n",
    "    df_fed['date'] = pd.to_datetime(df_fed['date'])\n",
    "\n",
    "    # 7. Merge & Aggregate to Quarter\n",
    "    print(\"Merging and aggregating macro data to quarterly level...\")\n",
    "\n",
    "    macro_df = pd.merge(df_sent, df_fed, on='date', how='outer')\n",
    "    macro_df['quarter_key'] = macro_df['date'].dt.to_period('Q').astype(str)\n",
    "\n",
    "    macro_clean = (\n",
    "        macro_df.groupby('quarter_key')[['fed_sentiment', 'fed_funds_rate']]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    macro_output = 'MACRO_DATA_CLEAN.csv'\n",
    "    macro_clean.to_csv(macro_output, index=False)\n",
    "    print(f\"\\nSUCCESS! Created '{macro_output}' with {len(macro_clean)} quarters.\")\n",
    "\n",
    "    # 8. Load Bank Panel and merge with macro to build regression-ready dataset\n",
    "    print(f\"\\nProcessing Bank Panel from {bank_file}...\")\n",
    "    df_bank = pd.read_csv(bank_file)\n",
    "\n",
    "    required_bank_cols = {'bank_id', 'date', 'assets', 'risk_taking'}\n",
    "    missing = required_bank_cols - set(df_bank.columns)\n",
    "    if missing:\n",
    "        print(f\"Error: Bank file missing required columns: {missing}\")\n",
    "        return\n",
    "\n",
    "    df_bank['date'] = pd.to_datetime(df_bank['date'])\n",
    "    df_bank['quarter_key'] = df_bank['date'].dt.to_period('Q').astype(str)\n",
    "\n",
    "    merged = pd.merge(df_bank, macro_clean, on='quarter_key', how='left')\n",
    "\n",
    "    # Keep core variables regression.py expects\n",
    "    cols_order = [\n",
    "        'bank_id', 'date', 'assets', 'ci_loans', 'risk_taking',\n",
    "        'fed_sentiment', 'fed_funds_rate'\n",
    "    ]\n",
    "    # ci_loans may be missing, so include if present\n",
    "    cols_order = [c for c in cols_order if c in merged.columns]\n",
    "    merged = merged[cols_order + [c for c in merged.columns if c not in cols_order]]\n",
    "\n",
    "    output = 'master_merged_dataset.csv'\n",
    "    merged.to_csv(output, index=False)\n",
    "\n",
    "    # Write uppercase alias for backwards compatibility.\n",
    "    Path('MASTER_MERGED_DATASET.csv').write_bytes(Path(output).read_bytes())\n",
    "\n",
    "    print(f\"\\nSUCCESS! Created '{output}' with {len(merged)} rows.\")\n",
    "    print(\"Sample rows:\")\n",
    "    print(merged.head())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_macro_golden_source()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462321f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix C: Regression Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb51c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.panel import PanelOLS\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "def run_robust_regression():\n",
    "    print(\"--- [1] DATA PREPARATION & CLEANING ---\")\n",
    "    dataset_paths = [\n",
    "        Path('master_merged_dataset.csv'),\n",
    "        Path('MASTER_MERGED_DATASET.csv'),\n",
    "    ]\n",
    "    df = None\n",
    "    for path in dataset_paths:\n",
    "        if path.exists():\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Loaded dataset from {path}\")\n",
    "            break\n",
    "    if df is None:\n",
    "        print(\"MASTER dataset not found. Please run merge.py first.\")\n",
    "        return\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # --- ADDRESSING POINT 9: Drop Short Panels ---\n",
    "    # Banks with fewer than 8 observations (2 years) do not contribute enough within-variance\n",
    "    counts = df.groupby('bank_id')['date'].count()\n",
    "    valid_banks = counts[counts >= 8].index\n",
    "    df = df[df['bank_id'].isin(valid_banks)].copy()\n",
    "    print(\n",
    "        f\" > Dropped short-lived entities. Banks remaining: {df['bank_id'].nunique():,}\")\n",
    "\n",
    "    # --- ADDRESSING POINT 1: Orthogonalization ---\n",
    "    # Isolate pure sentiment from the rate level\n",
    "    ortho_data = df[['fed_sentiment', 'fed_funds_rate']].dropna()\n",
    "    X_ortho = sm.add_constant(ortho_data['fed_funds_rate'])\n",
    "    model_ortho = sm.OLS(ortho_data['fed_sentiment'], X_ortho).fit()\n",
    "    df.loc[ortho_data.index, 'sentiment_shock'] = model_ortho.resid\n",
    "\n",
    "    # --- ADDRESSING POINT 2: Standardization ---\n",
    "    # Standardize shock to 1 SD for interpretability\n",
    "    df['sentiment_shock_std'] = df['sentiment_shock'] / \\\n",
    "        df['sentiment_shock'].std()\n",
    "\n",
    "    # --- ADDRESSING POINT 4 & 10: Functional Form & Scaling ---\n",
    "    # Log Assets\n",
    "    df['log_assets'] = np.log(df['assets'].replace(0, np.nan))\n",
    "\n",
    "    # CENTER Log Assets.\n",
    "    # This is crucial. Now 'log_assets_centered' = 0 means \"Average Sized Bank\".\n",
    "    # This fixes multicollinearity by orthogonalizing the interaction from the main effect.\n",
    "    df['log_assets_centered'] = df['log_assets'] - df['log_assets'].mean()\n",
    "\n",
    "    # Sort for Lagging\n",
    "    df = df.sort_values(['bank_id', 'date'])\n",
    "\n",
    "    # --- ADDRESSING POINT 5: Dynamic Structure ---\n",
    "    # We MUST include lags of the dependent variable (Risk Taking)\n",
    "    # We also lag controls to mitigate simultaneity (Point 8)\n",
    "    vars_to_lag = ['risk_taking', 'sentiment_shock_std',\n",
    "                   'fed_funds_rate', 'log_assets_centered']\n",
    "\n",
    "    for var in vars_to_lag:\n",
    "        df[f'{var}_lag'] = df.groupby('bank_id')[var].shift(1)\n",
    "\n",
    "    # --- ADDRESSING POINT 3: Continuous Interaction ---\n",
    "    # Instead of a dummy, we use Size as a continuous moderator.\n",
    "    # Interaction = (Last Quarter's Shock) * (Last Quarter's Size)\n",
    "    df['shock_x_size'] = df['sentiment_shock_std_lag'] * \\\n",
    "        df['log_assets_centered_lag']\n",
    "\n",
    "    # --- ESTIMATION ---\n",
    "    print(\"--- [2] ESTIMATING DYNAMIC PANEL MODEL ---\")\n",
    "\n",
    "    df = df.set_index(['bank_id', 'date'])\n",
    "\n",
    "    reg_df = df.dropna(subset=[\n",
    "        'risk_taking', 'risk_taking_lag',\n",
    "        'sentiment_shock_std_lag', 'shock_x_size',\n",
    "        'fed_funds_rate_lag', 'log_assets_centered_lag'\n",
    "    ])\n",
    "\n",
    "    exog_vars = [\n",
    "        'const',\n",
    "        'risk_taking_lag',           # Dynamics (Fixes Point 5)\n",
    "        'sentiment_shock_std_lag',   # Main Effect (at mean size)\n",
    "        # Interaction (Does Size change sensitivity?)\n",
    "        'shock_x_size',\n",
    "        'log_assets_centered_lag',   # Control for Size\n",
    "        'fed_funds_rate_lag'         # Control for Rate\n",
    "    ]\n",
    "\n",
    "    reg_df['const'] = 1\n",
    "\n",
    "    # ADDRESSING POINT 7: Fixed Effects\n",
    "    mod = PanelOLS(reg_df['risk_taking'],\n",
    "                   reg_df[exog_vars], entity_effects=True)\n",
    "\n",
    "    # ADDRESSING POINT 6: Cluster Level\n",
    "    # Explicitly clustering by Entity (Bank) and Time (Date)\n",
    "    res = mod.fit(cov_type='clustered', cluster_entity=True, cluster_time=True)\n",
    "\n",
    "    print(res)\n",
    "\n",
    "    # --- DIAGNOSTIC CHECKS ---\n",
    "    print(\"\\n--- DIAGNOSTICS ---\")\n",
    "    print(f\"Within R-squared: {res.rsquared_within:.4f}\")\n",
    "    print(\n",
    "        f\"Autocorrelation check (LDV coeff): {res.params['risk_taking_lag']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_robust_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc41545",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix D: Graph Code\n",
    "\n",
    "All graphs should be run with `master_merged_dataset.csv` in the same directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148066e",
   "metadata": {},
   "source": [
    "# Figure 1: Time Series of Sentiment and Fed Funds Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a4347c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmdates\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('master_merged_dataset.csv')\n",
    "\n",
    "# Pre-processing: Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "\n",
    "def plot_macro_time_series(df):\n",
    "    # Aggregate to time-series level (since macro vars are constant across banks per quarter)\n",
    "    macro_df = df.groupby(\n",
    "        'date')[['fed_sentiment', 'fed_funds_rate']].mean().reset_index()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot Sentiment (Left Axis)\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Raw Sentiment (fed_sentiment)', color=color)\n",
    "    ax1.plot(macro_df['date'], macro_df['fed_sentiment'],\n",
    "             color=color, label='Fed Sentiment')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Create a second y-axis for Fed Funds Rate\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Fed Funds Rate (%)', color=color)\n",
    "    ax2.plot(macro_df['date'], macro_df['fed_funds_rate'],\n",
    "             color=color, label='Fed Funds Rate')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Time Series: Raw Sentiment and Fed Funds Rate',\n",
    "              fontsize=14, pad=15)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_macro_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a713a1",
   "metadata": {},
   "source": [
    "## Figure 2: Data Construction Diagram (FFIEC Changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890cecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_data_construction_diagram():\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    legacy_text = (\n",
    "        \"Legacy FFIEC Data (Pre-2001)\\n\"\n",
    "        \"• SAS XPT fixed-width files\\n\"\n",
    "        \"• Cryptic variable codes\\n\"\n",
    "        \"• Nonstandard date strings\\n\"\n",
    "        \"• Blank missing values\\n\"\n",
    "        \"• Irregular reporting dates\\n\"\n",
    "        \"• Incomplete bank identifiers\"\n",
    "    )\n",
    "\n",
    "    modern_text = (\n",
    "        \"Modern FFIEC Data (Post-2001)\\n\"\n",
    "        \"• CSV/TXT delimited files\\n\"\n",
    "        \"• Consistent variable labels\\n\"\n",
    "        \"• Standard quarter-end dates\\n\"\n",
    "        \"• Explicit NA markers\\n\"\n",
    "        \"• Uniform reporting structure\\n\"\n",
    "        \"• Full RSSD + cert IDs\"\n",
    "    )\n",
    "\n",
    "    ax.text(\n",
    "        0.05, 0.5, legacy_text,\n",
    "        fontsize=11, va=\"center\", ha=\"left\", family=\"monospace\"\n",
    "    )\n",
    "\n",
    "    ax.text(\n",
    "        0.60, 0.5, modern_text,\n",
    "        fontsize=11, va=\"center\", ha=\"left\", family=\"monospace\"\n",
    "    )\n",
    "\n",
    "    # Arrow using annotate (much better scaling control)\n",
    "    ax.annotate(\n",
    "        \"\",\n",
    "        xy=(0.58, 0.5),\n",
    "        xytext=(0.42, 0.5),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"->\",\n",
    "            lw=2,\n",
    "            mutation_scale=12,\n",
    "            color=\"#6FA8DC\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_data_construction_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26192c67",
   "metadata": {},
   "source": [
    "## Figure 3: Binscatter of Risk Taking vs. Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('master_merged_dataset.csv')\n",
    "\n",
    "# Pre-processing: Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def plot_binscatter_risk_sentiment(df):\n",
    "    # Bin the sentiment into deciles to reduce noise\n",
    "    df['sentiment_bin'] = pd.qcut(df['fed_sentiment'], q=20, labels=False)\n",
    "    binned_data = df.groupby('sentiment_bin')[\n",
    "        ['fed_sentiment', 'risk_taking']].mean()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(x=binned_data['fed_sentiment'], y=binned_data['risk_taking'], scatter_kws={\n",
    "                's': 100}, line_kws={'color': 'red'})\n",
    "\n",
    "    plt.xlabel('Fed Sentiment (Binned)')\n",
    "    plt.ylabel('Average Bank Risk Taking')\n",
    "    plt.title('Correlation: Fed Sentiment vs. Bank Risk Taking', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_binscatter_risk_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d726ed",
   "metadata": {},
   "source": [
    "## Figure 4: The \"Identification\" Plot (Risk Taking by Bank Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7452991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('master_merged_dataset.csv')\n",
    "\n",
    "# Pre-processing: Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def plot_heterogeneity_by_size(df):\n",
    "    # Create size quintiles based on Assets\n",
    "    df['size_quintile'] = df.groupby('date')['assets'].transform(\n",
    "        lambda x: pd.qcut(x, 5, labels=['Smallest', '2', '3', '4', 'Largest']))\n",
    "\n",
    "    # Filter to just Smallest vs Largest for clarity\n",
    "    subset = df[df['size_quintile'].isin(['Smallest', 'Largest'])]\n",
    "\n",
    "    # Aggregate over time\n",
    "    time_series = subset.groupby(['date', 'size_quintile'])[\n",
    "        'risk_taking'].mean().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=time_series, x='date', y='risk_taking',\n",
    "                 hue='size_quintile', style='size_quintile')\n",
    "\n",
    "    plt.title('Risk Taking over Time: Small vs. Large Banks', fontsize=14)\n",
    "    plt.ylabel('Average Risk Taking')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend(title='Bank Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_heterogeneity_by_size(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491d091",
   "metadata": {},
   "source": [
    "## Figure 5: Distribution of Risk Taking (Pre/Post Crisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd7623d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmdates\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('master_merged_dataset.csv')\n",
    "\n",
    "# Pre-processing: Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "def plot_risk_distribution(df):\n",
    "    # Define periods\n",
    "    df['period'] = np.where(df['date'].dt.year < 2008, 'Pre-2008', 'Post-2008')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(data=df, x='risk_taking', hue='period',\n",
    "                fill=True, common_norm=False, palette='crest')\n",
    "\n",
    "    plt.title('Distribution of Bank Risk Taking: Pre vs. Post 2008', fontsize=14)\n",
    "    plt.xlabel('Risk Taking Measure')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_risk_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc719558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
