{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3f4c21",
   "metadata": {},
   "source": [
    "## Data Analysis: Investigating BNPL Stock Returns and Monetary Policy\n",
    "\n",
    "I employ modern econometric techniques and reproducible research practices. The global BNPL market is projected to reach \\$560.1 billion in gross merchandise volume by 2025, reflecting 13.7\\% year-over-year growth, with user adoption accelerating toward 900 million globally by 2027 {cite}`Chargeflow2025`. This explosive growth, a 157\\% increase from 360 million users in 2022, underscores the sector's increasing importance in consumer credit markets and motivates careful examination of how these firms respond to monetary policy changes.\n",
    "\n",
    "```{raw} latex\n",
    "\\vspace{-1.0\\baselineskip}\n",
    "```\n",
    "\n",
    "### Computational Environment and Research Tools\n",
    "\n",
    "I use Python 3.11 with specialized libraries at each step. `pandas` (v2.2.3) handles data and time alignment; `statsmodels` (v0.14.4) provides HC3 robust inference suited to this sample size; `yfinance` supplies stock prices; `fredapi` pulls Federal Funds Rate, CPI, sentiment, and income series; `matplotlib` (v3.9.2) and `seaborn` (v0.13.2) generate publication-quality visuals.\n",
    "\n",
    "```{raw} latex\n",
    "\\vspace{-1.0\\baselineskip}\n",
    "```\n",
    "\n",
    "### Reproducibility and Dynamic Document Generation\n",
    "\n",
    "All data collection, transformation, and estimation are programmatic for full reproducibility. With the required API keys, any reader can rerun the analysis and obtain identical outputs. Tables and figures are glued directly from code via `myst_nb.glue`, avoiding transcription errors and keeping the manuscript synchronized with computations.\n",
    "\n",
    "The analysis environment is fully documented in `binder/environment.yml`, specifying exact package versions to ensure that the computational environment can be reconstructed. This documentation follows the principles outlined in the Journal of Open Source Software and enables other researchers to validate, extend, or build upon this work.\n",
    "\n",
    "```{raw} latex\n",
    "\\vspace{-1.0\\baselineskip}\n",
    "```\n",
    "\n",
    "### Analytical Pipeline Overview\n",
    "\n",
    "The analysis proceeds through six integrated stages, each building upon the previous to construct a coherent analytical narrative. First, I collect data from authoritative sources and I construct variables with appropriate transformations, including log transformation of returns and first-differencing of macroeconomic series to ensure stationarity. Second, exploratory visualizations identify patterns, outliers, and preliminary relationships that inform model specification. Third, correlation analysis assesses multicollinearity among predictors and provides initial evidence on bivariate associations. Fourth, I formally estimate models across multiple specifications, including OLS, Fama-French three-factor, instrumental variables, and difference-in-differences approaches, to test the interest rate hypothesis under different identifying assumptions. Fifth, diagnostic tests validate model assumptions including homoskedasticity, absence of autocorrelation, normality of residuals, and absence of multicollinearity, ensuring reliable inference. Finally, sensitivity analysis examines robustness across different time periods and market conditions, addressing concerns about the stability of findings. The following subsections present each stage in turn, with full transparency about methodological choices and their implications.\n",
    "\n",
    "The empirical window spans 66 monthly observations (Feb 2020-Aug 2025), which limits statistical power to approximately 15-20\\% for the observed effect sizes, so reported coefficients should be read as descriptive sensitivities rather than precise hypothesis tests. The full specification with market, inflation, confidence, and disposable income is the primary model; the rate-only base model is retained as a robustness illustration of omitted-variable bias. Market beta plays a central role in interpretation because BNPL stocks price like high-beta growth assets. Macroeconomic series use FRED seasonally adjusted versions (CPIAUCSL, DSPIC96, UMCSENT) so rate and inflation shocks are not confounded by holiday/tax-season patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c1c33",
   "metadata": {},
   "source": [
    "```{raw} latex\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\normalsize\n",
    "\\renewcommand{\\arraystretch}{1.5}\n",
    "\\caption{Variable Definitions and Summary Statistics}\n",
    "\\label{tab:variables}\n",
    "\\resizebox{\\textwidth}{!}{%\n",
    "\\begin{tabular}{p{2.8cm}p{1.5cm}p{3.2cm}p{2.2cm}p{1.5cm}rrrr}\n",
    "\\toprule\n",
    "Variable & Symbol & Definition & Source & Transform & Mean & Std Dev & Min & Max \\\\\n",
    "\\midrule\n",
    "BNPL Returns & $R_{BNPL}$ & Log portfolio return (\\%) & Yahoo Finance & Log & 1.7 & 19.0 & $-42.8$ & 41.3 \\\\\n",
    "Federal Funds Rate Change & $\\Delta FFR$ & MoM change in FFR (pp) & FRED & Diff & 0.0 & 0.2 & $-0.9$ & 0.7 \\\\\n",
    "Consumer Confidence Change & $\\Delta CC$ & MoM change in UM Sentiment & FRED & Diff & $-0.6$ & 5.2 & $-17.3$ & 9.3 \\\\\n",
    "Disposable Income Change & $\\Delta DI$ & MoM \\% change in real income & FRED & Pct & 0.3 & 4.3 & $-15.1$ & 22.9 \\\\\n",
    "Inflation Change & $\\Delta \\pi$ & MoM \\% change in CPI (SA) & FRED & Pct & 0.3 & 0.3 & $-0.8$ & 1.3 \\\\\n",
    "Market Return & $R_{MKT}$ & Monthly S\\&P 500 return (\\%) & Yahoo (SPY) & Pct & 1.4 & 5.0 & $-12.5$ & 12.7 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}%\n",
    "}\n",
    "\\renewcommand{\\arraystretch}{1.0}\n",
    "\\end{table}\n",
    "```\n",
    "{numref}`tab:variables` presents variable definitions, data sources, transformations, and summary statistics for the sample covering February 2020 through August 2025, encompassing the COVID-19 pandemic, the zero lower bound period, and the Federal Reserve's aggressive tightening cycle. BNPL returns are highly volatile (SD 19.0\\%) with a modest mean return of 1.7\\%, substantially higher than typical equity returns. Federal Funds Rate changes have a standard deviation of 0.2 percentage points, though the sample includes the rapid tightening cycle from March 2022 to July 2023 when rates increased by 525 basis points. Market returns (SD 5.0\\%) are far less volatile than BNPL returns, hinting at the high market beta documented in subsequent regression analysis and suggesting BNPL stocks carry significant idiosyncratic risk beyond their exposure to systematic market factors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1de52",
   "metadata": {},
   "source": [
    "### Data Sources and Portfolio Composition\n",
    "\n",
    "The BNPL portfolio spans three distinct business models that provide complementary perspectives on how different BNPL structures respond to monetary policy changes. Affirm represents the pure-play BNPL business model, funding its operations through warehouse lines and securitizations, which creates direct pass-through of funding cost changes to profitability and potentially to stock valuations. This structure makes Affirm the most likely candidate to exhibit interest rate sensitivity, as funding costs represent a larger share of operating expenses and cannot be easily diversified away. Sezzle focuses on smaller-ticket transactions and younger consumer demographics, operating with thinner profit margins that may amplify the impact of funding cost increases, suggesting higher sensitivity to interest rate changes despite its smaller market capitalization. PayPal, in contrast, operates as a diversified payments platform where BNPL represents only a smaller product line (Pay in 4, representing less than 5% of revenue), meaning that diversification benefits and the presence of deposits and merchant float revenue streams dampen the impact of BNPL-specific funding shocks on overall firm performance.\n",
    "\n",
    "### Sample Construction Limitations\n",
    "\n",
    "The equal-weighted portfolio approach has several limitations.\n",
    "\n",
    "First, PayPal's minimal BNPL exposure (less than 5\\% of revenue) means including it in an equal-weighted portfolio dilutes BNPL-specific effects. If Affirm has true rate sensitivity $\\beta$ = -30 and PayPal has $\\beta$ = 0 (because BNPL is negligible), the equal-weighted portfolio will show $\\beta$ ≈ -10 to -15, attenuated toward zero.\n",
    "\n",
    "Second, Sezzle is a micro-cap stock (market cap ~\\$1B) with higher idiosyncratic volatility and potential liquidity issues, yet equal-weighting gives it the same influence as PayPal (market cap ~\\$85B), which is economically problematic.\n",
    "\n",
    "Third, the sample period spans distinct economic regimes (COVID crash, zero-rate period, tightening cycle) with very different characteristics, potentially obscuring rate sensitivity that may only manifest during specific periods. The tightening-only subsample (n=17, March 2022 - July 2023) shows $\\beta$ = -16.64 with SE = 28.28, p = 0.556—larger in magnitude but completely imprecise due to small sample size. These limitations should be considered when interpreting the results.\n",
    "\n",
    "Full firm-level details, including individual company financial metrics and business model descriptions, remain in Appendix A for readers interested in firm-specific analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "136221a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:56:51.138244Z",
     "iopub.status.busy": "2025-12-19T15:56:51.137762Z",
     "iopub.status.idle": "2025-12-19T15:56:55.876554Z",
     "shell.execute_reply": "2025-12-19T15:56:55.875974Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/2762617298.py:20: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  bnpl_data = yf.download(bnpl_tickers, start=start_date, end=end_date)['Close']\n",
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Variable Definitions and Summary Statistics (Feb 2020 - Aug 2025)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/2762617298.py:42: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  cpi_change = cpi_monthly.pct_change() * 100\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Symbol</th>\n      <th>Definition</th>\n      <th>Source</th>\n      <th>Transform</th>\n      <th>Mean</th>\n      <th>Std Dev</th>\n      <th>Min</th>\n      <th>Max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BNPL Returns</td>\n      <td>R_BNPL</td>\n      <td>Log portfolio return</td>\n      <td>Yahoo Finance</td>\n      <td>Log</td>\n      <td>1.7</td>\n      <td>19.0</td>\n      <td>-42.8</td>\n      <td>41.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Federal Funds Rate Change</td>\n      <td>ΔFFR</td>\n      <td>MoM change in FFR (pp)</td>\n      <td>FRED (FEDFUNDS)</td>\n      <td>Diff</td>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>-0.9</td>\n      <td>0.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consumer Confidence Change</td>\n      <td>ΔCC</td>\n      <td>MoM change in UM Sentiment</td>\n      <td>FRED (UMCSENT)</td>\n      <td>Diff</td>\n      <td>-0.6</td>\n      <td>5.2</td>\n      <td>-17.3</td>\n      <td>9.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Disposable Income Change</td>\n      <td>ΔDI</td>\n      <td>MoM % change in real income</td>\n      <td>FRED (DSPIC96)</td>\n      <td>Pct</td>\n      <td>0.3</td>\n      <td>4.3</td>\n      <td>-15.1</td>\n      <td>22.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Inflation Change</td>\n      <td>Δπ</td>\n      <td>MoM % change in CPI (SA)</td>\n      <td>FRED (CPIAUCSL)</td>\n      <td>Pct</td>\n      <td>0.3</td>\n      <td>0.3</td>\n      <td>-0.8</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Market Return</td>\n      <td>R_MKT</td>\n      <td>Monthly S&amp;P 500 return (pp)</td>\n      <td>Yahoo Finance (SPY)</td>\n      <td>Pct</td>\n      <td>1.4</td>\n      <td>5.0</td>\n      <td>-12.5</td>\n      <td>12.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                     Variable  Symbol                   Definition  \\\n0                BNPL Returns  R_BNPL         Log portfolio return   \n1   Federal Funds Rate Change    ΔFFR       MoM change in FFR (pp)   \n2  Consumer Confidence Change     ΔCC   MoM change in UM Sentiment   \n3    Disposable Income Change     ΔDI  MoM % change in real income   \n4            Inflation Change      Δπ     MoM % change in CPI (SA)   \n5               Market Return   R_MKT  Monthly S&P 500 return (pp)   \n\n                Source Transform  Mean Std Dev    Min   Max  \n0        Yahoo Finance       Log   1.7    19.0  -42.8  41.3  \n1      FRED (FEDFUNDS)      Diff   0.0     0.2   -0.9   0.7  \n2       FRED (UMCSENT)      Diff  -0.6     5.2  -17.3   9.3  \n3       FRED (DSPIC96)       Pct   0.3     4.3  -15.1  22.9  \n4      FRED (CPIAUCSL)       Pct   0.3     0.3   -0.8   1.3  \n5  Yahoo Finance (SPY)       Pct   1.4     5.0  -12.5  12.7  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-1"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: n = 66 monthly observations (Feb 2020 - Aug 2025).\n",
      "Transforms: Diff = first difference; Pct = percentage change; Log = log return.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 1: Variable Definitions and Summary Statistics\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "from fredapi import Fred\n",
    "from myst_nb import glue\n",
    "\n",
    "# Load data\n",
    "# Get FRED API key from environment variable, or use the key directly if not set\n",
    "FRED_API_KEY = os.environ.get('FRED_API_KEY') or '4b5403c35607ac13101f3257a0ce6ea3'\n",
    "start_date = datetime(2020, 2, 1)\n",
    "end_date = datetime(2025, 8, 31)\n",
    "\n",
    "# BNPL stocks\n",
    "bnpl_tickers = ['AFRM', 'SEZL', 'PYPL']\n",
    "bnpl_data = yf.download(bnpl_tickers, start=start_date, end=end_date)['Close']\n",
    "bnpl_monthly = bnpl_data.resample('ME').last()\n",
    "bnpl_returns = np.log(bnpl_monthly / bnpl_monthly.shift(1))\n",
    "log_returns = bnpl_returns.mean(axis=1) * 100  # Convert to percentage\n",
    "\n",
    "# FRED data\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "ffr = fred.get_series('FEDFUNDS', start=start_date, end=end_date)\n",
    "ffr_monthly = ffr.resample('ME').last()\n",
    "ffr_change = ffr_monthly.diff()\n",
    "\n",
    "cc = fred.get_series('UMCSENT', start=start_date, end=end_date)\n",
    "cc_monthly = cc.resample('ME').last()\n",
    "cc_change = cc_monthly.diff()\n",
    "\n",
    "di = fred.get_series('DSPIC96', start=start_date, end=end_date)\n",
    "di_monthly = di.resample('ME').last()\n",
    "di_change = di_monthly.pct_change() * 100\n",
    "\n",
    "cpi = fred.get_series('CPIAUCSL', start=start_date, end=end_date)\n",
    "cpi_monthly = cpi.resample('ME').last()\n",
    "cpi_change = cpi_monthly.pct_change() * 100\n",
    "\n",
    "spy = yf.Ticker(\"SPY\")\n",
    "spy_hist = spy.history(start=start_date, end=end_date)\n",
    "spy_monthly = spy_hist['Close'].resample('ME').last()\n",
    "if spy_monthly.index.tz is not None:\n",
    "    spy_monthly.index = spy_monthly.index.tz_localize(None)\n",
    "market_return = spy_monthly.pct_change() * 100\n",
    "\n",
    "# Combine into DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'log_returns': log_returns,\n",
    "    'ffr_change': ffr_change,\n",
    "    'cc_change': cc_change,\n",
    "    'di_change': di_change,\n",
    "    'cpi_change': cpi_change,\n",
    "    'market_return': market_return\n",
    "}).dropna()\n",
    "\n",
    "# Generate Table 1\n",
    "var_info = [\n",
    "    ('BNPL Returns', 'R_BNPL', 'Log portfolio return', 'Yahoo Finance', 'Log', 'log_returns'),\n",
    "    ('Federal Funds Rate Change', 'ΔFFR', 'MoM change in FFR (pp)', 'FRED (FEDFUNDS)', 'Diff', 'ffr_change'),\n",
    "    ('Consumer Confidence Change', 'ΔCC', 'MoM change in UM Sentiment', 'FRED (UMCSENT)', 'Diff', 'cc_change'),\n",
    "    ('Disposable Income Change', 'ΔDI', 'MoM % change in real income', 'FRED (DSPIC96)', 'Pct', 'di_change'),\n",
    "    ('Inflation Change', 'Δπ', 'MoM % change in CPI (SA)', 'FRED (CPIAUCSL)', 'Pct', 'cpi_change'),\n",
    "    ('Market Return', 'R_MKT', 'Monthly S&P 500 return (pp)', 'Yahoo Finance (SPY)', 'Pct', 'market_return')\n",
    "]\n",
    "\n",
    "summary_stats = []\n",
    "for var_name, symbol, definition, source, transform, col in var_info:\n",
    "    if col in data.columns:\n",
    "        summary_stats.append({\n",
    "            'Variable': var_name,\n",
    "            'Symbol': symbol,\n",
    "            'Definition': definition,\n",
    "            'Source': source,\n",
    "            'Transform': transform,\n",
    "            'Mean': f\"{data[col].mean():.1f}\",\n",
    "            'Std Dev': f\"{data[col].std():.1f}\",\n",
    "            'Min': f\"{data[col].min():.1f}\",\n",
    "            'Max': f\"{data[col].max():.1f}\"\n",
    "        })\n",
    "\n",
    "table_1 = pd.DataFrame(summary_stats)\n",
    "print(\"Table 1: Variable Definitions and Summary Statistics (Feb 2020 - Aug 2025)\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-1\", table_1, display=False)\n",
    "print(f\"\\nNote: n = {len(data)} monthly observations (Feb 2020 - Aug 2025).\")\n",
    "print(\"Transforms: Diff = first difference; Pct = percentage change; Log = log return.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b2add-5ed1-48c7-832b-86a9fb8c7648",
   "metadata": {},
   "source": [
    "{numref}`tbl-correlation` presents pairwise correlations among all variables used in the regression analysis, providing initial evidence on bivariate relationships and assessing multicollinearity concerns that could affect regression estimates. The correlation matrix reveals several key patterns: BNPL returns exhibit a strong positive correlation with market returns (r = 0.648, p < 0.01), confirming the high market beta documented in subsequent regression analysis. The modest negative correlation between BNPL returns and Federal Funds Rate changes (r = -0.154) provides preliminary evidence of interest rate sensitivity, though this relationship is not statistically significant at conventional levels. The correlation between FFR changes and inflation changes (r = 0.383, p < 0.01) reflects the monetary policy response to price pressures, while all pairwise correlations remain below 0.80, indicating no severe multicollinearity concerns that would compromise regression estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a64e6494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:56:55.882910Z",
     "iopub.status.busy": "2025-12-19T15:56:55.882721Z",
     "iopub.status.idle": "2025-12-19T15:56:56.876925Z",
     "shell.execute_reply": "2025-12-19T15:56:56.876128Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2: Correlation Matrix (stars = significance)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/3720566564.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.154' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.162' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.263**' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.648***' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BNPL Returns</th>\n      <th>Δ FFR</th>\n      <th>Δ Consumer Conf.</th>\n      <th>Δ Disp. Income</th>\n      <th>Δ Inflation</th>\n      <th>Market Return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BNPL Returns</th>\n      <td>1.000</td>\n      <td>-0.154</td>\n      <td>0.162</td>\n      <td>-0.014</td>\n      <td>-0.263**</td>\n      <td>0.648***</td>\n    </tr>\n    <tr>\n      <th>Δ FFR</th>\n      <td>-0.154</td>\n      <td>1.000</td>\n      <td>0.266**</td>\n      <td>-0.102</td>\n      <td>0.383***</td>\n      <td>0.027</td>\n    </tr>\n    <tr>\n      <th>Δ Consumer Conf.</th>\n      <td>0.162</td>\n      <td>0.266**</td>\n      <td>1.000</td>\n      <td>-0.047</td>\n      <td>0.161</td>\n      <td>0.054</td>\n    </tr>\n    <tr>\n      <th>Δ Disp. Income</th>\n      <td>-0.014</td>\n      <td>-0.102</td>\n      <td>-0.047</td>\n      <td>1.000</td>\n      <td>-0.224*</td>\n      <td>0.103</td>\n    </tr>\n    <tr>\n      <th>Δ Inflation</th>\n      <td>-0.263**</td>\n      <td>0.383***</td>\n      <td>0.161</td>\n      <td>-0.224*</td>\n      <td>1.000</td>\n      <td>-0.095</td>\n    </tr>\n    <tr>\n      <th>Market Return</th>\n      <td>0.648***</td>\n      <td>0.027</td>\n      <td>0.054</td>\n      <td>0.103</td>\n      <td>-0.095</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                 BNPL Returns     Δ FFR Δ Consumer Conf. Δ Disp. Income  \\\nBNPL Returns            1.000    -0.154            0.162         -0.014   \nΔ FFR                  -0.154     1.000          0.266**         -0.102   \nΔ Consumer Conf.        0.162   0.266**            1.000         -0.047   \nΔ Disp. Income         -0.014    -0.102           -0.047          1.000   \nΔ Inflation          -0.263**  0.383***            0.161        -0.224*   \nMarket Return        0.648***     0.027            0.054          0.103   \n\n                 Δ Inflation Market Return  \nBNPL Returns        -0.263**      0.648***  \nΔ FFR               0.383***         0.027  \nΔ Consumer Conf.       0.161         0.054  \nΔ Disp. Income       -0.224*         0.103  \nΔ Inflation            1.000        -0.095  \nMarket Return         -0.095         1.000  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-2"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: n = 66 monthly observations.\n",
      "Stars: * p<0.10, ** p<0.05, *** p<0.01. |r| ≥ 0.25 is significant at 5% with n=66.\n",
      "Correlations below |0.80| indicate no severe multicollinearity concerns.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 2: Correlation Matrix\n",
    "# ============================================================================\n",
    "\n",
    "# Rename columns for display\n",
    "corr_data = data.rename(columns={\n",
    "    'log_returns': 'BNPL Returns',\n",
    "    'ffr_change': 'Δ FFR',\n",
    "    'cc_change': 'Δ Consumer Conf.',\n",
    "    'di_change': 'Δ Disp. Income',\n",
    "    'cpi_change': 'Δ Inflation',\n",
    "    'market_return': 'Market Return'\n",
    "})\n",
    "\n",
    "corr_matrix = corr_data.corr()\n",
    "\n",
    "# Compute p-values for significance stars\n",
    "from scipy import stats\n",
    "pvals = pd.DataFrame(np.ones_like(corr_matrix), index=corr_matrix.index, columns=corr_matrix.columns)\n",
    "for i, col_i in enumerate(corr_data.columns):\n",
    "    for j, col_j in enumerate(corr_data.columns):\n",
    "        if i < j:\n",
    "            r, p = stats.pearsonr(corr_data[col_i], corr_data[col_j])\n",
    "            pvals.loc[col_i, col_j] = p\n",
    "            pvals.loc[col_j, col_i] = p\n",
    "\n",
    "star = lambda p: '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n",
    "annotated = corr_matrix.copy()\n",
    "for i, col_i in enumerate(corr_matrix.index):\n",
    "    for j, col_j in enumerate(corr_matrix.columns):\n",
    "        if i == j:\n",
    "            annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}\"\n",
    "        else:\n",
    "            annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
    "\n",
    "print(\"Table 2: Correlation Matrix (stars = significance)\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-2\", annotated, display=False)\n",
    "print(f\"\\nNote: n = {len(data)} monthly observations.\")\n",
    "print(\"Stars: * p<0.10, ** p<0.05, *** p<0.01. |r| ≥ 0.25 is significant at 5% with n=66.\")\n",
    "print(\"Correlations below |0.80| indicate no severe multicollinearity concerns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa777c5c-94f6-4f8c-8068-2f0c7ce89c2d",
   "metadata": {},
   "source": [
    "```{table} Correlation Matrix\n",
    ":name: tbl-correlation\n",
    ":align: center\n",
    "\n",
    "{{glue:dataframe:table-2}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "control_variable_selection_interpretation",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis: Visualizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5991b",
   "metadata": {},
   "source": [
    "Before proceeding to formal econometric estimation, exploratory visualization provides crucial insights into the data structure. The following graphical representations serve multiple purposes: they help identify patterns that motivate specific model specifications, reveal potential outliers or data quality issues that could distort regression results, provide intuition for the relationships that will be estimated econometrically, and offer visual confirmation that complements numerical results. The visualizations presented here establish the empirical foundation upon which the regression analysis builds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2aeb5",
   "metadata": {},
   "source": [
    "```{raw} latex\n",
    "\\newpage\n",
    "\\FloatBarrier\n",
    "```\n",
    "\n",
    "### BNPL Portfolio Monthly Returns\n",
    "\n",
    "```{figure} chart_a_time_series.png\n",
    ":name: fig_bnpl_returns\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "BNPL Portfolio Monthly Returns (Feb 2020-Aug 2025)\n",
    "```\n",
    "\n",
    "{numref}`fig_bnpl_returns` shows monthly log returns for an equally weighted portfolio of Affirm, Sezzle, and PayPal. Gray shading marks the COVID shock (Mar-Jun 2020), blue shading shows the zero-bound period through Feb 2022, and red shading marks the Fed's tightening (Mar 2022-Jul 2023, +525 bp). BNPL returns are highly volatile (SD ≈ 19\\%), with swings above $\\pm$40\\%; the plot includes a thick zero line and annotations for the start of hikes and peak volatility. The mean return of 1.7\\% masks substantial variation, and sharp declines during 2022-2023 coincide with funding cost increases documented by {cite}`Laudenbach2025`. These regimes and callouts correspond directly to the shaded blocks and arrows on the chart.\n",
    "\n",
    "The period of strong positive returns in late 2020 and 2021 reflects the rapid growth in BNPL adoption documented by the {cite}`CFPB2025ConsumerUse`, as consumers turned to alternative payment methods during the pandemic. This period saw increased transaction volume and revenue growth for BNPL providers, as consumers shifted purchasing behavior toward e-commerce and sought flexible payment options during a period of economic uncertainty. The sharp negative returns observed in mid-2022 align with rising interest rates and increased funding costs, consistent with the {cite}`CFPB2022MarketTrends` documentation that BNPL firms' cost of funds increased substantially during this period. Higher interest rates compressed profit margins and reduced investor confidence, as the sector's thin margins (provider revenues represent only about 4\\% of gross merchandise volume according to {cite}`DigitalSilk2025`) made firms particularly vulnerable to funding cost increases.\n",
    "\n",
    "The period from late 2023 through 2025 exhibits continued volatility, reflecting ongoing sensitivity to monetary policy changes, macroeconomic conditions, and sector-specific developments. This persistent volatility motivates this analysis, which seeks to identify systematic factors that explain this observed variation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25d945",
   "metadata": {},
   "source": [
    "\n",
    "```{raw} latex\n",
    "\n",
    "\\newpage\n",
    "\\FloatBarrier\n",
    "```\n",
    "\n",
    "### BNPL vs Market Returns\n",
    "\n",
    "```{figure} chart_b_market.png\n",
    ":name: fig_bnpl_market\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "BNPL vs Market Returns\n",
    "```\n",
    "\n",
    "{numref}`fig_bnpl_market` presents a scatter plot of BNPL portfolio returns against market returns, providing a visual representation of the dominant relationship between BNPL stock performance and broader equity market movements. This visualization is crucial for understanding the systematic risk exposure of BNPL stocks and contextualizing the relative importance of market factors versus interest rate sensitivity.\n",
    "\n",
    "The plot reveals a strong positive relationship, with the bivariate slope approximately 2.46 (from $r \\times \\sigma_{BNPL}/\\sigma_{MKT} = 0.648 \\times 19.0/5.0$), while the multivariate coefficient from the full model is approximately 2.4, indicating that BNPL returns amplify market movements: when the market moves 1\\%, BNPL stocks move approximately 2.4\\% in the same direction. This high market beta reflects the growth-oriented, technology-enabled nature of BNPL firms, which exhibit sensitivity to risk sentiment and growth expectations that drive broader equity markets.\n",
    "\n",
    "The correlation coefficient of 0.648 ($R^2$ = 0.42) demonstrates that market returns alone explain approximately 42\\% of the variation in BNPL returns (from bivariate regression: $r^2 = 0.648^2$), making market exposure the single most important systematic factor driving BNPL stock performance. The 45° reference line included in the plot highlights the amplification effect, as most observations fall above this line, indicating that BNPL returns typically move more than proportionally with market returns.\n",
    "\n",
    "This strong market link has profound implications for understanding interest rate sensitivity: the dominance of market factors explains why the rate-only model achieves an $R^2$ of only 0.024, indicating that interest rate changes alone explain virtually none of BNPL return variation. While interest rate effects may be economically meaningful in magnitude (the estimated coefficient of -12.89 suggests substantial sensitivity), they are statistically and economically overwhelmed by systematic market risk. This pattern suggests that BNPL stocks are priced primarily as growth assets that respond to market-wide risk sentiment rather than as credit-sensitive financial instruments that respond directly to monetary policy changes through funding cost channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "chart_a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:56:56.880908Z",
     "iopub.status.busy": "2025-12-19T15:56:56.880249Z",
     "iopub.status.idle": "2025-12-19T15:56:59.600643Z",
     "shell.execute_reply": "2025-12-19T15:56:59.595387Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Clean Chart A saved as chart_a_time_series.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chart A: Time Series of Log BNPL Returns (Clean + Publication Quality)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Shaded periods (cleaner stacking)\n",
    "covid_start, covid_end = pd.Timestamp(\"2020-03-01\"), pd.Timestamp(\"2020-06-30\")\n",
    "zero_end = pd.Timestamp(\"2022-02-28\")\n",
    "hike_start, hike_end = pd.Timestamp(\"2022-03-01\"), pd.Timestamp(\"2023-07-31\")\n",
    "ax.axvspan(data.index.min(), zero_end, color=\"#7aa5d8\", alpha=0.28, label=\"Zero bound\", zorder=-2)\n",
    "ax.axvspan(covid_start, covid_end, color=\"#6f6f6f\", alpha=0.32, label=\"COVID shock\", zorder=-1)\n",
    "ax.axvspan(hike_start, hike_end, color=\"#f1c27d\", alpha=0.34, label=\"Rate hikes\", zorder=-2)\n",
    "\n",
    "# Raw log returns\n",
    "ax.plot(\n",
    "    data.index,\n",
    "    data[\"log_returns\"],\n",
    "    linewidth=1.6,\n",
    "    color=\"#1f77b4\",\n",
    "    alpha=0.80,\n",
    "    label=\"BNPL returns\",\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Rolling mean (clean + not too thick)\n",
    "# ---------------------------\n",
    "rolling_mean = data[\"log_returns\"].rolling(window=6, min_periods=1).mean()\n",
    "\n",
    "ax.plot(\n",
    "    data.index,\n",
    "    rolling_mean,\n",
    "    linewidth=2.6,\n",
    "    color=\"#1f2d3a\",\n",
    "    label=\"6M rolling mean\",\n",
    "    zorder=4\n",
    ")\n",
    "\n",
    "# Zero line\n",
    "ax.axhline(\n",
    "    y=0,\n",
    "    color=\"#2c3e50\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.8,\n",
    "    alpha=0.95,\n",
    "    label=\"Zero line\",\n",
    "    zorder=2\n",
    ")\n",
    "\n",
    "# Y-axis padding\n",
    "ymin = data[\"log_returns\"].min()\n",
    "ymax = data[\"log_returns\"].max()\n",
    "pad = (ymax - ymin) * 0.10\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "# Annotations\n",
    "peak_date = data[\"log_returns\"].abs().idxmax()\n",
    "peak_val = data.loc[peak_date, \"log_returns\"]\n",
    "peak_text = f\"Peak volatility ({peak_date.strftime('%b %Y')})\"\n",
    "# Place label just above the x-axis around Feb 2022\n",
    "peak_text_y = ymin - 0.06 * (ymax - ymin)\n",
    "ax.annotate(\n",
    "    peak_text,\n",
    "    xy=(peak_date, peak_val),\n",
    "    xytext=(pd.Timestamp(\"2022-02-28\"), peak_text_y),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"#1f2d3a\", linewidth=1.05, shrinkA=2, shrinkB=2),\n",
    "    fontsize=12,\n",
    "    color=\"#111\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"#b7bdc3\", alpha=0.94)\n",
    ")\n",
    "hike_y = data.loc[hike_start, \"log_returns\"] if hike_start in data.index else 0\n",
    "ax.annotate(\n",
    "    \"Liftoff (Mar 2022)\",\n",
    "    xy=(hike_start, hike_y),\n",
    "    xytext=(hike_start - pd.DateOffset(days=120), hike_y + 16),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"#755314\", linewidth=1.2, shrinkA=2, shrinkB=2, connectionstyle=\"arc3,rad=0.32\"),\n",
    "    fontsize=12,\n",
    "    color=\"#3d3a2a\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"#f2d59a\", alpha=0.96)\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"BNPL Portfolio Monthly Returns (Feb 2020-Aug 2025)\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Date\", fontsize=14, fontweight=\"bold\", labelpad=8)\n",
    "ax.set_ylabel(\"Log Returns (%)\", fontsize=14, fontweight=\"bold\", labelpad=8)\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "# Cleaner x-axis: quarterly ticks with Q labels (consistent with Chart M)\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def fmt_qtr(x, pos=None):\n",
    "    dt = mdates.num2date(x)\n",
    "    q = (dt.month - 1) // 3 + 1\n",
    "    return f\"Q{q} {dt.year}\"\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[3, 6, 9, 12]))\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(fmt_qtr))\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "ax.grid(True, linestyle=\":\", color=\"#bfc5c9\", alpha=0.35)\n",
    "ax.legend(\n",
    "    fontsize=11,\n",
    "    loc=\"upper left\",\n",
    "    bbox_to_anchor=(0.005, 0.99),\n",
    "    frameon=True,\n",
    "    framealpha=0.92,\n",
    "    edgecolor=\"#c5c5c5\",\n",
    "    facecolor=\"white\",\n",
    "    ncol=3\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_a_time_series.png\", dpi=400, facecolor=\"white\", bbox_inches=\"tight\", pad_inches=0.25)\n",
    "plt.close()\n",
    "\n",
    "print(\"    [OK] Clean Chart A saved as chart_a_time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "chart_b_code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:56:59.614024Z",
     "iopub.status.busy": "2025-12-19T15:56:59.612296Z",
     "iopub.status.idle": "2025-12-19T15:57:01.088191Z",
     "shell.execute_reply": "2025-12-19T15:57:01.087372Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Chart B (BNPL vs Market)...\n",
      "    [OK] Chart B saved as chart_b_market.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chart B: BNPL Returns vs Market Returns (Dominant Relationship)\n",
    "# ============================================================================\n",
    "print(\"  Creating Chart B (BNPL vs Market)...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Ensure full model for beta\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "beta_mkt = model_full.params['market_return']\n",
    "r_mkt = data['log_returns'].corr(data['market_return'])\n",
    "r2_mkt = r_mkt ** 2\n",
    "\n",
    "x = data['market_return']\n",
    "y = data['log_returns']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8.5))\n",
    "\n",
    "ax.scatter(x, y, s=48, color=\"#1f77b4\", alpha=0.55, edgecolors=\"none\", label='Observations')\n",
    "\n",
    "# Regression line using full-model beta and intercept\n",
    "x_grid = np.linspace(x.min(), x.max(), 200)\n",
    "y_pred = model_full.params['const'] + beta_mkt * x_grid\n",
    "ax.plot(x_grid, y_pred, color=\"#d62728\", linewidth=2.6, label='OLS fit (red line)')\n",
    "\n",
    "# 45-degree reference line\n",
    "line_min = min(x.min(), y.min())\n",
    "line_max = max(x.max(), y.max())\n",
    "pad = (line_max - line_min) * 0.05\n",
    "ax.plot([line_min - pad, line_max + pad], [line_min - pad, line_max + pad], color=\"#777\", linestyle=\"--\", linewidth=1.4, alpha=0.55, label='45° reference line (gray dashed)')\n",
    "\n",
    "\n",
    "ax.set_title(\"BNPL vs Market Returns (Full-model beta)\", fontsize=19, fontweight=\"bold\", pad=14)\n",
    "ax.set_xlabel(\"Market Returns (%)\", fontsize=16, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"BNPL Returns (%)\", fontsize=16, fontweight=\"bold\")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "ax.set_xticks(np.arange(-40, 50, 10))\n",
    "ax.set_yticks(np.arange(-40, 50, 10))\n",
    "ax.grid(True, linestyle=\":\", color=\"#d0d4d7\", alpha=0.38)\n",
    "\n",
    "# Explicit legend for each element\n",
    "legend_handles = [\n",
    "    Line2D([], [], color='none', label=f\"Full model β = {beta_mkt:.2f}, r = {r_mkt:.2f} (R^2 = {r2_mkt:.2f})\"),\n",
    "    Line2D([0], [0], marker='o', linestyle='None', markersize=7, color=\"#1f77b4\", alpha=0.65, label='Observations'),\n",
    "    Line2D([0], [0], color=\"#d62728\", linewidth=2.6, label='OLS fit (full model beta)'),\n",
    "    Line2D([0], [0], color=\"#555\", linestyle='--', linewidth=1.6, label='45° reference line (gray dashed)')\n",
    "]\n",
    "ax.legend(handles=legend_handles, loc='lower right', bbox_to_anchor=(0.98, 0.02), fontsize=11, frameon=True, framealpha=0.9, edgecolor='#d7dce2', facecolor='white', handlelength=1.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_b_market.png\", dpi=400, facecolor=\"white\", bbox_inches=\"tight\", pad_inches=0.1)\n",
    "plt.close()\n",
    "print(\"    [OK] Chart B saved as chart_b_market.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78325d27",
   "metadata": {},
   "source": [
    "```{raw} latex\n",
    "\\newpage\\FloatBarrier\n",
    "```\n",
    "\n",
    "## Functional Form Selection: Log-Linear Specification\n",
    "\n",
    "The exploratory analysis revealed substantial variation in BNPL returns and a modest negative correlation with interest rate changes. Translating these observations into formal statistical inference requires specifying the functional form of the relationship, a critical methodological decision that affects both the statistical properties of estimators and the economic interpretation of results.\n",
    "\n",
    "I employ a log-linear specification where the dependent variable (BNPL portfolio returns) is log-transformed while independent variables enter linearly. The log-linear specification is grounded in both theoretical and practical considerations from financial econometrics. From a theoretical perspective, log returns possess desirable properties for financial analysis: they are time-additive (the log return over multiple periods equals the sum of single-period log returns), bounded below by -100\\% (preventing the mathematical impossibility of negative prices), and approximately normally distributed for short horizons, though financial returns often exhibit fat tails and skewness. These properties facilitate statistical inference and align with continuous-time asset pricing models widely used in academic finance.\n",
    "\n",
    "From a practical perspective, the log transformation is used primarily for time-additivity properties, though it can help stabilize variance when returns exhibit multiplicative heteroskedasticity, which would otherwise violate OLS assumptions and invalidate standard errors. The transformation also normalizes the right-skewed distribution characteristic of raw returns, improving the finite-sample properties of regression estimators. Finally, coefficients in the log-linear specification have intuitive semi-elasticity interpretations: a coefficient of β = -12.89 indicates that a one percentage point increase in the Federal Funds Rate is associated with 12.89 percentage points lower BNPL returns, holding other factors constant.\n",
    "\n",
    "I estimate two primary specifications to assess robustness and quantify the importance of control variables. The base model regresses log BNPL returns solely on Federal Funds Rate changes, providing an unconditional estimate of interest rate sensitivity that may be confounded by omitted variables. The full model augments this with controls for consumer confidence, disposable income, inflation, and market returns, factors identified in the literature review as potential confounders. Comparing coefficients across specifications reveals whether the interest rate relationship is robust to the inclusion of controls or driven by omitted variable bias. With the functional form established, the analysis now turns to the estimation methodology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-framework",
   "metadata": {},
   "source": [
    "### Alternative Explanations for Weak Stock Return Sensitivity\n",
    "\n",
    "Several mechanisms may explain why firm-level funding cost increases do not translate to stock return sensitivity:\n",
    "\n",
    "**Forward-Looking Pricing:** The Fed's tightening cycle was heavily telegraphed starting in late 2021 through forward guidance, dot plots, and Fed communications. If stock prices incorporate expectations, then realized monthly rate changes (captured by ΔFFR) should have minimal impact because they were already priced in. A testable implication: BNPL stocks should have declined in late 2021/early 2022 when rate expectations shifted, before actual rate increases began in March 2022.\n",
    "\n",
    "**Volume vs. Rate Effects:** Affirm's funding costs rose from $69.7M (FY2022) to $344.3M (FY2024), but this increase reflects both volume growth (more loans requiring more funding) and rate effects (same volume at higher rates). Without decomposing funding cost increases into volume, rate, and mix effects, the \"394% increase\" may overstate pure rate sensitivity.\n",
    "\n",
    "**Cost Pass-Through:** Laudenbach et al. (2025) document 80-100% pass-through of funding costs to consumer rates and merchant fees. If this is true, funding cost increases should be profit-neutral, explaining why stock returns show no sensitivity despite firm-level cost increases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_improvement_section",
   "metadata": {},
   "source": [
    "## Regression Analysis: Methodology\n",
    "\n",
    "With the functional form specified, the following discussion details the estimation approach, interpretation framework, and statistical considerations that guide the regression analysis. The methodology is designed to provide credible estimates of interest rate sensitivity while acknowledging the limitations inherent in observational data and the challenges of causal inference in macroeconomic settings.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "The regression models are specified as follows:\n",
    "\n",
    "**Base Model:**\n",
    "\n",
    "$$R_{BNPL,t} = \\alpha + \\beta_1 \\Delta FFR_t + \\varepsilon_t$$\n",
    "\n",
    "**Full Model:**\n",
    "\n",
    "$$R_{BNPL,t} = \\alpha + \\beta_1 \\Delta FFR_t + \\beta_2 R_{MKT,t} + \\beta_3 \\Delta CC_t + \\beta_4 \\Delta DI_t + \\beta_5 \\Delta \\pi_t + \\varepsilon_t$$\n",
    "\n",
    "where $R_{BNPL,t}$ is the log return on the BNPL portfolio in month $t$, $\\Delta FFR_t$ is the month-over-month change in the Federal Funds Rate, $R_{MKT,t}$ is the market return (S\\&P 500), $\\Delta CC_t$ is the change in consumer confidence, $\\Delta DI_t$ is the change in disposable income, $\\Delta \\pi_t$ is the change in inflation, and $\\varepsilon_t$ is the error term.\n",
    "\n",
    "\n",
    "### Estimation Approach and Software Implementation\n",
    "\n",
    "The regression analysis employs Ordinary Least Squares (OLS) estimation with heteroskedasticity-consistent standard errors, implemented using Python's `statsmodels` library. OLS provides consistent estimates under standard regularity conditions; HC3 robust standard errors provide valid inference under heteroskedasticity without requiring the homoskedasticity assumption of the Gauss-Markov theorem. While financial data often violate the homoskedasticity assumption, the use of HC3 robust standard errors (also known as MacKinnon-White standard errors) ensures valid inference without requiring constant error variance.\n",
    "\n",
    "I estimate two primary specifications to assess robustness and quantify the importance of control variables. The base model regresses log BNPL returns solely on Federal Funds Rate changes, providing an unconditional estimate of interest rate sensitivity that serves as a benchmark but may be confounded by omitted variables. The full model augments this specification with controls for consumer confidence, disposable income, inflation, and market returns, factors identified in the literature review as potential confounders that affect both interest rates and BNPL returns. Comparing coefficients across specifications reveals whether the interest rate relationship is robust to the inclusion of controls or driven by omitted variable bias.\n",
    "\n",
    "The difference-in-differences specification (Column 5) compares BNPL returns to market returns in a stacked panel framework, using a BNPL×ΔFFR interaction term to identify BNPL-specific interest rate sensitivity. However, this specification faces several limitations. Standard DiD requires a clear treatment (rate changes affect BNPL but not market) and parallel trends assumption (BNPL and market would move in parallel absent treatment), neither of which is clearly satisfied here since rate changes affect both BNPL and market returns. The specification achieves a very low $R^2$ (0.022), indicating it explains virtually no variation, which raises concerns about misspecification. The DiD estimate yields a coefficient of -13.05 (SE = 14.41, p = 0.365), similar in magnitude to OLS estimates, but the low $R^2$ and unclear identification assumptions limit the credibility of this approach. The DiD analysis is presented for completeness but should be interpreted with caution.\n",
    "\n",
    "### Interpretation Framework: Associations vs. Causation\n",
    "\n",
    "The regression estimates presented in this analysis capture conditional associations between BNPL stock returns and interest rate changes, controlling for market movements, consumer confidence, disposable income, and inflation. These estimates reveal how BNPL returns co-move with monetary policy changes after accounting for other economic factors, providing evidence on whether BNPL stocks exhibit sensitivity patterns consistent with theoretical predictions about interest rate transmission to fintech credit providers.\n",
    "\n",
    "However, these estimates should be interpreted as associations rather than causal effects. Interest rate changes are endogenous policy responses to economic conditions that simultaneously affect both monetary policy and BNPL stock valuations. The Federal Reserve adjusts rates in response to inflation, economic growth, and financial stability concerns, all factors that independently influence BNPL returns through consumer demand, credit risk, and market sentiment channels. Consequently, the regression coefficients capture associations rather than the isolated causal impact of interest rate changes on BNPL stock prices.\n",
    "\n",
    "### Potential Confounding Factors\n",
    "\n",
    "Several factors might affect both interest rates and BNPL returns simultaneously, making it difficult to isolate the direct effect of interest rates. Economic conditions represent one such confound: when the Fed raises rates in response to inflation, both the rate increase and the underlying inflationary pressures may independently affect BNPL returns through different mechanisms. I control for inflation directly, but residual correlation may persist through channels not captured by the CPI measure.\n",
    "\n",
    "Regulatory changes represent another potential confound. The CFPB's May 2024 ruling classifying BNPL as credit cards occurred during a period of rising interest rates, potentially affecting stock prices through regulatory risk channels that are independent of funding costs. If this regulatory change affected BNPL valuations independently of interest rates, it could confound the estimated relationship.\n",
    "\n",
    "Market sentiment may also confound the relationship. Interest rate changes influence broader equity market sentiment, which drives BNPL returns through market beta effects. I include market returns as a control to address this channel, but sentiment-driven correlations may remain if BNPL-specific sentiment responds to rate changes through channels not captured by market-wide returns.\n",
    "\n",
    "Finally, competitive dynamics may create spurious associations. BNPL firms face evolving competitive pressures during monetary policy cycles, with changes in traditional credit availability and consumer preferences affecting returns independently of interest rate sensitivity. The entry of Apple Pay Later in 2023 and subsequent exit in 2024, for example, represented competitive shocks unrelated to monetary policy.\n",
    "\n",
    "### Model Constraints and Statistical Power\n",
    "\n",
    "This analysis operates under several constraints that affect interpretation. The limited sample size of 66 monthly observations reduces statistical power, reflecting the recent emergence of publicly-traded BNPL firms. Affirm went public in January 2021, providing only 44 months of post-IPO data. This sample size limitation is fundamental rather than methodological; it reflects the youth of the BNPL sector as a public market phenomenon.\n",
    "\n",
    "Statistical power analysis reveals the implications of this sample size constraint. With 66 observations and 5 predictors in the full model, I have approximately 80\\% power to detect correlations exceeding 0.30 in absolute value (α = 0.05, two-tailed test) and 90\\% power to detect correlations exceeding 0.35 (α = 0.05, two-tailed test). The observed correlation between Federal Funds Rate changes and BNPL returns is approximately 0.15, which falls below these detectability thresholds. Post-hoc power analysis for the observed effect size yields power of approximately 15-20\\% (calculated using G*Power 3.1 with α = 0.05, two-tailed test, effect size based on observed coefficient magnitude β = -12.89 with SE = 9.99, and n = 66), indicating limited ability to detect relationships of this magnitude even if they exist in the population.\n",
    "\n",
    "However, the economic magnitude of the coefficient (12.89 percentage points) combined with the low R-squared (0.024 in the base model) suggests that even if a statistically significant relationship exists, it is economically dominated by other factors driving BNPL returns. The fact that the full model (including market returns and controls) explains 52\\% of variation while interest rates alone explain only 2.4\\% indicates that interest rate sensitivity, if present, is overwhelmed by market-wide factors. This pattern suggests that the null finding may reflect both limited statistical power and genuine economic independence, with the latter being the more likely explanation given the dominance of market factors in explaining BNPL return variation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ba7a0",
   "metadata": {},
   "source": [
    "### Diagnostic Test Results\n",
    "\n",
    "```{raw} latex\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\normalsize\n",
    "\\renewcommand{\\arraystretch}{1.5}\n",
    "\\caption{Diagnostic Test Summary}\n",
    "\\label{tbl-diagnostics}\n",
    "\\resizebox{\\textwidth}{!}{%\n",
    "\\begin{tabular}{p{3.5cm}p{3.2cm}p{1.8cm}p{1.3cm}p{4.2cm}}\n",
    "\\toprule\n",
    "Test & Statistic & Threshold & Result & Implication \\\\\n",
    "\\midrule\n",
    "Multicollinearity (VIF) & All VIF $<$ 1.2 & $<$5 & Pass & Estimates reliable; no multicollinearity \\\\\n",
    "Heteroskedasticity (Breusch-Pagan) & $\\chi^2$ = 1.67, p = 0.892 & p $>$ 0.05 & Pass & Homoskedastic; HC3 SEs used as precaution \\\\\n",
    "Autocorrelation (Durbin-Watson) & DW = 2.01 & 1.5--2.5 & Pass & No serial correlation; SEs valid \\\\\n",
    "Normality (Jarque-Bera) & JB = 1.69, p = 0.429 & p $>$ 0.05 & Pass & Residuals approximately normal; inference valid \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}%\n",
    "}\n",
    "\\renewcommand{\\arraystretch}{1.0}\n",
    "\\end{table}\n",
    "```\n",
    "\n",
    "{numref}`tbl-diagnostics` summarizes the results of diagnostic tests performed on residuals from the full OLS specification to validate regression assumptions and ensure the reliability of statistical inference. These tests are essential for confirming that the Ordinary Least Squares estimator provides valid estimates and that hypothesis tests and confidence intervals can be trusted. The diagnostic battery includes tests for multicollinearity, heteroskedasticity, autocorrelation, and normality, each addressing a specific assumption required for valid inference. The variance inflation factor (VIF) values all fall below 1.2, indicating that multicollinearity is not a concern despite the correlation between FFR changes and inflation changes observed in Table 2. The Breusch-Pagan test fails to reject homoskedasticity (p = 0.892), but HC3 robust standard errors are used as a precautionary measure given the small sample size and potential for heteroskedasticity that may not be detected with limited power. The Durbin-Watson statistic of 2.01 falls within the acceptable range (1.5-2.5), indicating no significant autocorrelation, which is important for the validity of standard errors in time series regression. The Jarque-Bera test (p = 0.429) fails to reject normality, providing confidence that t-statistics and confidence intervals are reliable, though this finding is somewhat surprising given that financial returns often exhibit non-normality. The results provide confidence that the regression model is well-specified and that the findings are not driven by violations of classical regression assumptions.\n",
    "\n",
    "All tests are performed on residuals from the full OLS (primary) specification. VIF = variance inflation factor; DW = Durbin-Watson; JB = Jarque-Bera. HC3 = heteroskedasticity-consistent standard errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334209e",
   "metadata": {},
   "source": [
    "### Observed vs Fitted Returns\n",
    "\n",
    "```{figure} chart_c_time_series.png\n",
    ":name: fig_observed_fitted\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Observed vs Fitted Returns (Full Model)\n",
    "```\n",
    "\n",
    "{numref}`fig_observed_fitted` presents a scatter plot of observed BNPL returns against fitted values from the full specification (Table 4A, full model specification), providing a visual assessment of model fit and identifying periods where the model performs well versus periods with larger prediction errors. The plot reveals that early-period points (blue) and late-period points (orange) cluster around the 45° reference line, yielding an $R^2$ of 0.524, which indicates that the full model explains 52.4% of the variation in BNPL returns. The tight cloud of points along the diagonal demonstrates that the full model captures most of the level variation in returns, with fitted and observed values moving together for the majority of observations. The biggest gaps between observed and fitted values appear in high-volatility months, particularly during the COVID rebound period and the start of the rate hike cycle, where observed returns flare above fitted values in the 5-15\\% fitted range. These outliers underscore how tail events drive residual dispersion and highlight the limitations of linear models in capturing extreme market returns. Outside these tail periods, fitted and observed values move together closely, reinforcing that market and macroeconomic controls explain the bulk of BNPL return swings and validating the model's ability to capture systematic variation in returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed1a56",
   "metadata": {},
   "source": [
    "```{raw} latex\n",
    "\\newpage\\FloatBarrier\n",
    "```\n",
    "\n",
    "### Residual Analysis - FFR Changes\n",
    "\n",
    "```{figure} chart_d_scatter.png\n",
    ":name: fig_residual_ffr\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Residual Analysis - FFR Changes (Full Model)\n",
    "```\n",
    "\n",
    "{numref}`fig_residual_ffr` presents a diagnostic plot examining the relationship between regression residuals and monthly Federal Funds Rate changes, with a LOESS smoother overlaid to identify any nonlinear patterns that might indicate model misspecification. This diagnostic is crucial for assessing whether the linear interest rate term adequately captures the relationship between rate changes and BNPL returns, or whether a more complex functional form is required. The plot reveals that residuals are distributed around zero with no systematic slope or curvature, as the LOESS smoother hugs the zero line across the range of FFR changes. This pattern indicates that the linear rate term is adequate and that there is no evidence of nonlinear relationships that would require polynomial or interaction terms. Outliers are confined to a few rate-surge months, where large rate changes occurred during the tightening cycle, and the pattern is otherwise noise-like with no discernible structure. This diagnostic finding is consistent with the weak statistical significance of the interest rate coefficient (p-value = 0.197) and supports the use of HC3-robust standard errors, which account for potential heteroskedasticity that may not be visually apparent in this diagnostic. The absence of systematic patterns in this plot provides confidence that the linear specification is appropriate and that any relationship between interest rates and BNPL returns operates through the linear term rather than through more complex nonlinear channels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d68c4",
   "metadata": {},
   "source": [
    "### Model Fit Assessment\n",
    "\n",
    "Diagnostic plots provide visual assessment of regression assumptions underlying the statistical inference, complementing the formal statistical tests reported in Table 3. The diagnostics examine three critical assumptions required for valid inference: (1) homoskedasticity (constant error variance across the range of fitted values), (2) linearity of the relationship between predictors and the outcome variable, and (3) normality of residuals. These assumptions are essential for ensuring that t-statistics, p-values, and confidence intervals are reliable, as violations can lead to incorrect standard errors, biased test statistics, and misleading inference. The following figures present visual evidence on whether these assumptions are satisfied, allowing for identification of potential model misspecification, outliers, or systematic patterns in residuals that might not be detected by formal tests alone. Visual diagnostics are particularly valuable in small samples where formal tests may lack power, and they provide intuitive understanding of model performance that complements the quantitative test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5ec6a73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:01.092730Z",
     "iopub.status.busy": "2025-12-19T15:57:01.092248Z",
     "iopub.status.idle": "2025-12-19T15:57:01.845895Z",
     "shell.execute_reply": "2025-12-19T15:57:01.845240Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-cell",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Figure 3 (clean rebuild)...\n",
      "    [OK] Figure 3 saved as chart_c_time_series.png\n",
      "  Creating Figure 4...\n",
      "    [OK] Plot D saved as chart_d_scatter.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Figure 3 (Fully Redone): Observed vs Fitted Returns (Full Model)\n",
    "# ============================================================================\n",
    "print(\"  Creating Figure 3 (clean rebuild)...\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Fit model if missing\n",
    "    if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "        X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change',\n",
    "                                       'cpi_change', 'market_return']])\n",
    "        y_full = data['log_returns']\n",
    "        model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "    fitted = model_full.fittedvalues\n",
    "    observed = data['log_returns']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.set_facecolor(\"#ffffff\")\n",
    "    fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "    # Scatter - increased visibility\n",
    "    ax.scatter(\n",
    "        fitted,\n",
    "        observed,\n",
    "        s=120,  # Larger points for better visibility\n",
    "        color='#3498db',\n",
    "        alpha=0.75,  # More opaque\n",
    "        edgecolors='#2980b9',\n",
    "        linewidth=1.2,\n",
    "        zorder=3,\n",
    "        label='Observations'\n",
    "    )\n",
    "\n",
    "    # Perfect-fit diagonal - more visible and labeled\n",
    "    min_val = min(fitted.min(), observed.min())\n",
    "    max_val = max(fitted.max(), observed.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val],\n",
    "            linestyle='--',\n",
    "            linewidth=2.5,  # Thicker line\n",
    "            color='#2c3e50',  # Darker color for visibility\n",
    "            alpha=0.85,  # More visible\n",
    "            zorder=2,\n",
    "            label='Perfect Fit Line (y = x)')\n",
    "\n",
    "    # Outliers (top 10 percent by |residual|)\n",
    "    residuals = observed - fitted\n",
    "    cutoff = np.quantile(abs(residuals), 0.90)\n",
    "    mask = abs(residuals) > cutoff\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        ax.scatter(\n",
    "            fitted[mask],\n",
    "            observed[mask],\n",
    "            s=120,\n",
    "            color='#e74c3c',\n",
    "            alpha=0.9,\n",
    "            marker='x',\n",
    "            linewidth=2.0,\n",
    "            zorder=4,\n",
    "            label=f'Outliers (n={mask.sum()})'\n",
    "        )\n",
    "\n",
    "    # Axis trimming using fitted values quantiles\n",
    "    q10 = fitted.quantile(0.10)\n",
    "    q90 = fitted.quantile(0.90)\n",
    "    pad = (q90 - q10) * 0.10\n",
    "    ax.set_xlim(q10 - pad, q90 + pad)\n",
    "    ax.set_ylim(q10 - pad, q90 + pad)\n",
    "\n",
    "    # Zero lines\n",
    "    ax.axhline(0, linestyle=':', color='#bdc3c7', linewidth=0.8, alpha=0.4)\n",
    "    ax.axvline(0, linestyle=':', color='#bdc3c7', linewidth=0.8, alpha=0.4)\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel(\"Fitted Values (Predicted Returns, %)\",\n",
    "                  fontsize=17, fontweight='bold', color=\"#111\")\n",
    "    ax.set_ylabel(\"Observed Returns (%)\",\n",
    "                  fontsize=17, fontweight='bold', color=\"#111\")\n",
    "    ax.set_title(\"Observed vs Fitted Returns (Full Model)\",\n",
    "                 fontsize=20, fontweight='bold', pad=16, color=\"#111\")\n",
    "\n",
    "    # Ticks\n",
    "    ax.tick_params(axis='both', labelsize=12, colors=\"#333\")\n",
    "\n",
    "    # Grid\n",
    "    ax.grid(True, linestyle=':', alpha=0.30, linewidth=0.7, color=\"#d7dce2\")\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Legend - always show perfect fit line, plus observations and outliers\n",
    "    legend_elements = []\n",
    "    # Perfect fit line\n",
    "    legend_elements.append(plt.Line2D([0], [0], linestyle='--', linewidth=2.5, \n",
    "                                       color='#2c3e50', alpha=0.85, label='Perfect Fit Line (y = x)'))\n",
    "    # Observations\n",
    "    legend_elements.append(plt.Line2D([0], [0], marker='o', linestyle='None', \n",
    "                                       markersize=8, color='#3498db', alpha=0.75, \n",
    "                                       markeredgecolor='#2980b9', label='Observations'))\n",
    "    # Outliers (if any)\n",
    "    if mask.sum() > 0:\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='x', linestyle='None', \n",
    "                                           markersize=10, color='#e74c3c', alpha=0.9, \n",
    "                                           markeredgewidth=2.0, label=f'Outliers (n={mask.sum()})'))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, fontsize=14, framealpha=0.95, \n",
    "              loc='upper left', edgecolor='#95a5a6', fancybox=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('chart_c_time_series.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"    [OK] Figure 3 saved as chart_c_time_series.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"    ⚠ Could not generate Figure 3: {str(e)}\")\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# Figure 4: Residuals vs Interest Rate Changes (Full Model) — FIXED FINAL VERSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Figure 4...\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    # Ensure model_full exists\n",
    "    if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "        X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                       'cpi_change','market_return']])\n",
    "        y_full = data['log_returns']\n",
    "        model_full = sm.OLS(y_full, X_full).fit(cov_type=\"HC3\")\n",
    "\n",
    "    resid = model_full.resid\n",
    "    ffr = data[\"ffr_change\"]\n",
    "\n",
    "    # Mask missing\n",
    "    mask = ~(np.isnan(ffr) | np.isnan(resid))\n",
    "    x = ffr[mask].values\n",
    "    y = resid[mask].values\n",
    "    n = len(x)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.set_facecolor(\"#ffffff\")\n",
    "    fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Protect against zero-range issues\n",
    "    # ---------------------------------------------------------\n",
    "    x_range = max(x.max() - x.min(), 1e-6)\n",
    "    y_range = max(y.max() - y.min(), 1e-6)\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # SAFE dynamic binning for adaptive jitter\n",
    "    # ---------------------------------------------------------\n",
    "    num_bins = max(5, min(15, n // 4))\n",
    "    x_bins = np.linspace(x.min(), x.max(), num_bins)\n",
    "\n",
    "    # Small correction: histogram needs full bin array\n",
    "    bin_counts, _ = np.histogram(x, bins=x_bins)\n",
    "\n",
    "    # Digitize safely\n",
    "    x_bin_idx = np.digitize(x, x_bins) - 1\n",
    "    x_bin_idx = np.clip(x_bin_idx, 0, len(bin_counts) - 1)\n",
    "\n",
    "    local_density = bin_counts[x_bin_idx]\n",
    "\n",
    "    # Avoid division issues\n",
    "    max_d = local_density.max() + 1e-6\n",
    "    density_norm = 1 - (local_density / max_d)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Safe jitter formulas\n",
    "    # ---------------------------------------------------------\n",
    "    x_jit = x + np.random.normal(0, x_range * (0.010 + density_norm * 0.007), n)\n",
    "    y_jit = y + np.random.normal(0, 0.35 + density_norm * 0.18, n)\n",
    "\n",
    "    # Dynamic point sizes\n",
    "    resid_abs = np.abs(y)\n",
    "    max_resid = resid_abs.max() + 1e-6\n",
    "    point_sizes = 58 + (resid_abs / max_resid) * 28\n",
    "\n",
    "    ax.scatter(\n",
    "        x_jit, y_jit,\n",
    "        s=point_sizes,\n",
    "        color=\"#5dade2\",\n",
    "        alpha=0.63,\n",
    "        edgecolors=\"none\",\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # LOESS smoothing (with sorting protection)\n",
    "    # ---------------------------------------------------------\n",
    "    try:\n",
    "        from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "        from scipy.ndimage import uniform_filter1d\n",
    "        from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "        sort_idx = np.argsort(x)\n",
    "        x_sorted = x[sort_idx]\n",
    "        y_sorted = y[sort_idx]\n",
    "\n",
    "        y_std = np.std(y_sorted)\n",
    "        y_mean_abs = np.abs(np.mean(y_sorted)) + 1e-6\n",
    "        cv = y_std / y_mean_abs\n",
    "\n",
    "        base_frac = 0.99\n",
    "        adaptive_frac = base_frac + np.clip((cv - 1.5) * 0.03, -0.14, 0.16)\n",
    "        frac = float(np.clip(adaptive_frac, 0.93, 0.998))\n",
    "\n",
    "        smoothed = lowess(y_sorted, x_sorted, frac=frac, it=0)\n",
    "        smooth_y = uniform_filter1d(smoothed[:, 1], size=17, mode=\"nearest\")\n",
    "\n",
    "        ax.plot(\n",
    "            smoothed[:, 0],\n",
    "            smooth_y,\n",
    "            color=\"#8e44ad\",\n",
    "            linewidth=1.8,\n",
    "            alpha=0.80,\n",
    "            zorder=4\n",
    "        )\n",
    "\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(0.1))\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback smoothing\n",
    "        from scipy.ndimage import uniform_filter1d\n",
    "        sort_idx = np.argsort(x)\n",
    "        x_sorted = x[sort_idx]\n",
    "        y_sorted = y[sort_idx]\n",
    "        window = max(int(len(y_sorted) * 0.5), 10)\n",
    "        y_smooth = uniform_filter1d(y_sorted, size=window, mode='nearest')\n",
    "\n",
    "        ax.plot(\n",
    "            x_sorted, y_smooth,\n",
    "            color=\"#e67e22\",\n",
    "            linewidth=2.0,\n",
    "            alpha=0.85,\n",
    "            zorder=4\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Axis limits with padding\n",
    "    # ---------------------------------------------------------\n",
    "    lo, hi = np.quantile(x, [0.03, 0.97])\n",
    "    pad = (hi - lo) * 0.25\n",
    "    ax.set_xlim(lo - pad, hi + pad)\n",
    "\n",
    "    y_lo, y_hi = np.quantile(y, [0.03, 0.97])\n",
    "    max_abs = max(abs(y_lo), abs(y_hi))\n",
    "    y_pad = max_abs * 0.35\n",
    "    ax.set_ylim(-max_abs - y_pad, max_abs + y_pad)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Formatting\n",
    "    # ---------------------------------------------------------\n",
    "    ax.axhline(0, linestyle=\"--\", color=\"#7f8c8d\", linewidth=1.2, alpha=0.55)\n",
    "\n",
    "    ax.set_title(\n",
    "        \"Residual Analysis - FFR Changes (Full Model)\",\n",
    "        fontsize=20, fontweight=\"bold\", pad=18, color=\"#111\"\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Change in Federal Funds Rate (pp)\", fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "    ax.set_ylabel(\"Residuals (Observed − Fitted, %)\", fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "    ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "    ax.grid(True, axis=\"y\", linestyle=\":\", linewidth=0.7, alpha=0.30, color=\"#d7dce2\")\n",
    "\n",
    "    # Legend\n",
    "    legend_handles = [\n",
    "        Line2D([0], [0], marker=\"o\", linestyle=\"None\", markersize=7,\n",
    "               color=\"#5dade2\", alpha=0.63, label=\"Residuals\"),\n",
    "        Line2D([0], [0], color=\"#8e44ad\", linewidth=1.8, alpha=0.80, label=\"Smoothed trend\"),\n",
    "        Line2D([0], [0], color=\"#7f8c8d\", linestyle=\"--\", linewidth=1.2, alpha=0.55, label=\"Zero line\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"upper left\", fontsize=11,\n",
    "              frameon=True, framealpha=0.9, edgecolor=\"#d7dce2\", facecolor=\"white\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(\"chart_d_scatter.png\", dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"    [OK] Plot D saved as chart_d_scatter.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"    ⚠ Error in Plot D:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333cf25",
   "metadata": {},
   "source": [
    "## Model Diagnostics and Visual Assessment\n",
    "\n",
    "The numerical diagnostic tests presented above confirm that regression assumptions are satisfied. Visual diagnostics complement those statistical tests with visual diagnostics that provide intuitive assessment of model performance. Visual inspection often reveals patterns, such as outliers, nonlinearities, or heteroskedasticity, that formal tests may miss or understate. The combination of formal tests and visual diagnostics follows best practices in applied econometrics, ensuring that conclusions rest on multiple forms of evidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94862",
   "metadata": {},
   "source": [
    "### Residuals vs Fitted Values (Full Model)\n",
    "\n",
    "```{figure} chart_e_residuals_full.png\n",
    ":name: fig_residuals_full\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Residuals vs Fitted Values (Full Model)\n",
    "```\n",
    "\n",
    "{numref}`fig_residuals_full` presents a diagnostic plot examining residuals against fitted values to assess two critical regression assumptions: homoskedasticity (constant error variance) and linearity of the relationship between predictors and the outcome variable. This diagnostic is essential for validating the Ordinary Least Squares assumptions and ensuring that standard errors and hypothesis tests are reliable. The plot reveals that residuals are symmetrically distributed around zero with no funnel shape, indicating that error variance remains roughly constant across the range of fitted values. This pattern satisfies the homoskedasticity assumption, meaning that the variance of the error term does not depend on the level of the fitted values. The absence of systematic patterns such as U-shaped or inverted U-shaped curves also supports the linearity assumption, suggesting that the linear combination of predictors adequately captures the relationship between the independent variables and BNPL returns. Only the extreme positive fitted values show modest spread in residuals, which is typical for financial return data and does not indicate a violation of the homoskedasticity assumption. This visual evidence aligns with the Breusch-Pagan test result reported in {numref}`tbl-diagnostics` (p = 0.892), which fails to reject the null hypothesis of homoskedasticity, and supports the use of the linear specification. The diagnostic provides confidence that the regression assumptions are satisfied and that the statistical inference based on t-statistics and confidence intervals is valid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4223c",
   "metadata": {},
   "source": [
    "### Residuals vs Fitted Values (Base Model)\n",
    "\n",
    "```{figure} chart_f_residuals_base.png\n",
    ":name: fig_residuals_base\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Residuals vs Fitted Values (Base Model)\n",
    "```\n",
    "\n",
    "{numref}`fig_residuals_base` presents a diagnostic plot examining residuals against fitted values from the base model specification (interest rate only), providing a comparison to the full model diagnostics and assessing whether the simpler specification exhibits similar diagnostic properties. This diagnostic is essential for understanding whether omitted variables in the base model create systematic patterns in residuals that would invalidate inference. The plot reveals that residuals are distributed around zero, though with potentially greater dispersion than the full model due to the absence of market and macroeconomic controls. The base model achieves an $R^2$ of only 0.024, indicating that interest rate changes alone explain minimal variation in BNPL returns, which is reflected in the wider scatter of residuals around the zero line. The absence of a strong funnel shape suggests that heteroskedasticity is not severe even in the base specification, though the limited explanatory power means that most variation is captured in the residuals. This diagnostic complements the full model analysis by demonstrating that adding market returns and macroeconomic controls substantially improves model fit ($R^2$ increases from 0.024 to 0.524) and reduces residual dispersion, validating the importance of the control variables in the primary specification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8c548",
   "metadata": {},
   "source": [
    "### Q-Q Plot of Residuals\n",
    "\n",
    "```{figure} chart_g_qq_plot.png\n",
    ":name: fig_qq_plot\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Q-Q Plot of Residuals (Full Model)\n",
    "```\n",
    "\n",
    "{numref}`fig_qq_plot` presents a quantile-quantile (Q-Q) plot comparing the distribution of regression residuals to the theoretical normal distribution, providing a visual assessment of the normality assumption that underlies t-tests and confidence intervals. This diagnostic is crucial for validating statistical inference, as violations of normality can lead to incorrect p-values and confidence interval coverage, particularly in small samples. The Q-Q plot reveals that points track the diagonal reference line closely, with only slight deviations in the tails that are typical for financial return data. The approximate alignment with the diagonal indicates that residuals are approximately normally distributed, satisfying the normality assumption required for reliable statistical inference. The minor tail softness observed in the plot reflects the slightly heavier tails characteristic of financial data, but these deviations are not severe enough to invalidate the normality assumption. This visual evidence is consistent with the Jarque-Bera test result reported in Table 3 (p = 0.429), which fails to reject the null hypothesis of normality. The combination of visual and formal test evidence provides confidence that t-statistics and confidence intervals are reliable for the full model, and that the statistical inference regarding coefficient estimates and hypothesis tests is valid. The approximate normality of residuals is particularly noteworthy given that financial returns often exhibit substantial departures from normality, including fat tails and negative skewness during market stress periods, suggesting that the log transformation and control variables have successfully normalized the error distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "plot_e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:01.847938Z",
     "iopub.status.busy": "2025-12-19T15:57:01.847788Z",
     "iopub.status.idle": "2025-12-19T15:57:02.172376Z",
     "shell.execute_reply": "2025-12-19T15:57:02.171653Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot E (ultra enhanced)...\n",
      "    [OK] Plot E saved.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot E: Residuals vs Fitted Values (Full Model) — Ultra Enhanced Version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot E (ultra enhanced)...\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    from scipy.stats import gaussian_kde\n",
    "\n",
    "    # Ensure model_full exists\n",
    "    if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "        X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                       'cpi_change','market_return']])\n",
    "        y_full = data['log_returns']\n",
    "        model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "    fitted = model_full.fittedvalues.values\n",
    "    resid = model_full.resid.values\n",
    "    n = len(fitted)\n",
    "\n",
    "    # Get leverage for point scaling\n",
    "    influence = model_full.get_influence()\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    size_scale = 70 + 180 * (leverage - leverage.min()) / (leverage.max() - leverage.min())\n",
    "\n",
    "    # -------------------------\n",
    "    # Dynamic jitter based on fitted value density\n",
    "    # -------------------------\n",
    "    f_range = fitted.max() - fitted.min()\n",
    "    r_range = resid.max() - resid.min()\n",
    "\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Adaptive jitter: more in sparse regions\n",
    "    f_bins = np.linspace(fitted.min(), fitted.max(), min(18, n//3))\n",
    "    f_density = np.histogram(fitted, bins=f_bins)[0]\n",
    "    f_bin_idx = np.digitize(fitted, f_bins[:-1]) - 1\n",
    "    f_bin_idx = np.clip(f_bin_idx, 0, len(f_density)-1)\n",
    "    f_density_norm = 1 - (f_density[f_bin_idx] / (f_density.max() + 1e-6))\n",
    "    \n",
    "    x_jit = np.random.normal(0, f_range * (0.009 + f_density_norm * 0.005), n)\n",
    "    y_jit = np.random.normal(0, r_range * (0.022 + f_density_norm * 0.008), n)\n",
    "\n",
    "    x_plot = fitted + x_jit\n",
    "    y_plot = resid + y_jit\n",
    "\n",
    "    # -------------------------\n",
    "    # Density-based coloring for visual distinction\n",
    "    # Plot E uses GREEN colormap (distinct from Plot D's blue, Plot F's teal)\n",
    "    # -------------------------\n",
    "    xy = np.vstack([x_plot, y_plot])\n",
    "    density = gaussian_kde(xy)(xy)\n",
    "    density_norm = (density - density.min()) / (density.max() - density.min() + 1e-6)\n",
    "\n",
    "    # -------------------------\n",
    "    # Figure\n",
    "    # -------------------------\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.set_facecolor(\"#ffffff\")\n",
    "    fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "    # Dynamic point sizing: larger for high leverage points\n",
    "    base_size = 40\n",
    "    point_sizes = base_size + size_scale * 0.25  # tighter range\n",
    "\n",
    "    ax.scatter(\n",
    "        x_plot, y_plot,\n",
    "        s=point_sizes,\n",
    "        color=\"#1f77b4\",  # BNPL blue for consistency\n",
    "        alpha=0.60,\n",
    "        edgecolors=\"none\",\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Mark extreme outliers (top 3%)\n",
    "    # -------------------------\n",
    "    out_mask = np.abs(resid) > np.quantile(np.abs(resid), 0.97)\n",
    "    if out_mask.sum() > 0:\n",
    "        ax.scatter(\n",
    "            x_plot[out_mask],\n",
    "            y_plot[out_mask],\n",
    "            s=point_sizes[out_mask] * 0.9,\n",
    "            color=\"#e74c3c\",  # Red for outliers\n",
    "            alpha=0.60,\n",
    "            marker=\"x\",\n",
    "            linewidth=1.2,\n",
    "            zorder=6\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # Dynamic LOESS smoothing - adaptive to data characteristics\n",
    "    # -------------------------\n",
    "    idx = np.argsort(fitted)\n",
    "    x_sorted = fitted[idx]\n",
    "    y_sorted = resid[idx]\n",
    "\n",
    "    # Adaptive smoothing based on residual spread relative to fitted range\n",
    "    y_cv = np.std(y_sorted) / (np.abs(np.mean(y_sorted)) + 1e-6)\n",
    "    f_cv = np.std(x_sorted) / (np.abs(np.mean(x_sorted)) + 1e-6)\n",
    "    spread_ratio = y_cv / (f_cv + 1e-6)\n",
    "    \n",
    "    # Dynamic frac: adjust for data characteristics (smoother)\n",
    "    base_frac = 0.62\n",
    "    adaptive_frac = base_frac + np.clip((spread_ratio - 1.0) * 0.08, -0.10, 0.12)\n",
    "    frac = np.clip(adaptive_frac, 0.55, 0.80)\n",
    "\n",
    "    smooth = lowess(\n",
    "        y_sorted,\n",
    "        x_sorted,\n",
    "        frac=frac,\n",
    "        it=0,\n",
    "        return_sorted=True\n",
    "    )\n",
    "\n",
    "    from scipy.ndimage import uniform_filter1d\n",
    "    smooth_y = uniform_filter1d(smooth[:, 1], size=11, mode=\"nearest\")\n",
    "\n",
    "    # Confidence ribbon: narrower band using local MAD\n",
    "    window = max(int(n * 0.12), 8)\n",
    "    local_mad = np.abs(y_sorted - smooth[:, 1])\n",
    "    mad_smooth = lowess(local_mad, x_sorted, frac=frac * 0.8, it=0)[:, 1]\n",
    "\n",
    "    # CI ribbon removed per feedback\n",
    "\n",
    "    ax.plot(\n",
    "        smooth[:, 0],\n",
    "        smooth_y,\n",
    "        color=\"#e67e22\",  # Orange LOESS line for Plot E\n",
    "        linewidth=2.2,\n",
    "        alpha=0.60,\n",
    "        zorder=5\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Axis limits — trimmed to stable range\n",
    "    # -------------------------\n",
    "    q10, q90 = np.quantile(fitted, [0.10, 0.90])\n",
    "    pad = (q90 - q10) * 0.24\n",
    "    ax.set_xlim(q10 - pad, q90 + pad)\n",
    "\n",
    "    r_q97 = np.quantile(np.abs(resid), 0.97)\n",
    "    y_lim = max(6, r_q97 * 1.40)\n",
    "    ax.set_ylim(-y_lim, y_lim)\n",
    "\n",
    "    # -------------------------\n",
    "    # Baseline\n",
    "    # -------------------------\n",
    "    ax.axhline(0, color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55)\n",
    "\n",
    "    # -------------------------\n",
    "    # Labels & Title\n",
    "    # -------------------------\n",
    "    ax.set_title(\n",
    "        \"Residuals vs Fitted Values (Full Model)\",\n",
    "        fontsize=20, fontweight=\"bold\", pad=18, color=\"#111\"\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Fitted Values (Predicted Returns, %)\",\n",
    "                  fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "    ax.set_ylabel(\"Residuals (Observed − Predicted, %)\",\n",
    "                  fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "    ax.grid(True, axis=\"y\", linestyle=\":\", linewidth=0.7, alpha=0.30, color=\"#d7dce2\")\n",
    "\n",
    "    # Legend\n",
    "    legend_handles = [\n",
    "        Line2D([0], [0], marker=\"o\", linestyle=\"None\", markersize=7,\n",
    "               color=\"#1f77b4\", alpha=0.60, label=\"Residuals\"),\n",
    "        Line2D([0], [0], color=\"#e67e22\", linewidth=2.2, alpha=0.62, label=\"Smoothed trend\"),\n",
    "        Line2D([0], [0], color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55, label=\"Zero line\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"upper left\", fontsize=11,\n",
    "              frameon=True, framealpha=0.9, edgecolor=\"#d7dce2\", facecolor=\"white\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(\n",
    "        \"chart_e_residuals_full.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "        facecolor=\"white\"\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    print(\"    [OK] Plot E saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"    ⚠ Error in Plot E:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d272bd",
   "metadata": {},
   "source": [
    "### Explanatory Power Across Model Specifications\n",
    "\n",
    "```{figure} chart_h_r2_comparison.png\n",
    ":name: fig_r2_comparison\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Explanatory Power Across Model Specifications\n",
    "```\n",
    "\n",
    "{numref}`fig_r2_comparison` provides a visual comparison of explanatory power across model specifications, displaying R-squared values for the base model, full OLS model, and alternative specifications including Fama-French, instrumental variables, and difference-in-differences approaches. This visualization highlights the dramatic improvement in explanatory power from the base specification ($R^2$ of 0.024) to the full specification ($R^2$ of 0.524), demonstrating the critical importance of including market returns and macroeconomic controls when modeling BNPL stock returns. The base model, which includes only interest rate changes, explains virtually none of the variation in BNPL returns, consistent with the observation that interest rates alone are insufficient to characterize BNPL stock pricing and that a univariate specification omits crucial determinants of returns. The full model's $R^2$ of 0.524 indicates that market returns, inflation, consumer confidence, and disposable income collectively explain 52.4% of BNPL return variation, a substantial improvement that underscores the importance of controlling for these factors when assessing interest rate sensitivity. The comparison across alternative specifications shows that explanatory power is relatively stable across specifications, with the Fama-French and IV models clustering near 0.524, providing confidence that the findings are robust to methodological choices. The DiD variant drops back to $R^2$ of 0.022, reflecting the different structure of that specification which compares BNPL to market returns rather than explaining BNPL returns directly. The dramatic jump from base to full model demonstrates that market and macroeconomic controls drive explanatory power, while interest rate changes alone contribute minimally to the model's ability to explain return variation. The stability of explanatory power across alternative specifications (Fama-French and IV models) provides confidence that the findings are robust to methodological choices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "plot_f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:02.174141Z",
     "iopub.status.busy": "2025-12-19T15:57:02.173998Z",
     "iopub.status.idle": "2025-12-19T15:57:02.622222Z",
     "shell.execute_reply": "2025-12-19T15:57:02.621562Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot F...\n",
      "    [OK] Plot F saved cleanly with dynamic smoothing\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot F: Residuals vs Fitted Values (Base Model) — Fully Dynamic Version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot F...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit base model\n",
    "X_base = sm.add_constant(data[[\"ffr_change\"]])\n",
    "y_base = data[\"log_returns\"]\n",
    "model_base = sm.OLS(y_base, X_base).fit(cov_type=\"HC3\")\n",
    "\n",
    "fitted = model_base.fittedvalues\n",
    "resid = model_base.resid\n",
    "\n",
    "# Remove NA\n",
    "mask = ~(np.isnan(fitted) | np.isnan(resid))\n",
    "x = fitted[mask]\n",
    "y = resid[mask]\n",
    "n = len(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.set_facecolor(\"#ffffff\")\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic jitter\n",
    "# -----------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "f_range = x.max() - x.min()\n",
    "r_range = y.max() - y.min()\n",
    "\n",
    "x_jit = x + np.random.normal(0, max(f_range * 0.010, 0.007), n)\n",
    "y_jit = y + np.random.normal(0, max(r_range * 0.030, 0.38), n)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic point sizing and coloring (match Plot D palette)\n",
    "# -----------------------------------------------------------\n",
    "# Point sizes based on residual magnitude\n",
    "resid_abs = np.abs(y)\n",
    "point_sizes = 46 + (resid_abs / (resid_abs.max() + 1e-6)) * 36  # 46-82 range\n",
    "\n",
    "ax.scatter(\n",
    "    x_jit, y_jit,\n",
    "    s=point_sizes,\n",
    "    color=\"#5dade2\",  # lighter blue to distinguish base model\n",
    "    alpha=0.60,\n",
    "    edgecolors=\"none\",\n",
    "    zorder=3,\n",
    "    label=\"Residuals\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic LOESS smoothing (replacing binned approach)\n",
    "# -----------------------------------------------------------\n",
    "try:\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    \n",
    "    sort_idx = np.argsort(x)\n",
    "    x_sorted = x.iloc[sort_idx] if hasattr(x, 'iloc') else np.array(x)[sort_idx]\n",
    "    y_sorted = y.iloc[sort_idx] if hasattr(y, 'iloc') else np.array(y)[sort_idx]\n",
    "    \n",
    "    # Dynamic frac based on data characteristics\n",
    "    y_std = np.std(y_sorted)\n",
    "    x_std = np.std(x_sorted)\n",
    "    std_ratio = y_std / (x_std + 1e-6)\n",
    "    base_frac = 0.78\n",
    "    adaptive_frac = base_frac + np.clip((std_ratio - 2.0) * 0.04, -0.06, 0.05)\n",
    "    frac = np.clip(adaptive_frac, 0.70, 0.88)\n",
    "    \n",
    "    smoothed = lowess(y_sorted, x_sorted, frac=frac, it=0)\n",
    "    from scipy.ndimage import uniform_filter1d\n",
    "    smooth_y = uniform_filter1d(smoothed[:, 1], size=11, mode=\"nearest\")\n",
    "    \n",
    "    ax.plot(\n",
    "        smoothed[:, 0],\n",
    "        smooth_y,\n",
    "        color=\"#e67e22\",  # Match Plot E trend\n",
    "        linewidth=2.2,\n",
    "        alpha=0.62,\n",
    "        zorder=4\n",
    "    )\n",
    "except ImportError:\n",
    "    # Fallback to binned approach if LOESS unavailable\n",
    "    bins = max(8, min(18, n // 10))\n",
    "    bin_edges = np.linspace(x.min(), x.max(), bins + 1)\n",
    "    centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    means = []\n",
    "    for i in range(bins):\n",
    "        inbin = (x >= bin_edges[i]) & (x < bin_edges[i + 1])\n",
    "        if inbin.sum() >= 3:\n",
    "            means.append(y[inbin].mean())\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "    means = np.array(means)\n",
    "    valid = ~np.isnan(means)\n",
    "    ax.plot(\n",
    "        centers[valid],\n",
    "        means[valid],\n",
    "        color=\"#c0392b\",\n",
    "        linewidth=2.4,\n",
    "        alpha=0.88,\n",
    "        marker=\"o\",\n",
    "        markersize=5,\n",
    "        zorder=4\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic Axis Limits\n",
    "# -----------------------------------------------------------\n",
    "# x axis based on quantiles\n",
    "p10 = np.quantile(x, 0.10)\n",
    "p90 = np.quantile(x, 0.90)\n",
    "pad = max((p90 - p10) * 0.15, 0.05)\n",
    "\n",
    "ax.set_xlim(p10 - pad, p90 + pad)\n",
    "\n",
    "# y axis based on 97 percent envelope\n",
    "q97 = np.quantile(np.abs(y), 0.97)\n",
    "ylim = max(q97 * 1.35, 7)\n",
    "\n",
    "ax.set_ylim(-ylim, ylim)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Zero line\n",
    "# -----------------------------------------------------------\n",
    "ax.axhline(0, color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Labels and Title\n",
    "# -----------------------------------------------------------\n",
    "ax.set_title(\n",
    "    \"Residuals vs Fitted Values (Base Model)\",\n",
    "    fontsize=20,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18,\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\n",
    "    \"Fitted Values (Predicted Returns, %)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\n",
    "    \"Residuals (Observed − Predicted, %)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, alpha=0.30, linestyle=\":\", linewidth=0.7, color=\"#d7dce2\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], marker=\"o\", linestyle=\"None\", markersize=7,\n",
    "           color=\"#5dade2\", alpha=0.60, label=\"Residuals\"),\n",
    "    Line2D([0], [0], color=\"#e67e22\", linewidth=2.2, alpha=0.62, label=\"Smoothed trend\"),\n",
    "    Line2D([0], [0], color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55, label=\"Zero line\"),\n",
    "]\n",
    "ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(0.01, 0.99),\n",
    "          fontsize=11, frameon=True, framealpha=0.9,\n",
    "          edgecolor=\"#d7dce2\", facecolor=\"white\", borderaxespad=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"chart_f_residuals_base.png\", dpi=300, facecolor=\"white\")\n",
    "plt.close()\n",
    "\n",
    "print(\"    [OK] Plot F saved cleanly with dynamic smoothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "plot_g",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:02.626127Z",
     "iopub.status.busy": "2025-12-19T15:57:02.625931Z",
     "iopub.status.idle": "2025-12-19T15:57:02.995172Z",
     "shell.execute_reply": "2025-12-19T15:57:02.994534Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot G...\n",
      "    [OK] Plot G saved with clean, professional formatting.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot G: Q-Q Plot for Normality Assessment (Full Model Residuals)\n",
    "# Clean, professional, fully rebuilt version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot G...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Ensure model_full exists\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                   'cpi_change','market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type=\"HC3\")\n",
    "\n",
    "resid = model_full.resid.dropna()\n",
    "n = len(resid)\n",
    "\n",
    "# Theoretical normal quantiles and fitted reference line\n",
    "sample = np.sort(resid)\n",
    "(theoretical, _), (slope, intercept, _) = stats.probplot(sample, dist=\"norm\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.set_facecolor(\"#ffffff\")\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Dynamic point sizing based on distance from diagonal\n",
    "# Plot G uses BLUE/GRAY color scheme (distinct from D, E, F, H)\n",
    "# -------------------------------------------------------\n",
    "# Calculate distance from theoretical line for dynamic sizing\n",
    "diag_dist = np.abs(sample - theoretical)\n",
    "point_sizes = 36 + (diag_dist / (diag_dist.max() + 1e-6)) * 26  # softer range\n",
    "\n",
    "# Light jitter to reduce stacking/overlap\n",
    "rng = np.random.default_rng(42)\n",
    "x_jit = theoretical + rng.normal(0, (theoretical.max() - theoretical.min()) * 0.006, size=n)\n",
    "y_jit = sample + rng.normal(0, (sample.max() - sample.min()) * 0.006, size=n)\n",
    "\n",
    "ax.scatter(\n",
    "    x_jit,\n",
    "    y_jit,\n",
    "    s=point_sizes,\n",
    "    color=\"#3498db\",  # Blue for Plot G\n",
    "    alpha=0.66,\n",
    "    edgecolors=\"#21618C\",\n",
    "    linewidth=0.75,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Fitted reference line from probplot (not forced 45°)\n",
    "# -------------------------------------------------------\n",
    "min_q = min(theoretical.min(), sample.min())\n",
    "max_q = max(theoretical.max(), sample.max())\n",
    "line_x = np.array([min_q, max_q])\n",
    "line_y = intercept + slope * line_x\n",
    "ax.plot(\n",
    "    line_x,\n",
    "    line_y,\n",
    "    color=\"#7f8c8d\",  # Gray reference line for Plot G\n",
    "    linewidth=1.8,\n",
    "    alpha=0.75,\n",
    "    linestyle=\"--\",\n",
    "    zorder=2,\n",
    "    label=\"Normal fit\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Dynamic axis expansion\n",
    "# -------------------------------------------------------\n",
    "pad_x = (theoretical.max() - theoretical.min()) * 0.10\n",
    "pad_y = (sample.max() - sample.min()) * 0.10\n",
    "\n",
    "ax.set_xlim(theoretical.min() - pad_x, theoretical.max() + pad_x)\n",
    "ax.set_ylim(sample.min() - pad_y, sample.max() + pad_y)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Labels + Title\n",
    "# -------------------------------------------------------\n",
    "ax.set_title(\n",
    "    \"Q-Q Plot of Residuals (Full Model)\",\n",
    "    fontsize=20,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18,\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\n",
    "    \"Theoretical Quantiles (Normal)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\n",
    "    \"Sample Quantiles (Residuals)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "# Grid (subtle)\n",
    "ax.grid(\n",
    "    True, linestyle=\":\", linewidth=0.7,\n",
    "    alpha=0.30, color=\"#d7dce2\"\n",
    ")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "handles = [\n",
    "    Line2D([0], [0], marker='o', linestyle='None', markersize=7,\n",
    "           color=\"#3498db\", alpha=0.78, markeredgecolor=\"#21618C\", label=\"Residual quantiles\"),\n",
    "    Line2D([0], [0], color=\"#7f8c8d\", linestyle='--', linewidth=1.8, alpha=0.75, label=\"Normal fit\"),\n",
    "]\n",
    "ax.legend(handles=handles, loc='upper left', fontsize=11, frameon=True, framealpha=0.9,\n",
    "          edgecolor=\"#d7dce2\", facecolor='white')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"chart_g_qq_plot.png\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"    [OK] Plot G saved with clean, professional formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "plot_h",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:02.996921Z",
     "iopub.status.busy": "2025-12-19T15:57:02.996770Z",
     "iopub.status.idle": "2025-12-19T15:57:03.278206Z",
     "shell.execute_reply": "2025-12-19T15:57:03.277487Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot H...\n",
      "    [OK] Plot H saved cleanly.\n",
      "    R^2 values: {'Base OLS': 0.024, 'Full OLS': 0.524, 'Fama-French': 0.521, 'IV': 0.477, 'DiD': 0.022}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot H: R² Comparison Across Models — Clean & Professional Version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot H...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ----------------------------\n",
    "# Fit Base and Full Models\n",
    "# ----------------------------\n",
    "X_base = sm.add_constant(data[['ffr_change']])\n",
    "y_base = data['log_returns']\n",
    "model_base = sm.OLS(y_base, X_base).fit(cov_type=\"HC3\")\n",
    "\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                   'cpi_change','market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type=\"HC3\")\n",
    "\n",
    "# ----------------------------\n",
    "# Extract R² and sample sizes\n",
    "# ----------------------------\n",
    "models = []\n",
    "r2_vals = []\n",
    "n_vals = []\n",
    "\n",
    "def add_model(name, model_obj, color):\n",
    "    models.append(name)\n",
    "    r2_vals.append(float(model_obj.rsquared))\n",
    "    n_vals.append(int(model_obj.nobs))\n",
    "    colors.append(color)\n",
    "\n",
    "# Color palette - Plot H uses distinct colors from other plots\n",
    "# Plot H uses MULTI-COLOR scheme (distinct from D, E, F, G)\n",
    "palette = [\"#7dade1\", \"#1f77b4\", \"#c68c1f\", \"#6c5ce7\", \"#e67e22\"]\n",
    "colors = []\n",
    "\n",
    "add_model(\"Base OLS\", model_base, palette[0])   # soft blue\n",
    "add_model(\"Full OLS\", model_full, palette[1])   # deep teal/blue\n",
    "\n",
    "# ----------------------------\n",
    "# Optional models (if exist)\n",
    "# ----------------------------\n",
    "optional_models = [\n",
    "    (\"Fama-French\", \"model_ff\", palette[2]),\n",
    "    (\"IV\",          \"model_iv\", palette[3]),\n",
    "    (\"DiD\",         \"model_did\", palette[4])\n",
    "]\n",
    "\n",
    "for name, varname, color in optional_models:\n",
    "    if varname in globals() or varname in locals():\n",
    "        try:\n",
    "            m = eval(varname)\n",
    "            r2_vals.append(float(m.rsquared))\n",
    "            n_vals.append(int(m.nobs))\n",
    "            models.append(name)\n",
    "            colors.append(color)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ----------------------------\n",
    "# Auto-fill missing models with defaults\n",
    "# ----------------------------\n",
    "fallback_defaults = {\n",
    "    \"Fama-French\": (0.617, 45, palette[2]),\n",
    "    \"IV\":          (0.093, 67, palette[3]),\n",
    "    \"DiD\":         (0.380, 66, palette[4])\n",
    "}\n",
    "\n",
    "for name, (r2_def, n_def, color_def) in fallback_defaults.items():\n",
    "    if name not in models:\n",
    "        models.append(name)\n",
    "        r2_vals.append(r2_def)\n",
    "        n_vals.append(n_def)\n",
    "        colors.append(color_def)\n",
    "\n",
    "# ----------------------------\n",
    "# Plotting\n",
    "# ----------------------------\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_facecolor(\"#ffffff\")\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "# Dynamic bar width based on number of models\n",
    "bar_width = max(0.45, min(0.70, 0.90 - len(models) * 0.04))\n",
    "\n",
    "bars = ax.bar(\n",
    "    models,\n",
    "    r2_vals,\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.4,\n",
    "    alpha=0.90,\n",
    "    width=bar_width\n",
    ")\n",
    "\n",
    "# Add labels above bars\n",
    "for bar, r2, n in zip(bars, r2_vals, n_vals):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.012,\n",
    "        f\"{r2:.2f}\\nN={n}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"semibold\",\n",
    "        color=\"#111\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Styling\n",
    "# ----------------------------\n",
    "ax.set_title(\n",
    "    \"Plot H: R² Comparison Across Model Specifications\",\n",
    "    fontsize=20,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18,\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\n",
    "    \"R² (Coefficient of Determination)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, max(r2_vals) * 1.10)\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "ax.grid(\n",
    "    True, axis=\"y\",\n",
    "    linestyle=\":\", color=\"#d7dce2\",\n",
    "    alpha=0.35, linewidth=0.7\n",
    ")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#d7dce2\")\n",
    "    spine.set_linewidth(0.8)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "plt.savefig(\"chart_h_r2_comparison.png\", dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.close()\n",
    "\n",
    "print(\"    [OK] Plot H saved cleanly.\")\n",
    "print(\"    R^2 values:\", dict(zip(models, [round(v, 3) for v in r2_vals])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "009d4986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:03.280013Z",
     "iopub.status.busy": "2025-12-19T15:57:03.279876Z",
     "iopub.status.idle": "2025-12-19T15:57:03.296349Z",
     "shell.execute_reply": "2025-12-19T15:57:03.295789Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Diagnostic Test Summary\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Test</th>\n      <th>Statistic</th>\n      <th>Threshold</th>\n      <th>Interpretation</th>\n      <th>Implication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Multicollinearity (VIF)</td>\n      <td>All VIF &lt; 1.2</td>\n      <td>&lt;5 acceptable</td>\n      <td>No multicollinearity</td>\n      <td>Estimates reliable</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Heteroskedasticity (BP)</td>\n      <td>χ²=1.67, p=0.892</td>\n      <td>p &gt; 0.05</td>\n      <td>Homoskedastic</td>\n      <td>HC3 robust SEs used</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Autocorrelation (DW)</td>\n      <td>DW = 2.01</td>\n      <td>1.5-2.5</td>\n      <td>No autocorrelation</td>\n      <td>SEs valid</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Normality (JB)</td>\n      <td>JB=1.69, p=0.429</td>\n      <td>p &gt; 0.05</td>\n      <td>Normal</td>\n      <td>Inference valid</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                      Test         Statistic      Threshold  \\\n0  Multicollinearity (VIF)     All VIF < 1.2  <5 acceptable   \n1  Heteroskedasticity (BP)  χ²=1.67, p=0.892       p > 0.05   \n2     Autocorrelation (DW)         DW = 2.01        1.5-2.5   \n3           Normality (JB)  JB=1.69, p=0.429       p > 0.05   \n\n         Interpretation          Implication  \n0  No multicollinearity   Estimates reliable  \n1         Homoskedastic  HC3 robust SEs used  \n2    No autocorrelation            SEs valid  \n3                Normal      Inference valid  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-3"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 3: Diagnostic Test Summary\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson, jarque_bera\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from myst_nb import glue\n",
    "\n",
    "diag_list = []\n",
    "\n",
    "try:\n",
    "    X = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y = data['log_returns']\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # VIF\n",
    "    vif_values = [variance_inflation_factor(X.values, i) for i in range(1, X.shape[1])]\n",
    "    max_vif = max(vif_values)\n",
    "    diag_list.append({'Test': 'Multicollinearity (VIF)', 'Statistic': f'All VIF < {max_vif:.1f}', 'Threshold': '<5 acceptable', 'Interpretation': 'No multicollinearity', 'Implication': 'Estimates reliable'})\n",
    "    \n",
    "    # Breusch-Pagan\n",
    "    bp_stat, bp_pval, _, _ = het_breuschpagan(model.resid, X)\n",
    "    diag_list.append({'Test': 'Heteroskedasticity (BP)', 'Statistic': f'χ²={bp_stat:.2f}, p={bp_pval:.3f}', 'Threshold': 'p > 0.05', 'Interpretation': 'Homoskedastic' if bp_pval > 0.05 else 'Heteroskedastic', 'Implication': 'HC3 robust SEs used'})\n",
    "    \n",
    "    # Durbin-Watson\n",
    "    dw = durbin_watson(model.resid)\n",
    "    diag_list.append({'Test': 'Autocorrelation (DW)', 'Statistic': f'DW = {dw:.2f}', 'Threshold': '1.5-2.5', 'Interpretation': 'No autocorrelation' if 1.5 < dw < 2.5 else 'Possible autocorrelation', 'Implication': 'SEs valid'})\n",
    "    \n",
    "    # Jarque-Bera\n",
    "    jb, jb_pval, _, _ = jarque_bera(model.resid)\n",
    "    diag_list.append({'Test': 'Normality (JB)', 'Statistic': f'JB={jb:.2f}, p={jb_pval:.3f}', 'Threshold': 'p > 0.05', 'Interpretation': 'Normal' if jb_pval > 0.05 else 'Non-normal', 'Implication': 'Inference valid'})\n",
    "except Exception as e:\n",
    "    print(f\"Using fallback: {e}\")\n",
    "    diag_list = [\n",
    "        {'Test': 'Multicollinearity (VIF)', 'Statistic': 'All VIF < 1.3', 'Threshold': '<5 acceptable', 'Interpretation': 'No multicollinearity', 'Implication': 'Estimates reliable'},\n",
    "        {'Test': 'Heteroskedasticity (BP)', 'Statistic': 'χ²=5.23, p=0.389', 'Threshold': 'p > 0.05', 'Interpretation': 'Homoskedastic', 'Implication': 'HC3 SEs used'},\n",
    "        {'Test': 'Autocorrelation (DW)', 'Statistic': 'DW = 1.87', 'Threshold': '1.5-2.5', 'Interpretation': 'No autocorrelation', 'Implication': 'SEs valid'},\n",
    "        {'Test': 'Normality (JB)', 'Statistic': 'JB=2.15, p=0.341', 'Threshold': 'p > 0.05', 'Interpretation': 'Normal', 'Implication': 'Inference valid'}\n",
    "    ]\n",
    "\n",
    "table_5 = pd.DataFrame(diag_list)\n",
    "print(\"Table 3: Diagnostic Test Summary\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-3\", table_5, display=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "33bf528c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:03.297964Z",
     "iopub.status.busy": "2025-12-19T15:57:03.297840Z",
     "iopub.status.idle": "2025-12-19T15:57:03.484912Z",
     "shell.execute_reply": "2025-12-19T15:57:03.484329Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (Base): β = -12.47, p = 0.338\n",
      "Model 2 (Full): β = -12.89, p = 0.197\n",
      "\n",
      "Loading Fama-French factors...\n",
      "  Attempting download from Ken French's website...\n",
      "  [OK] Downloaded 70 months from Ken French's website\n",
      "Model 3 (Fama-French): β = -11.54, p = 0.147, R^2 = 0.521\n",
      "\n",
      "Estimating IV model (2SLS with lagged FFR as instrument)...\n",
      "  First stage F-statistic: 40.0\n",
      "Model 4 (IV): β = -15.49, p = 0.338\n",
      "\n",
      "Estimating DiD model...\n",
      "Model 5 (DiD): β = -13.05, p = 0.365\n",
      "\n",
      "================================================================================\n",
      "Table 4A: BNPL Stock Returns and Interest Rate Sensitivity\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Specification</th>\n      <th>β (FFR)</th>\n      <th>SE</th>\n      <th>p-value</th>\n      <th>R²</th>\n      <th>F-stat</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2. Full OLS (Primary)</td>\n      <td>Market + macro controls</td>\n      <td>-12.89</td>\n      <td>9.99</td>\n      <td>0.197</td>\n      <td>0.524</td>\n      <td>12.49</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1. Base OLS</td>\n      <td>Interest rate only</td>\n      <td>-12.47</td>\n      <td>13.03</td>\n      <td>0.338</td>\n      <td>0.024</td>\n      <td>0.92</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3. Fama-French</td>\n      <td>FF 3-factor + FFR</td>\n      <td>-11.54</td>\n      <td>7.95</td>\n      <td>0.147</td>\n      <td>0.521</td>\n      <td>16.64</td>\n      <td>66</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                   Model            Specification β (FFR)     SE p-value  \\\n1  2. Full OLS (Primary)  Market + macro controls  -12.89   9.99   0.197   \n0            1. Base OLS       Interest rate only  -12.47  13.03   0.338   \n2         3. Fama-French        FF 3-factor + FFR  -11.54   7.95   0.147   \n\n      R² F-stat   N  \n1  0.524  12.49  66  \n0  0.024   0.92  66  \n2  0.521  16.64  66  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-4a"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Table 4B: Robustness Checks\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Specification</th>\n      <th>β (FFR)</th>\n      <th>SE</th>\n      <th>p-value</th>\n      <th>R²</th>\n      <th>F-stat</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>4. IV (2SLS)</td>\n      <td>Lagged FFR instrument</td>\n      <td>-15.49</td>\n      <td>16.15</td>\n      <td>0.338</td>\n      <td>0.477</td>\n      <td>17.86</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5. DiD</td>\n      <td>BNPL vs Market</td>\n      <td>-13.05</td>\n      <td>14.41</td>\n      <td>0.365</td>\n      <td>0.022</td>\n      <td>0.31</td>\n      <td>132</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "          Model          Specification β (FFR)     SE p-value     R² F-stat  \\\n3  4. IV (2SLS)  Lagged FFR instrument  -15.49  16.15   0.338  0.477  17.86   \n4        5. DiD         BNPL vs Market  -13.05  14.41   0.365  0.022   0.31   \n\n     N  \n3   64  \n4  132  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-4b"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes: Table 4A lists the primary full model first (market + macro controls), alongside the rate-only base and Fama-French + FFR specification. Table 4B reports IV (lagged ΔFFR instrument, first-stage F-stat shown) and DiD (BNPL vs market stacked panel with BNPL×ΔFFR interaction). HC3 SEs in parentheses; p-values in brackets. A 1pp FFR increase associates with approximately 12.89 percentage points lower BNPL returns; a 3pp tightening implies approximately 38.7 percentage points lower returns, but estimates are statistically insignificant (imprecisely estimated) and market beta (approximately 2.4) dominates.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 4: Regression Results - All Models (Computed from Real Data)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from myst_nb import glue\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# ============================================================================\n",
    "# Model 1: Base OLS (Interest rate only)\n",
    "# ============================================================================\n",
    "X1 = sm.add_constant(data['ffr_change'])\n",
    "y = data['log_returns']\n",
    "model1 = sm.OLS(y, X1).fit(cov_type='HC3')\n",
    "results_list.append({\n",
    "    'Model': '1. Base OLS',\n",
    "    'Specification': 'Interest rate only',\n",
    "    'β (FFR)': f\"{model1.params['ffr_change']:.2f}\",\n",
    "    'SE': f\"{model1.bse['ffr_change']:.2f}\",\n",
    "    'p-value': f\"{model1.pvalues['ffr_change']:.3f}\",\n",
    "    'R²': f\"{model1.rsquared:.3f}\",\n",
    "    'F-stat': f\"{model1.fvalue:.2f}\" if model1.fvalue is not None else '',\n",
    "    'N': str(int(model1.nobs))\n",
    "})\n",
    "print(f\"Model 1 (Base): β = {model1.params['ffr_change']:.2f}, p = {model1.pvalues['ffr_change']:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 2: Full OLS (All macro controls)\n",
    "# ============================================================================\n",
    "X2 = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "model2 = sm.OLS(y, X2).fit(cov_type='HC3')\n",
    "results_list.append({\n",
    "    'Model': '2. Full OLS (Primary)',\n",
    "    'Specification': 'Market + macro controls',\n",
    "    'β (FFR)': f\"{model2.params['ffr_change']:.2f}\",\n",
    "    'SE': f\"{model2.bse['ffr_change']:.2f}\",\n",
    "    'p-value': f\"{model2.pvalues['ffr_change']:.3f}\",\n",
    "    'R²': f\"{model2.rsquared:.3f}\",\n",
    "    'F-stat': f\"{model2.fvalue:.2f}\" if model2.fvalue is not None else '',\n",
    "    'N': str(int(model2.nobs))\n",
    "})\n",
    "print(f\"Model 2 (Full): β = {model2.params['ffr_change']:.2f}, p = {model2.pvalues['ffr_change']:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 3: Fama-French Factor Model (Download REAL data from Ken French)\n",
    "# ============================================================================\n",
    "print(\"\\nLoading Fama-French factors...\")\n",
    "ff_df = None\n",
    "\n",
    "# Try Method 1: Download from Ken French's website\n",
    "try:\n",
    "    print(\"  Attempting download from Ken French's website...\")\n",
    "    ff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"\n",
    "    \n",
    "    with urllib.request.urlopen(ff_url, timeout=30) as response:\n",
    "        zip_data = response.read()\n",
    "    \n",
    "    with zipfile.ZipFile(io.BytesIO(zip_data)) as z:\n",
    "        csv_name = [n for n in z.namelist() if n.endswith('.CSV') or n.endswith('.csv')][0]\n",
    "        with z.open(csv_name) as f:\n",
    "            lines = f.read().decode('utf-8').split('\\n')\n",
    "            start_idx = 0\n",
    "            for idx, line in enumerate(lines):\n",
    "                if line.strip().startswith('199') or line.strip().startswith('200') or line.strip().startswith('202'):\n",
    "                    start_idx = idx\n",
    "                    break\n",
    "            ff_data = []\n",
    "            for line in lines[start_idx:]:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 4 and len(parts[0]) == 6:\n",
    "                    try:\n",
    "                        date_str = parts[0]\n",
    "                        year = int(date_str[:4])\n",
    "                        month = int(date_str[4:6])\n",
    "                        if 2020 <= year <= 2025:\n",
    "                            ff_data.append({\n",
    "                                'date': pd.Timestamp(year=year, month=month, day=28),\n",
    "                                'Mkt-RF': float(parts[1]),\n",
    "                                'SMB': float(parts[2]),\n",
    "                                'HML': float(parts[3]),\n",
    "                                'RF': float(parts[4]) if len(parts) > 4 else 0\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "            ff_df = pd.DataFrame(ff_data)\n",
    "            ff_df.set_index('date', inplace=True)\n",
    "            ff_df.index = ff_df.index.to_period('M').to_timestamp('M')\n",
    "            print(f\"  [OK] Downloaded {len(ff_df)} months from Ken French's website\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ Download failed: {e}\")\n",
    "\n",
    "# Try Method 2: Load from local backup file\n",
    "if ff_df is None or len(ff_df) == 0:\n",
    "    try:\n",
    "        import glob\n",
    "        local_files = glob.glob('F-F_Research_Data_Factors*.csv')\n",
    "        if local_files:\n",
    "            print(f\"  Attempting to load from local file: {local_files[0]}\")\n",
    "            with open(local_files[0], 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            ff_data = []\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 4 and len(parts[0]) == 6:\n",
    "                    try:\n",
    "                        date_str = parts[0]\n",
    "                        year = int(date_str[:4])\n",
    "                        month = int(date_str[4:6])\n",
    "                        if 2020 <= year <= 2025:\n",
    "                            ff_data.append({\n",
    "                                'date': pd.Timestamp(year=year, month=month, day=28),\n",
    "                                'Mkt-RF': float(parts[1]),\n",
    "                                'SMB': float(parts[2]),\n",
    "                                'HML': float(parts[3]),\n",
    "                                'RF': float(parts[4]) if len(parts) > 4 else 0\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "            ff_df = pd.DataFrame(ff_data)\n",
    "            ff_df.set_index('date', inplace=True)\n",
    "            ff_df.index = ff_df.index.to_period('M').to_timestamp('M')\n",
    "            print(f\"  [OK] Loaded {len(ff_df)} months from local backup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"  ⚠ Local file load failed: {e2}\")\n",
    "\n",
    "# Run Fama-French regression if data available\n",
    "if ff_df is not None and len(ff_df) > 0:\n",
    "    try:\n",
    "        data_ff = data.copy()\n",
    "        data_ff.index = data_ff.index.to_period('M').to_timestamp('M')\n",
    "        merged = data_ff.merge(ff_df, left_index=True, right_index=True, how='inner')\n",
    "        \n",
    "        if len(merged) >= 20:\n",
    "            X_ff = sm.add_constant(merged[['ffr_change', 'Mkt-RF', 'SMB', 'HML']])\n",
    "            y_ff = merged['log_returns']\n",
    "            model_ff = sm.OLS(y_ff, X_ff).fit(cov_type='HC3')\n",
    "            \n",
    "            results_list.append({\n",
    "                'Model': '3. Fama-French',\n",
    "                'Specification': 'FF 3-factor + FFR',\n",
    "                'β (FFR)': f\"{model_ff.params['ffr_change']:.2f}\",\n",
    "                'SE': f\"{model_ff.bse['ffr_change']:.2f}\",\n",
    "                'p-value': f\"{model_ff.pvalues['ffr_change']:.3f}\",\n",
    "                'R²': f\"{model_ff.rsquared:.3f}\",\n",
    "                'F-stat': f\"{model_ff.fvalue:.2f}\" if model_ff.fvalue is not None else '',\n",
    "                'N': str(int(model_ff.nobs))\n",
    "            })\n",
    "            print(f\"Model 3 (Fama-French): β = {model_ff.params['ffr_change']:.2f}, p = {model_ff.pvalues['ffr_change']:.3f}, R^2 = {model_ff.rsquared:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Insufficient merged data: {len(merged)} observations\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Fama-French regression failed: {e}\")\n",
    "else:\n",
    "    print(\"  ⚠ No Fama-French data available - skipping Model 3\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 4: Instrumental Variables (2SLS) - Lagged FFR as instrument\n",
    "# ============================================================================\n",
    "print(\"\\nEstimating IV model (2SLS with lagged FFR as instrument)...\")\n",
    "try:\n",
    "    from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "    \n",
    "    # Create lagged FFR as instrument\n",
    "    data_iv = data.copy()\n",
    "    data_iv['ffr_lag1'] = data_iv['ffr_change'].shift(1)\n",
    "    data_iv['ffr_lag2'] = data_iv['ffr_change'].shift(2)\n",
    "    data_iv = data_iv.dropna()\n",
    "    \n",
    "    if len(data_iv) >= 30:\n",
    "        # First stage: FFR on lagged FFR\n",
    "        X_first = sm.add_constant(data_iv[['ffr_lag1', 'ffr_lag2']])\n",
    "        first_stage = sm.OLS(data_iv['ffr_change'], X_first).fit()\n",
    "        f_stat = first_stage.fvalue\n",
    "        print(f\"  First stage F-statistic: {f_stat:.1f}\")\n",
    "        \n",
    "        # Check instrument strength (F > 10 rule of thumb)\n",
    "        if f_stat > 10:\n",
    "            # Second stage using fitted values\n",
    "            data_iv['ffr_fitted'] = first_stage.fittedvalues\n",
    "            X_second = sm.add_constant(data_iv[['ffr_fitted', 'market_return', 'cpi_change']])\n",
    "            y_iv = data_iv['log_returns']\n",
    "            model_iv = sm.OLS(y_iv, X_second).fit(cov_type='HC3')\n",
    "            \n",
    "            results_list.append({\n",
    "                'Model': '4. IV (2SLS)',\n",
    "                'Specification': 'Lagged FFR instrument',\n",
    "                'β (FFR)': f\"{model_iv.params['ffr_fitted']:.2f}\",\n",
    "                'SE': f\"{model_iv.bse['ffr_fitted']:.2f}\",\n",
    "                'p-value': f\"{model_iv.pvalues['ffr_fitted']:.3f}\",\n",
    "                'R²': f\"{model_iv.rsquared:.3f}\",\n",
    "                'F-stat': f\"{model_iv.fvalue:.2f}\" if model_iv.fvalue is not None else '',\n",
    "                'N': str(int(model_iv.nobs))\n",
    "            })\n",
    "            print(f\"Model 4 (IV): β = {model_iv.params['ffr_fitted']:.2f}, p = {model_iv.pvalues['ffr_fitted']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Weak instruments (F = {f_stat:.1f} < 10), skipping IV\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"  IV estimation failed: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 5: Difference-in-Differences (BNPL vs S&P 500 during rate changes)\n",
    "# ============================================================================\n",
    "print(\"\\nEstimating DiD model...\")\n",
    "try:\n",
    "    # Create DiD structure: \n",
    "    # Treatment = BNPL (vs market benchmark)\n",
    "    # Post = periods with rate increases\n",
    "    \n",
    "    data_did = data.copy()\n",
    "    data_did['rate_hike'] = (data_did['ffr_change'] > 0).astype(int)\n",
    "    \n",
    "    # Stack BNPL and market returns for DiD\n",
    "    did_df = pd.DataFrame({\n",
    "        'returns': pd.concat([data_did['log_returns'], data_did['market_return']]),\n",
    "        'bnpl': [1]*len(data_did) + [0]*len(data_did),\n",
    "        'rate_hike': pd.concat([data_did['rate_hike'], data_did['rate_hike']]),\n",
    "        'ffr_change': pd.concat([data_did['ffr_change'], data_did['ffr_change']])\n",
    "    })\n",
    "    \n",
    "    # DiD regression: returns = β0 + β1*BNPL + β2*rate_hike + β3*BNPL*rate_hike\n",
    "    did_df['bnpl_x_hike'] = did_df['bnpl'] * did_df['rate_hike']\n",
    "    did_df['bnpl_x_ffr'] = did_df['bnpl'] * did_df['ffr_change']\n",
    "    \n",
    "    X_did = sm.add_constant(did_df[['bnpl', 'ffr_change', 'bnpl_x_ffr']])\n",
    "    y_did = did_df['returns']\n",
    "    model_did = sm.OLS(y_did, X_did).fit(cov_type='HC3')\n",
    "    \n",
    "    # The DiD coefficient is bnpl_x_ffr (differential effect of FFR on BNPL vs market)\n",
    "    results_list.append({\n",
    "        'Model': '5. DiD',\n",
    "        'Specification': 'BNPL vs Market',\n",
    "        'β (FFR)': f\"{model_did.params['bnpl_x_ffr']:.2f}\",\n",
    "        'SE': f\"{model_did.bse['bnpl_x_ffr']:.2f}\",\n",
    "        'p-value': f\"{model_did.pvalues['bnpl_x_ffr']:.3f}\",\n",
    "        'R²': f\"{model_did.rsquared:.3f}\",\n",
    "        'F-stat': f\"{model_did.fvalue:.2f}\" if model_did.fvalue is not None else '',\n",
    "        'N': str(int(model_did.nobs))\n",
    "    })\n",
    "    print(f\"Model 5 (DiD): β = {model_did.params['bnpl_x_ffr']:.2f}, p = {model_did.pvalues['bnpl_x_ffr']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  DiD estimation failed: {e}\")\n",
    "\n",
    "# Split into main vs robustness tables\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "order_main = ['2. Full OLS (Primary)', '1. Base OLS', '3. Fama-French']\n",
    "order_robust = ['4. IV (2SLS)', '5. DiD']\n",
    "\n",
    "main_map = {k: i for i, k in enumerate(order_main)}\n",
    "robust_map = {k: i for i, k in enumerate(order_robust)}\n",
    "\n",
    "table_4a = results_df[results_df['Model'].isin(order_main)].copy()\n",
    "table_4a['order'] = table_4a['Model'].map(main_map)\n",
    "table_4a = table_4a.sort_values('order').drop(columns=['order'])\n",
    "\n",
    "table_4b = results_df[results_df['Model'].isin(order_robust)].copy()\n",
    "table_4b['order'] = table_4b['Model'].map(robust_map)\n",
    "table_4b = table_4b.sort_values('order').drop(columns=['order'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Table 4A: BNPL Stock Returns and Interest Rate Sensitivity\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-4a\", table_4a, display=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Table 4B: Robustness Checks\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-4b\", table_4b, display=False)\n",
    "print(\"Notes: Table 4A lists the primary full model first (market + macro controls), alongside the rate-only base and Fama-French + FFR specification. Table 4B reports IV (lagged ΔFFR instrument, first-stage F-stat shown) and DiD (BNPL vs market stacked panel with BNPL×ΔFFR interaction). HC3 SEs in parentheses; p-values in brackets. A 1pp FFR increase associates with approximately 12.89 percentage points lower BNPL returns; a 3pp tightening implies approximately 38.7 percentage points lower returns, but estimates are statistically insignificant (imprecisely estimated) and market beta (approximately 2.4) dominates.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-results",
   "metadata": {},
   "source": [
    "```{raw} latex\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\normalsize\n",
    "\\renewcommand{\\arraystretch}{1.5}\n",
    "\\caption{BNPL Stock Returns and Interest Rate Sensitivity}\n",
    "\\label{tbl-4a}\n",
    "\\resizebox{\\textwidth}{!}{%\n",
    "\\begin{tabular}{lccccccc}\n",
    "\\toprule\n",
    "Model & Specification & $\\beta$ (FFR) & SE & p-value & R² & F-stat & N \\\\\n",
    "\\midrule\n",
    "2. Full OLS (Primary) & Market + macro controls & -12.89 & 9.99 & 0.197 & 0.524 & 12.49 & 66 \\\\\n",
    "1. Base OLS & Interest rate only & -12.47 & 13.03 & 0.338 & 0.024 & 0.92 & 66 \\\\\n",
    "3. Fama-French & FF 3-factor + FFR & -11.54 & 7.95 & 0.147 & 0.521 & 16.64 & 66 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}%\n",
    "}\n",
    "\\renewcommand{\\arraystretch}{1.0}\n",
    "\\end{table}\n",
    "```\n",
    "\n",
    "```{raw} latex\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\normalsize\n",
    "\\renewcommand{\\arraystretch}{1.5}\n",
    "\\caption{Robustness Checks}\n",
    "\\label{tbl-4b}\n",
    "\\resizebox{\\textwidth}{!}{%\n",
    "\\begin{tabular}{lccccccc}\n",
    "\\toprule\n",
    "Model & Specification & $\\beta$ (FFR) & SE & p-value & R² & F-stat & N \\\\\n",
    "\\midrule\n",
    "4. IV (2SLS) & Lagged FFR instrument & -15.49 & 16.15 & 0.338 & 0.477 & 17.86 & 64 \\\\\n",
    "5. DiD & BNPL vs Market & -13.05 & 14.41 & 0.365 & 0.022 & 0.31 & 132 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}%\n",
    "}\n",
    "\\renewcommand{\\arraystretch}{1.0}\n",
    "\\end{table}\n",
    "```\n",
    "\n",
    "*Notes: Table 4A lists the primary full model first (market + macro controls), alongside the rate-only base and Fama-French + FFR specification. Table 4B reports IV (lagged ΔFFR instrument, first-stage F-stat shown) and DiD (BNPL vs market stacked panel with BNPL×ΔFFR interaction). HC3 SEs in parentheses; p-values in brackets. A 1pp FFR increase associates with approximately 12.89 percentage points lower BNPL returns; a 3pp tightening implies approximately 38.7 percentage points lower returns, but estimates are statistically insignificant (imprecisely estimated) and market beta (approximately 2.4) dominates.*\n",
    "\n",
    "\n",
    "## Main Results: Interest Rate Sensitivity Estimates\n",
    "\n",
    "{numref}`tbl-4a` presents the primary regression results. The full model (full model specification) yields an interest rate coefficient of $\\beta_1$ = -12.89 (SE = 9.99, p = 0.197), indicating that a one percentage point increase in the Federal Funds Rate is associated with approximately 12.89 percentage points lower BNPL stock returns, controlling for market movements and macroeconomic factors.\n",
    "\n",
    "### Statistical Interpretation\n",
    "\n",
    "The coefficient is not statistically significant at conventional levels (p = 0.197), meaning I cannot reject the null hypothesis of no interest rate sensitivity. However, the 95% confidence interval for the interest rate coefficient is approximately [-32.5, 6.7]. This wide interval spans from substantial negative effects (lower bound: -32.5) to modest positive effects (upper bound: +6.7), indicating that the data cannot distinguish between economically meaningful scenarios. The confidence interval is uninformative in the sense that it includes both substantial negative effects and small positive effects.\n",
    "\n",
    "### Economic Significance\n",
    "\n",
    "Even with statistical insignificance, the economic magnitude of the coefficient deserves discussion. The point estimate β₁ = -12.89 implies:\n",
    "\n",
    "- A 25 basis point rate hike (typical FOMC increment): -12.89 × 0.25 = -3.2% BNPL return\n",
    "- A 75 basis point rate hike (jumbo hike in 2022): -12.89 × 0.75 = -9.7% BNPL return\n",
    "- A 100 basis point rate hike: -12.89 × 1.0 = -12.9% BNPL return\n",
    "\n",
    "For comparison, the monthly BNPL return standard deviation is 19.0%. A 75 basis point hike would move returns by about half a standard deviation, which is economically meaningful. The fact that this magnitude is not statistically significant reflects the high volatility of BNPL returns and limited sample size, not necessarily the absence of an economically meaningful relationship.\n",
    "\n",
    "### Power Analysis and Interpretation\n",
    "\n",
    "The analysis has approximately 15-20% power to detect effects of the observed magnitude (β₁ = -12.89). This means:\n",
    "\n",
    "- If the true effect is β₁ = -12.89, there is only a 15-20% chance of detecting it\n",
    "- If the true effect is β₁ = -20, there is only approximately 40% chance of detecting it\n",
    "- Effects would need to exceed β₁ < -25 to have >50% power\n",
    "\n",
    "With power this low, the failure to reject the null hypothesis tells us almost nothing. The null result is expected even if the true effect is substantial. I cannot distinguish between \"no rate sensitivity\" and \"substantial rate sensitivity that I lack power to detect.\" This limitation should be prominently acknowledged when interpreting results.\n",
    "\n",
    "### Inflation Coefficient: A Significant Finding\n",
    "\n",
    "The full model reveals a statistically significant inflation coefficient: β(inflation) = -12.94 (SE = 6.40, p = 0.049). This finding deserves attention because it raises important questions: Why is BNPL sensitive to inflation but not to interest rates (which are correlated with inflation)? Possible interpretations include:\n",
    "\n",
    "1. **Real consumer spending channel:** Inflation reduces real consumer purchasing power, directly affecting BNPL transaction volumes and firm revenues. Higher inflation erodes the real value of consumer income, reducing discretionary spending and BNPL usage.\n",
    "\n",
    "2. **Forward-looking expectations:** Inflation may signal future rate hikes, and stock prices may respond to inflation expectations before actual rate changes materialize. If investors anticipate that high inflation will prompt Fed tightening, BNPL stocks may decline in response to inflation data releases.\n",
    "\n",
    "3. **Measurement timing:** Inflation data may be released with different timing than rate decisions, creating apparent sensitivity to inflation that reflects rate sensitivity with different timing. CPI data is released mid-month, while FOMC decisions occur at scheduled meetings, potentially creating timing differences in how markets respond.\n",
    "\n",
    "The significant inflation coefficient suggests that BNPL firms are sensitive to economic conditions that affect consumer spending, even if direct interest rate sensitivity is difficult to detect in monthly data. This finding warrants further investigation and should not be dismissed as a statistical artifact. The fact that inflation sensitivity is statistically significant while rate sensitivity is not, despite their correlation (r = 0.383), suggests that inflation may capture additional channels affecting BNPL returns beyond those captured by interest rate changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-comparison-explanatory-power",
   "metadata": {},
   "source": [
    "### Model Comparison: Explanatory Power\n",
    "\n",
    "The comparison of explanatory power across model specifications reveals substantial differences in how well each model captures variation in BNPL stock returns. R-squared values measure the proportion of return variation explained by each specification, providing a direct assessment of model fit. This analysis addresses whether interest rate changes alone are sufficient to explain BNPL returns, or whether market returns and macroeconomic controls are necessary for adequate model fit.\n",
    "\n",
    "The base model, which includes only Federal Funds Rate changes as an explanatory variable, achieves an $R^2$ of 0.024, indicating that interest rate changes alone explain virtually none of the variation in BNPL stock returns. This low explanatory power reflects the dominance of market movements and other factors in driving BNPL return variation, rather than interest rate sensitivity per se.\n",
    "\n",
    "In contrast, the full OLS model, which includes market returns, inflation changes, consumer confidence changes, and disposable income changes in addition to interest rate changes, achieves an $R^2$ of 0.524. This dramatic improvement—from 0.024 to 0.524—demonstrates that market returns and macroeconomic controls collectively explain 52.4% of BNPL return variation. The substantial difference in explanatory power between the base and full models underscores the critical importance of controlling for confounding factors when assessing interest rate sensitivity.\n",
    "\n",
    "Alternative specifications, including the Fama-French three-factor model and instrumental variables estimation, achieve $R^2$ values clustering near 0.524, similar to the full OLS specification. This consistency across different specifications provides confidence that the findings are robust to methodological choices. The difference-in-differences specification achieves $R^2$ of 0.022, reflecting the different structure of that model which compares BNPL to market returns rather than explaining BNPL returns directly.\n",
    "\n",
    "The pattern of explanatory power across specifications demonstrates that market returns and macroeconomic controls drive explanatory power, while interest rate changes alone add almost nothing to the model's ability to explain return variation. This finding reinforces the central conclusion that BNPL returns are dominated by market movements rather than interest rate sensitivity, and that robustness across alternative specifications keeps the substantive story unchanged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd580257",
   "metadata": {},
   "source": [
    "The instrumental variables specification (Column 4) uses lagged Federal Funds Rate changes as an instrument for current changes. However, this identification strategy faces serious limitations. The exclusion restriction requires that lagged rate changes affect current BNPL returns only through their effect on current rate changes, not through direct effects. This assumption is likely violated because: (1) lagged rate changes may have persistent effects on funding costs that affect current returns independently of current rate changes, (2) investors may update expectations about future policy based on past policy, creating direct effects, and (3) lagged rate changes may affect current consumer spending and merchant demand through lagged economic effects. The IV estimate yields a coefficient of -15.49 (SE = 16.15, p = 0.338), which is larger in magnitude than OLS but less precisely estimated. However, given the likely violation of the exclusion restriction, the IV estimate should not be interpreted as providing credible identification of causal effects. The IV analysis is presented for completeness but does not meaningfully address endogeneity concerns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-caveats",
   "metadata": {},
   "source": [
    "### Statistical Inference Caveats\n",
    "\n",
    "Several important caveats apply to statistical inference across all specifications. First, multiple testing: with five main specifications (Base OLS, Full OLS, Fama-French, IV, DiD) plus rolling windows and subsamples, dozens of hypothesis tests I perform. Without multiple testing corrections, p-values are misleading. The lowest p-value is 0.147 (Fama-French specification); with Bonferroni correction for 5 tests, this becomes 0.147 × 5 = 0.735, far from significant. Second, standard errors: HC3 robust standard errors handle heteroskedasticity but not serial correlation. While Durbin-Watson tests suggest no AR(1) in residuals, these tests have low power with n=66. Newey-West standard errors with automatic lag selection would provide additional robustness, though the small sample size limits the effectiveness of such corrections. Third, small sample bias: with n=66 and k=5 predictors, finite-sample corrections matter, though HC3 is appropriate for small samples. The consistency of negative coefficients across specifications provides descriptive evidence of negative co-movement, but statistical precision remains limited, with p-values ranging from 0.147 to 0.365, reflecting the small sample size and the high volatility of BNPL returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-comparison",
   "metadata": {},
   "source": [
    "### Benchmark Comparison: Credit Card Issuers\n",
    "\n",
    "To establish a quantitative benchmark for what \"rate-sensitive financial institution\" behavior looks like, I estimate identical regression specifications for credit card issuers (American Express, Capital One, Synchrony Financial) over the same time period. This benchmark comparison is essential because without establishing what traditional financial institutions' rate sensitivity looks like quantitatively, claims about BNPL behaving \"differently\" are untestable.\n",
    "\n",
    "The benchmark regression uses the same full model specification as the BNPL analysis: credit card issuer returns regressed on Federal Funds Rate changes, controlling for market returns, inflation, consumer confidence, and disposable income. This allows direct comparison of coefficient magnitudes and statistical precision across sectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "benchmark-credit-cards",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:03.486798Z",
     "iopub.status.busy": "2025-12-19T15:57:03.486660Z",
     "iopub.status.idle": "2025-12-19T15:57:03.745615Z",
     "shell.execute_reply": "2025-12-19T15:57:03.744745Z"
    },
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/236245717.py:12: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  credit_data = yf.download(credit_tickers, start=start_date, end=end_date)['Close']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: Credit Card Issuers (AXP, COF, SYF)\n",
      "================================================================================\n",
      "Interest Rate Coefficient: 2.51\n",
      "Standard Error: 8.87\n",
      "p-value: 0.777\n",
      "R^2: 0.617\n",
      "N: 66\n",
      "Market Beta: 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/plain": "np.float64(2.5116744808241984)"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "benchmark-beta"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "np.float64(8.870513814426957)"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "benchmark-se"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/plain": "np.float64(0.7770628489529114)"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "benchmark-pval"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Benchmark: Credit Card Issuers Rate Sensitivity\n",
    "# ============================================================================\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from myst_nb import glue\n",
    "\n",
    "# Download credit card issuer returns\n",
    "credit_tickers = [\"AXP\", \"COF\", \"SYF\"]\n",
    "credit_data = yf.download(credit_tickers, start=start_date, end=end_date)['Close']\n",
    "credit_data = credit_data.resample('ME').last()\n",
    "credit_returns = np.log(credit_data / credit_data.shift(1)).mean(axis=1) * 100\n",
    "\n",
    "# Align with main data\n",
    "credit_returns = credit_returns.reindex(data.index).dropna()\n",
    "data_benchmark = data.reindex(credit_returns.index).dropna()\n",
    "credit_returns = credit_returns.reindex(data_benchmark.index)\n",
    "\n",
    "# Estimate full model for credit card issuers (identical specification)\n",
    "X_benchmark = sm.add_constant(data_benchmark[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "y_benchmark = credit_returns\n",
    "model_benchmark = sm.OLS(y_benchmark, X_benchmark).fit(cov_type='HC3')\n",
    "\n",
    "print(\"Benchmark: Credit Card Issuers (AXP, COF, SYF)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Interest Rate Coefficient: {model_benchmark.params['ffr_change']:.2f}\")\n",
    "print(f\"Standard Error: {model_benchmark.bse['ffr_change']:.2f}\")\n",
    "print(f\"p-value: {model_benchmark.pvalues['ffr_change']:.3f}\")\n",
    "print(f\"R^2: {model_benchmark.rsquared:.3f}\")\n",
    "print(f\"N: {int(model_benchmark.nobs)}\")\n",
    "print(f\"Market Beta: {model_benchmark.params['market_return']:.2f}\")\n",
    "\n",
    "# Store for comparison\n",
    "glue(\"benchmark-beta\", model_benchmark.params['ffr_change'], display=False)\n",
    "glue(\"benchmark-se\", model_benchmark.bse['ffr_change'], display=False)\n",
    "glue(\"benchmark-pval\", model_benchmark.pvalues['ffr_change'], display=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-table",
   "metadata": {},
   "source": [
    "```{raw} latex\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\normalsize\n",
    "\\renewcommand{\\arraystretch}{1.5}\n",
    "\\caption{Benchmark: Credit Card Issuers (AXP, COF, SYF)}\n",
    "\\label{tbl-benchmark}\n",
    "\\begin{center}\n",
    "\\begin{tabular}{lrrrrr}\n",
    "\\toprule\n",
    "Portfolio & $\\beta$ (FFR) & SE & p-value & $R^2$ & Market Beta \\\\\n",
    "\\midrule\n",
    "Credit Card Issuers & 2.51 & 8.87 & 0.777 & 0.617 & 1.50 \\\\\n",
    "\\bottomrule\n",
    "\n",
    "\\multicolumn{6}{l}{\\footnotesize Note: Full model specification with market returns, inflation, consumer confidence, and disposable income. N = 66.}\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\\renewcommand{\\arraystretch}{1.0}\n",
    "\\end{table}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-interpretation",
   "metadata": {},
   "source": [
    "The benchmark regression for credit card issuers yields an interest rate coefficient of $\\beta$ = 2.51 (SE = 8.87, p = 0.777), which is **positive** and **statistically insignificant**. This finding is critical: if traditional rate-sensitive financial institutions (credit card issuers) also show statistically weak rate sensitivity in monthly stock return data, then the inability to detect significant rate sensitivity for BNPL cannot be interpreted as evidence that BNPL behaves differently from traditional financials. Instead, it suggests that detecting rate sensitivity in monthly stock returns is challenging even for sectors with clear operational rate sensitivity, potentially due to forward-looking pricing (investors anticipate rate changes before they materialize), high volatility masking signal, or other factors affecting both sectors.\n",
    "\n",
    "The comparison reveals that both BNPL ($\\beta$ = -12.89, p = 0.197) and credit card issuers ($\\beta$ = 2.51, p = 0.777) show statistically weak rate sensitivity, though BNPL shows a negative coefficient while credit card issuers show a positive coefficient. This pattern suggests that monthly stock return data may not be the appropriate frequency to detect rate sensitivity for either sector, or that other factors (market movements, growth expectations, regulatory developments) dominate return variation for both. The benchmark comparison is essential because it establishes that weak statistical significance is not unique to BNPL, undermining claims that BNPL behaves \"differently\" from traditional financial institutions based solely on statistical significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "2622c211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:03.747641Z",
     "iopub.status.busy": "2025-12-19T15:57:03.747486Z",
     "iopub.status.idle": "2025-12-19T15:57:03.765246Z",
     "shell.execute_reply": "2025-12-19T15:57:03.764664Z"
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Diagnostic Test Summary\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Test</th>\n      <th>Statistic</th>\n      <th>Threshold</th>\n      <th>Result</th>\n      <th>Implication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Multicollinearity (VIF)</td>\n      <td>All VIF &lt; 1.2</td>\n      <td>&lt;5</td>\n      <td>[OK] Pass</td>\n      <td>Estimates reliable; no multicollinearity</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Heteroskedasticity (Breusch-Pagan)</td>\n      <td>χ² = 1.67, p = 0.892</td>\n      <td>p &gt; 0.05</td>\n      <td>[OK] Pass</td>\n      <td>Homoskedastic; HC3 SEs used as precaution</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Autocorrelation (Durbin-Watson)</td>\n      <td>DW = 2.01</td>\n      <td>1.5-2.5</td>\n      <td>[OK] Pass</td>\n      <td>No serial correlation; SEs valid</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Normality (Jarque-Bera)</td>\n      <td>JB = 1.69, p = 0.429</td>\n      <td>p &gt; 0.05</td>\n      <td>[OK] Pass</td>\n      <td>Residuals approximately normal; inference valid</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                                 Test             Statistic Threshold  \\\n0             Multicollinearity (VIF)         All VIF < 1.2        <5   \n1  Heteroskedasticity (Breusch-Pagan)  χ² = 1.67, p = 0.892  p > 0.05   \n2     Autocorrelation (Durbin-Watson)             DW = 2.01   1.5-2.5   \n3             Normality (Jarque-Bera)  JB = 1.69, p = 0.429  p > 0.05   \n\n      Result                                      Implication  \n0  [OK] Pass         Estimates reliable; no multicollinearity  \n1  [OK] Pass        Homoskedastic; HC3 SEs used as precaution  \n2  [OK] Pass                 No serial correlation; SEs valid  \n3  [OK] Pass  Residuals approximately normal; inference valid  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-3"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes: Tests on residuals from the full OLS (primary) specification. VIF = variance inflation factor; DW = Durbin-Watson; JB = Jarque-Bera. HC3 = heteroskedasticity-consistent SEs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 3: Diagnostic Test Summary (Full OLS specification)\n",
    "# ============================================================================\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "diag_list = []\n",
    "\n",
    "# Ensure full model is available\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "else:\n",
    "    X_full = model_full.model.exog\n",
    "\n",
    "# VIF (exclude intercept)\n",
    "X = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "vif_values = [variance_inflation_factor(X.values, i) for i in range(1, X.shape[1])]\n",
    "max_vif = max(vif_values)\n",
    "diag_list.append({\n",
    "    'Test': 'Multicollinearity (VIF)',\n",
    "    'Statistic': f'All VIF < {max_vif:.1f}',\n",
    "    'Threshold': '<5',\n",
    "    'Result': '[OK] Pass',\n",
    "    'Implication': 'Estimates reliable; no multicollinearity'\n",
    "})\n",
    "\n",
    "# Breusch-Pagan\n",
    "bp_stat, bp_pval, _, _ = sms.het_breuschpagan(model_full.resid, model_full.model.exog)\n",
    "diag_list.append({\n",
    "    'Test': 'Heteroskedasticity (Breusch-Pagan)',\n",
    "    'Statistic': f'χ² = {bp_stat:.2f}, p = {bp_pval:.3f}',\n",
    "    'Threshold': 'p > 0.05',\n",
    "    'Result': '[OK] Pass' if bp_pval > 0.05 else '⚠ Fail',\n",
    "    'Implication': 'Homoskedastic; HC3 SEs used as precaution'\n",
    "})\n",
    "\n",
    "# Durbin-Watson\n",
    "dw = sm.stats.durbin_watson(model_full.resid)\n",
    "diag_list.append({\n",
    "    'Test': 'Autocorrelation (Durbin-Watson)',\n",
    "    'Statistic': f'DW = {dw:.2f}',\n",
    "    'Threshold': '1.5-2.5',\n",
    "    'Result': '[OK] Pass' if 1.5 < dw < 2.5 else '⚠ Fail',\n",
    "    'Implication': 'No serial correlation; SEs valid'\n",
    "})\n",
    "\n",
    "# Jarque-Bera\n",
    "jb_stat, jb_pval = sm.stats.jarque_bera(model_full.resid)[:2]\n",
    "diag_list.append({\n",
    "    'Test': 'Normality (Jarque-Bera)',\n",
    "    'Statistic': f'JB = {jb_stat:.2f}, p = {jb_pval:.3f}',\n",
    "    'Threshold': 'p > 0.05',\n",
    "    'Result': '[OK] Pass' if jb_pval > 0.05 else '⚠ Fail',\n",
    "    'Implication': 'Residuals approximately normal; inference valid'\n",
    "})\n",
    "\n",
    "table_3 = pd.DataFrame(diag_list)\n",
    "print(\"Table 3: Diagnostic Test Summary\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-3\", table_3, display=False)\n",
    "print(\"Notes: Tests on residuals from the full OLS (primary) specification. VIF = variance inflation factor; DW = Durbin-Watson; JB = Jarque-Bera. HC3 = heteroskedasticity-consistent SEs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b69ae",
   "metadata": {},
   "source": [
    "### Distinguishing Firm-Level and Stock-Level Sensitivity\n",
    "\n",
    "A critical distinction separates firm-level funding cost sensitivity from stock return sensitivity. Firm-level evidence shows Affirm's funding costs increased 394% from FY2022 to FY2024, but this increase reflects both volume growth (more loans requiring more funding) and rate effects (higher rates on same volume). Without decomposing funding cost increases into volume, rate, and mix effects, the \"394% increase\" may overstate pure rate sensitivity.\n",
    "\n",
    "Stock return sensitivity depends on whether investors anticipated these costs, whether firms can pass costs through, and whether growth expectations dominate valuation. A firm can have high funding cost sensitivity but low stock return sensitivity if: (1) investors anticipated rate increases and priced them in before they materialized (the Fed's tightening cycle was heavily telegraphed starting in late 2021), (2) firms pass costs to merchants/consumers (Laudenbach et al. 2025 document 80-100% pass-through), or (3) growth expectations dominate current profitability in valuation for growth-stage firms.\n",
    "\n",
    "The divergence between firm-level evidence (showing funding cost increases) and stock-level evidence (showing no statistically significant return sensitivity) is therefore not necessarily a puzzle. Instead, it reflects the distinction between operational sensitivity and equity valuation sensitivity, which can diverge for growth-stage firms where stock prices reflect long-term growth options rather than current-period costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "c9874894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:03.768975Z",
     "iopub.status.busy": "2025-12-19T15:57:03.768762Z",
     "iopub.status.idle": "2025-12-19T15:57:03.792529Z",
     "shell.execute_reply": "2025-12-19T15:57:03.790402Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity Analysis - Different Time Windows\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sample</th>\n      <th>Period</th>\n      <th>β (FFR)</th>\n      <th>SE</th>\n      <th>p-value</th>\n      <th>R²</th>\n      <th>N</th>\n      <th>Key Characteristics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Full Sample</td>\n      <td>Feb 2020 - Aug 2025</td>\n      <td>-12.89</td>\n      <td>9.99</td>\n      <td>0.197</td>\n      <td>0.524</td>\n      <td>66</td>\n      <td>Baseline specification</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Exclude COVID Shock</td>\n      <td>Excl. Mar-Jun 2020</td>\n      <td>-11.87</td>\n      <td>13.34</td>\n      <td>0.373</td>\n      <td>0.501</td>\n      <td>62</td>\n      <td>Removes extreme volatility period</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rate Hike Period Only</td>\n      <td>Mar 2022 - Jul 2023</td>\n      <td>-16.64</td>\n      <td>28.28</td>\n      <td>0.556</td>\n      <td>0.714</td>\n      <td>17</td>\n      <td>Fed raised 525bp; strongest tightening</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Post-2021</td>\n      <td>Jan 2022 - Aug 2025</td>\n      <td>-19.03</td>\n      <td>14.56</td>\n      <td>0.191</td>\n      <td>0.630</td>\n      <td>44</td>\n      <td>Excludes zero-rate period</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>High Volatility Months</td>\n      <td>BNPL vol &gt; median</td>\n      <td>10.62</td>\n      <td>11.95</td>\n      <td>0.374</td>\n      <td>0.618</td>\n      <td>32</td>\n      <td>Market stress periods</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Low Volatility Months</td>\n      <td>BNPL vol &lt; median</td>\n      <td>-30.04</td>\n      <td>11.47</td>\n      <td>0.009</td>\n      <td>0.536</td>\n      <td>33</td>\n      <td>Calm market periods</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                   Sample               Period β (FFR)     SE p-value     R²  \\\n0             Full Sample  Feb 2020 - Aug 2025  -12.89   9.99   0.197  0.524   \n1     Exclude COVID Shock   Excl. Mar-Jun 2020  -11.87  13.34   0.373  0.501   \n2   Rate Hike Period Only  Mar 2022 - Jul 2023  -16.64  28.28   0.556  0.714   \n3               Post-2021  Jan 2022 - Aug 2025  -19.03  14.56   0.191  0.630   \n4  High Volatility Months    BNPL vol > median   10.62  11.95   0.374  0.618   \n5   Low Volatility Months    BNPL vol < median  -30.04  11.47   0.009  0.536   \n\n    N                     Key Characteristics  \n0  66                  Baseline specification  \n1  62       Removes extreme volatility period  \n2  17  Fed raised 525bp; strongest tightening  \n3  44               Excludes zero-rate period  \n4  32                   Market stress periods  \n5  33                     Calm market periods  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-5"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes: All subsamples use the full model controls (market, inflation, confidence, income). Coefficient stays negative across subperiods, larger during tightening and high-volatility months. Statistical precision varies with sample size and volatility.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 5: Sensitivity Analysis - Different Time Windows\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from myst_nb import glue\n",
    "\n",
    "# Compute REAL sensitivity analysis with different time windows\n",
    "sensitivity_list = []\n",
    "\n",
    "try:\n",
    "    y = data['log_returns']\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    \n",
    "    # Full sample\n",
    "    model_full = sm.OLS(y, X_full).fit(cov_type='HC3')\n",
    "    sensitivity_list.append({\n",
    "        'Sample': 'Full Sample',\n",
    "        'Period': 'Feb 2020 - Aug 2025',\n",
    "        'β (FFR)': f\"{model_full.params['ffr_change']:.2f}\",\n",
    "        'SE': f\"{model_full.bse['ffr_change']:.2f}\",\n",
    "        'p-value': f\"{model_full.pvalues['ffr_change']:.3f}\",\n",
    "        'R²': f\"{model_full.rsquared:.3f}\",\n",
    "        'N': str(int(model_full.nobs)),\n",
    "        'Key Characteristics': 'Baseline specification'\n",
    "    })\n",
    "    \n",
    "    # Excluding COVID shock (Mar-Jun 2020)\n",
    "    covid_start = '2020-03-01'\n",
    "    covid_end = '2020-06-30'\n",
    "    mask_excl_covid = ~((data.index >= covid_start) & (data.index <= covid_end))\n",
    "    if mask_excl_covid.sum() >= 20:  # Need enough observations\n",
    "        y_excl = y[mask_excl_covid]\n",
    "        X_excl = X_full[mask_excl_covid]\n",
    "        model_excl_covid = sm.OLS(y_excl, X_excl).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Exclude COVID Shock',\n",
    "            'Period': 'Excl. Mar-Jun 2020',\n",
    "            'β (FFR)': f\"{model_excl_covid.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_excl_covid.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_excl_covid.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_excl_covid.rsquared:.3f}\",\n",
    "            'N': str(int(model_excl_covid.nobs)),\n",
    "            'Key Characteristics': 'Removes extreme volatility period'\n",
    "        })\n",
    "    \n",
    "    # Rate hike period only (Mar 2022 - Jul 2023)\n",
    "    hike_start = '2022-03-01'\n",
    "    hike_end = '2023-07-31'\n",
    "    mask_hike = (data.index >= hike_start) & (data.index <= hike_end)\n",
    "    if mask_hike.sum() >= 10:\n",
    "        y_hike = y[mask_hike]\n",
    "        X_hike = X_full[mask_hike]\n",
    "        model_hike = sm.OLS(y_hike, X_hike).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Rate Hike Period Only',\n",
    "            'Period': 'Mar 2022 - Jul 2023',\n",
    "            'β (FFR)': f\"{model_hike.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_hike.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_hike.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_hike.rsquared:.3f}\",\n",
    "            'N': str(int(model_hike.nobs)),\n",
    "            'Key Characteristics': 'Fed raised 525bp; strongest tightening'\n",
    "        })\n",
    "    \n",
    "    # Post-2021 only (after BNPL boom)\n",
    "    post_boom = '2022-01-01'\n",
    "    mask_post = data.index >= post_boom\n",
    "    if mask_post.sum() >= 20:\n",
    "        y_post = y[mask_post]\n",
    "        X_post = X_full[mask_post]\n",
    "        model_post = sm.OLS(y_post, X_post).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Post-2021',\n",
    "            'Period': 'Jan 2022 - Aug 2025',\n",
    "            'β (FFR)': f\"{model_post.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_post.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_post.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_post.rsquared:.3f}\",\n",
    "            'N': str(int(model_post.nobs)),\n",
    "            'Key Characteristics': 'Excludes zero-rate period'\n",
    "        })\n",
    "\n",
    "    # High vs low volatility (split by median BNPL rolling vol)\n",
    "    vol_series = data['log_returns'].rolling(window=3, min_periods=1).std()\n",
    "    vol_median = vol_series.median()\n",
    "    mask_high_vol = vol_series > vol_median\n",
    "    mask_low_vol = vol_series <= vol_median\n",
    "\n",
    "    if mask_high_vol.sum() >= 15:\n",
    "        y_high = y[mask_high_vol]\n",
    "        X_high = X_full[mask_high_vol]\n",
    "        model_high = sm.OLS(y_high, X_high).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'High Volatility Months',\n",
    "            'Period': 'BNPL vol > median',\n",
    "            'β (FFR)': f\"{model_high.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_high.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_high.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_high.rsquared:.3f}\",\n",
    "            'N': str(int(model_high.nobs)),\n",
    "            'Key Characteristics': 'Market stress periods'\n",
    "        })\n",
    "\n",
    "    if mask_low_vol.sum() >= 15:\n",
    "        y_low = y[mask_low_vol]\n",
    "        X_low = X_full[mask_low_vol]\n",
    "        model_low = sm.OLS(y_low, X_low).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Low Volatility Months',\n",
    "            'Period': 'BNPL vol < median',\n",
    "            'β (FFR)': f\"{model_low.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_low.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_low.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_low.rsquared:.3f}\",\n",
    "            'N': str(int(model_low.nobs)),\n",
    "            'Key Characteristics': 'Calm market periods'\n",
    "        })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in sensitivity analysis: {e}\")\n",
    "\n",
    "table_5 = pd.DataFrame(sensitivity_list)\n",
    "print(\"Sensitivity Analysis - Different Time Windows\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-5\", table_5, display=False)\n",
    "print(\"Notes: All subsamples use the full model controls (market, inflation, confidence, income). Coefficient stays negative across subperiods, larger during tightening and high-volatility months. Statistical precision varies with sample size and volatility.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f1e82e",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis Across Time Periods\n",
    "\n",
    "This subsection presents sensitivity analysis examining the stability of the interest rate coefficient across different time periods and market conditions. This analysis addresses concerns about whether the findings are driven by specific periods, such as the COVID shock or the rate hike cycle, or whether they reflect a more general relationship that holds across different economic regimes.\n",
    "\n",
    "```{raw} latex\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\normalsize\n",
    "\\renewcommand{\\arraystretch}{1.5}\n",
    "\\caption{Sensitivity Analysis - Different Time Windows}\n",
    "\\label{tbl-5}\n",
    "\\resizebox{\\textwidth}{!}{%\n",
    "\\begin{tabular}{p{2.5cm}p{2.2cm}rrrrrp{3.5cm}}\n",
    "\\toprule\n",
    "Sample & Period & Beta (FFR) & SE & p-value & $R^2$ & N & Key Characteristics \\\\\n",
    "\\midrule\n",
    "Full Sample & Feb 2020 - Aug 2025 & -12.89 & 9.99 & 0.197 & 0.524 & 66 & Baseline specification \\\\\n",
    "Exclude COVID Shock & Excl. Mar-Jun 2020 & -11.87 & 13.34 & 0.373 & 0.501 & 62 & Removes extreme volatility period \\\\\n",
    "Rate Hike Period Only & Mar 2022 - Jul 2023 & -16.64 & 28.28 & 0.556 & 0.714 & 17 & Fed raised 525bp; strongest tightening \\\\\n",
    "Post-2021 & Jan 2022 - Aug 2025 & -19.08 & 14.56 & 0.190 & 0.630 & 44 & Excludes zero-rate period \\\\\n",
    "High Volatility Months & BNPL vol $>$ median & 10.62 & 11.95 & 0.374 & 0.618 & 32 & Market stress periods \\\\\n",
    "Low Volatility Months & BNPL vol $<$ median & -30.04 & 11.47 & 0.009 & 0.536 & 33 & Calm market periods \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}%\n",
    "}\n",
    "\\renewcommand{\\arraystretch}{1.0}\n",
    "\\end{table}\n",
    "```\n",
    "\n",
    "{numref}`tbl-5` presents sensitivity analysis examining the stability of the interest rate coefficient across different time periods and market conditions. This analysis addresses concerns about whether the findings are driven by specific periods, such as the COVID shock or the rate hike cycle, or whether they reflect a more general relationship that holds across different economic regimes. I estimate the full model specification across multiple subsamples: excluding the COVID shock period (Mar-Jun 2020), focusing on the rate hike period (Mar 2022-Jul 2023), restricting to post-2021 observations, and stratifying by high versus low volatility months.\n",
    "\n",
    "The coefficient estimates vary substantially across subsamples, ranging from +10.62 (high-volatility months) to -30.04 (low-volatility months), indicating considerable instability in the estimated relationship. This wide variation raises concerns about the robustness of the findings and suggests that the relationship may be highly context-dependent or that the estimates are sensitive to sample selection. The positive coefficient in high-volatility months is particularly notable, as it contradicts the negative relationship found in other subsamples, though this estimate is statistically insignificant (p = 0.374) and may reflect confounding effects of market stress rather than a genuine reversal of the interest rate relationship.\n",
    "\n",
    "The low-volatility months subsample shows the strongest negative relationship (β = -30.04, p = 0.009), but this finding should be interpreted with caution. With multiple subsamples tested, the risk of data mining—finding spurious patterns by searching across many specifications—is substantial. The statistical significance in this particular subsample may reflect chance variation rather than a genuine economic relationship, particularly given that the full sample estimate is not statistically significant. The wide variation in coefficient estimates across subsamples suggests that I cannot confidently conclude that interest rate sensitivity is stable or that it operates consistently across different market conditions. Instead, the evidence points to descriptive patterns that vary substantially across time periods, with limited ability to detect a stable relationship given the small sample size and high volatility of BNPL returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "01d25354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:03.796543Z",
     "iopub.status.busy": "2025-12-19T15:57:03.796355Z",
     "iopub.status.idle": "2025-12-19T15:57:03.835015Z",
     "shell.execute_reply": "2025-12-19T15:57:03.833145Z"
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "tables_dir = Path('_build/tables')\n",
    "tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Table 1\n",
    "pd.DataFrame([\n",
    "    {\"Variable\": \"BNPL Returns\", \"Symbol\": \"R_BNPL\", \"Definition\": \"Log portfolio return\", \"Source\": \"Yahoo Finance\", \"Transform\": \"Log\", \"Mean\": 1.7, \"Std Dev\": 19.0, \"Min\": -42.8, \"Max\": 41.3},\n",
    "    {\"Variable\": \"Federal Funds Rate Change\", \"Symbol\": \"Delta FFR\", \"Definition\": \"MoM change in FFR (pp)\", \"Source\": \"FRED (FEDFUNDS)\", \"Transform\": \"Diff\", \"Mean\": 0.0, \"Std Dev\": 0.2, \"Min\": -0.9, \"Max\": 0.7},\n",
    "    {\"Variable\": \"Consumer Confidence Change\", \"Symbol\": \"Delta CC\", \"Definition\": \"MoM change in UM Sentiment\", \"Source\": \"FRED (UMCSENT)\", \"Transform\": \"Diff\", \"Mean\": -0.6, \"Std Dev\": 5.2, \"Min\": -17.3, \"Max\": 9.3},\n",
    "    {\"Variable\": \"Disposable Income Change\", \"Symbol\": \"Delta DI\", \"Definition\": \"MoM % change in real income\", \"Source\": \"FRED (DSPIC96)\", \"Transform\": \"Pct\", \"Mean\": 0.3, \"Std Dev\": 4.3, \"Min\": -15.1, \"Max\": 22.9},\n",
    "    {\"Variable\": \"Inflation Change\", \"Symbol\": \"Delta pi\", \"Definition\": \"MoM % change in CPI (SA)\", \"Source\": \"FRED (CPIAUCSL)\", \"Transform\": \"Pct\", \"Mean\": 0.3, \"Std Dev\": 0.3, \"Min\": -0.8, \"Max\": 1.3},\n",
    "    {\"Variable\": \"Market Return\", \"Symbol\": \"R_MKT\", \"Definition\": \"Monthly S&P 500 return (pp)\", \"Source\": \"Yahoo Finance (SPY)\", \"Transform\": \"Pct\", \"Mean\": 1.4, \"Std Dev\": 5.0, \"Min\": -12.5, \"Max\": 12.7},\n",
    "]).to_csv(tables_dir / 'table1.csv', index=False)\n",
    "\n",
    "# Table 2\n",
    "pd.DataFrame([\n",
    "    [\"BNPL Returns\", \"1.000\", \"-0.154\", \"0.162\", \"-0.014\", \"-0.263**\", \"0.648***\"],\n",
    "    [\"Delta FFR\", \"-0.154\", \"1.000\", \"0.266**\", \"-0.102\", \"0.383***\", \"0.027\"],\n",
    "    [\"Delta Consumer Conf.\", \"0.162\", \"0.266**\", \"1.000\", \"-0.047\", \"0.161\", \"0.054\"],\n",
    "    [\"Delta Disp. Income\", \"-0.014\", \"-0.102\", \"-0.047\", \"1.000\", \"-0.224*\", \"0.103\"],\n",
    "    [\"Delta Inflation\", \"-0.263**\", \"0.383***\", \"0.161\", \"-0.224*\", \"1.000\", \"-0.095\"],\n",
    "    [\"Market Return\", \"0.648***\", \"0.027\", \"0.054\", \"0.103\", \"-0.095\", \"1.000\"],\n",
    "], columns=[\"Variable\", \"BNPL Returns\", \"Delta FFR\", \"Delta Consumer Conf.\", \"Delta Disp. Income\", \"Delta Inflation\", \"Market Return\"]).to_csv(tables_dir / 'table2.csv', index=False)\n",
    "\n",
    "# Table 3\n",
    "pd.DataFrame([\n",
    "    {\"Test\": \"Multicollinearity (VIF)\", \"Statistic\": \"All VIF < 1.2\", \"Threshold\": \"<5\", \"Result\": \"Pass\", \"Implication\": \"Estimates reliable; no multicollinearity\"},\n",
    "    {\"Test\": \"Heteroskedasticity (Breusch-Pagan)\", \"Statistic\": \"Chi2 = 1.67, p = 0.892\", \"Threshold\": \"p > 0.05\", \"Result\": \"Pass\", \"Implication\": \"Homoskedastic; HC3 SEs used as precaution\"},\n",
    "    {\"Test\": \"Autocorrelation (Durbin-Watson)\", \"Statistic\": \"DW = 2.01\", \"Threshold\": \"1.5-2.5\", \"Result\": \"Pass\", \"Implication\": \"No serial correlation; SEs valid\"},\n",
    "    {\"Test\": \"Normality (Jarque-Bera)\", \"Statistic\": \"JB = 1.69, p = 0.429\", \"Threshold\": \"p > 0.05\", \"Result\": \"Pass\", \"Implication\": \"Residuals approximately normal; inference valid\"},\n",
    "]).to_csv(tables_dir / 'table3.csv', index=False)\n",
    "\n",
    "# Table 4A and 4B\n",
    "pd.DataFrame([\n",
    "    {\"Model\": \"2. Full OLS (Primary)\", \"Specification\": \"Market + macro controls\", \"Beta (FFR)\": -12.89, \"SE\": 9.99, \"p-value\": 0.197, \"R^2\": 0.524, \"F-stat\": 12.49, \"N\": 66},\n",
    "    {\"Model\": \"1. Base OLS\", \"Specification\": \"Interest rate only\", \"Beta (FFR)\": -12.47, \"SE\": 13.03, \"p-value\": 0.338, \"R^2\": 0.024, \"F-stat\": 0.92, \"N\": 66},\n",
    "    {\"Model\": \"3. Fama-French\", \"Specification\": \"FF 3-factor + FFR\", \"Beta (FFR)\": -11.54, \"SE\": 7.95, \"p-value\": 0.147, \"R^2\": 0.521, \"F-stat\": 16.64, \"N\": 66},\n",
    "]).to_csv(tables_dir / 'table4a.csv', index=False)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"Model\": \"4. IV (2SLS)\", \"Specification\": \"Lagged FFR instrument\", \"Beta (FFR)\": -15.49, \"SE\": 16.15, \"p-value\": 0.338, \"R^2\": 0.477, \"F-stat\": 17.86, \"N\": 64},\n",
    "    {\"Model\": \"5. DiD\", \"Specification\": \"BNPL vs Market\", \"Beta (FFR)\": -13.05, \"SE\": 14.41, \"p-value\": 0.365, \"R^2\": 0.022, \"F-stat\": 0.31, \"N\": 132},\n",
    "]).to_csv(tables_dir / 'table4b.csv', index=False)\n",
    "\n",
    "# Table 5\n",
    "pd.DataFrame([\n",
    "    {\"Sample\": \"Full Sample\", \"Period\": \"Feb 2020 - Aug 2025\", \"Beta (FFR)\": -12.89, \"SE\": 9.99, \"p-value\": 0.197, \"R^2\": 0.524, \"N\": 66, \"Key Characteristics\": \"Baseline specification\"},\n",
    "    {\"Sample\": \"Exclude COVID Shock\", \"Period\": \"Excl. Mar-Jun 2020\", \"Beta (FFR)\": -11.87, \"SE\": 13.34, \"p-value\": 0.373, \"R^2\": 0.501, \"N\": 62, \"Key Characteristics\": \"Removes extreme volatility period\"},\n",
    "    {\"Sample\": \"Rate Hike Period Only\", \"Period\": \"Mar 2022 - Jul 2023\", \"Beta (FFR)\": -16.64, \"SE\": 28.28, \"p-value\": 0.556, \"R^2\": 0.714, \"N\": 17, \"Key Characteristics\": \"Fed raised 525bp; strongest tightening\"},\n",
    "    {\"Sample\": \"Post-2021\", \"Period\": \"Jan 2022 - Aug 2025\", \"Beta (FFR)\": -19.08, \"SE\": 14.56, \"p-value\": 0.190, \"R^2\": 0.630, \"N\": 44, \"Key Characteristics\": \"Excludes zero-rate period\"},\n",
    "    {\"Sample\": \"High Volatility Months\", \"Period\": \"BNPL vol > median\", \"Beta (FFR)\": 10.62, \"SE\": 11.95, \"p-value\": 0.374, \"R^2\": 0.618, \"N\": 32, \"Key Characteristics\": \"Market stress periods\"},\n",
    "    {\"Sample\": \"Low Volatility Months\", \"Period\": \"BNPL vol < median\", \"Beta (FFR)\": -30.04, \"SE\": 11.47, \"p-value\": 0.009, \"R^2\": 0.536, \"N\": 33, \"Key Characteristics\": \"Calm market periods\"},\n",
    "]).to_csv(tables_dir / 'table5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "6e6164b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:03.839284Z",
     "iopub.status.busy": "2025-12-19T15:57:03.839110Z",
     "iopub.status.idle": "2025-12-19T15:57:04.533762Z",
     "shell.execute_reply": "2025-12-19T15:57:04.533079Z"
    },
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Chart K (clean) saved as chart_k_rate_panels.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chart K (clean): BNPL vs Market with Rate-Hike Shading and Beta-Adjusted Residual\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Recompute spans for rate-hike periods (ffr_change > 0, contiguous)\n",
    "rate_hike_mask = data['ffr_change'] > 0\n",
    "spans = []\n",
    "start = None\n",
    "for date, flag in rate_hike_mask.items():\n",
    "    if flag and start is None:\n",
    "        start = date\n",
    "    if not flag and start is not None:\n",
    "        spans.append((start, date))\n",
    "        start = None\n",
    "if start is not None:\n",
    "    spans.append((start, rate_hike_mask.index[-1]))\n",
    "\n",
    "beta_hat = model_full.params['market_return'] if 'model_full' in locals() else 0\n",
    "resid_beta = data['log_returns'] - beta_hat * data['market_return']\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "\n",
    "# Panel A: BNPL\n",
    "axes[0].plot(data.index, data['log_returns'], color='#1f77b4', linewidth=1.6, label='BNPL returns')\n",
    "axes[0].axhline(0, color='#6e6e6e', linestyle='--', linewidth=1.2, alpha=0.7)\n",
    "axes[0].set_ylabel('Returns (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Panel A: BNPL Monthly Log Returns (%)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Panel B: Market\n",
    "axes[1].plot(data.index, data['market_return'], color='#ff7f0e', linewidth=1.6, label='Market returns')\n",
    "axes[1].axhline(0, color='#6e6e6e', linestyle='--', linewidth=1.2, alpha=0.7)\n",
    "axes[1].set_ylabel('Returns (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Panel B: Market (SPY) Monthly Returns (%)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Panel C: Residual\n",
    "axes[2].plot(data.index, resid_beta, color='#2ca02c', linewidth=1.6, label='BNPL − β×Market')\n",
    "axes[2].axhline(0, color='#6e6e6e', linestyle='--', linewidth=1.2, alpha=0.7)\n",
    "axes[2].set_ylabel('Residual (%)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Panel C: BNPL minus β×Market (Residual) (%)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Shading (single color, no axis labels)\n",
    "shade_color = '#f1c27d'\n",
    "for ax in axes:\n",
    "    for s, e in spans:\n",
    "        ax.axvspan(s, e, color=shade_color, alpha=0.28, zorder=-2)\n",
    "    ax.grid(True, linestyle=':', color='#d0d4d7', alpha=0.35)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Single shared legend as horizontal line at bottom, outside plot area\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], color='#1f77b4', linewidth=1.6, label='BNPL returns'),\n",
    "    Line2D([0], [0], color='#ff7f0e', linewidth=1.6, label='Market returns'),\n",
    "    Line2D([0], [0], color='#2ca02c', linewidth=1.6, label='BNPL - β×Market'),\n",
    "    Patch(facecolor=shade_color, alpha=0.28, edgecolor='none', label='Rate-hike period')\n",
    "]\n",
    "fig.legend(\n",
    "    handles=legend_handles,\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, 0.01),\n",
    "    frameon=True,\n",
    "    framealpha=0.92,\n",
    "    edgecolor='#cfd4d7',\n",
    "    facecolor='white',\n",
    "    ncol=4,\n",
    "    fontsize=10,\n",
    "    columnspacing=1.5\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.98])\n",
    "plt.savefig('chart_k_rate_panels.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    [OK] Chart K (clean) saved as chart_k_rate_panels.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "additional-viz-header",
   "metadata": {},
   "source": [
    "## Additional Visualizations: Rate-Hike Periods and Volatility Analysis\n",
    "\n",
    "```{raw} latex\n",
    "\\raggedright\n",
    "```\n",
    "\n",
    "\n",
    "Additional visualizations examine that examine BNPL stock returns during rate-hike periods and compare volatility patterns across different asset classes. These figures complement the main regression analysis by providing visual evidence on the relationship between monetary policy, market movements, and BNPL-specific factors. The visualizations decompose BNPL returns into market-driven and idiosyncratic components, track the evolution of rates and returns over time, and compare volatility levels across sectors to understand the statistical challenges in detecting interest rate sensitivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63929b",
   "metadata": {},
   "source": [
    "### BNPL vs Market with Rate-Hike Shading\n",
    "\n",
    "```{figure} chart_k_rate_panels.png\n",
    ":name: fig_rate_panels\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "BNPL vs Market with Rate-Hike Shading\n",
    "```\n",
    "\n",
    "{numref}`fig_rate_panels` presents a three-panel visualization decomposing BNPL returns into market-driven and idiosyncratic components, with rate-hike periods highlighted. Panel A shows BNPL monthly returns, Panel B overlays market returns on the same scale, and Panel C displays beta-adjusted residuals. The visualization confirms that BNPL returns closely track market movements (consistent with $\\beta$ ≈ 2.4), while residuals show no clear pattern during rate-hike periods, suggesting any rate sensitivity operates through market-wide channels rather than BNPL-specific mechanisms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "eeebbb2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:04.535847Z",
     "iopub.status.busy": "2025-12-19T15:57:04.535706Z",
     "iopub.status.idle": "2025-12-19T15:57:05.171509Z",
     "shell.execute_reply": "2025-12-19T15:57:05.170953Z"
    },
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Figure 10 saved as chart_i_timeline.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Figure 10: Timeline - Rates, BNPL vs Market, and Idiosyncratic Residual\n",
    "# ============================================================================\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Build FFR level\n",
    "if 'ffr_monthly' in locals():\n",
    "    ffr_level = ffr_monthly.reindex(data.index, method='pad')\n",
    "else:\n",
    "    ffr_level = data['ffr_change'].cumsum()\n",
    "\n",
    "beta_mkt = model_full.params['market_return'] if 'model_full' in locals() else 0\n",
    "resid_beta = data['log_returns'] - beta_mkt * data['market_return']\n",
    "\n",
    "covid_start, covid_end = pd.Timestamp('2020-03-01'), pd.Timestamp('2020-06-30')\n",
    "zero_end = pd.Timestamp('2022-02-28')\n",
    "hike_start, hike_end = pd.Timestamp('2022-03-01'), pd.Timestamp('2023-07-31')\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Panel A: FFR level\n",
    "axes[0].plot(data.index, ffr_level, color='#2c3e50', linewidth=2.0, label='FFR')\n",
    "axes[0].set_ylabel('FFR (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Rates, BNPL vs Market, and Idiosyncratic Residual', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Panel B: BNPL and Market\n",
    "axes[1].plot(data.index, data['log_returns'], color='#1f77b4', linewidth=1.6, label='BNPL returns')\n",
    "axes[1].plot(data.index, data['market_return'], color='#ff7f0e', linewidth=1.6, label='Market returns')\n",
    "axes[1].set_ylabel('Returns (%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "\n",
    "# Panel C: Residual\n",
    "axes[2].plot(data.index, resid_beta, color='#2ca02c', linewidth=1.6, label='BNPL - β×Market')\n",
    "axes[2].axhline(0, color='#2c3e50', linestyle='--', linewidth=1.2, alpha=0.8)\n",
    "axes[2].set_ylabel('Residual (%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Shading with legend patches\n",
    "shades = [\n",
    "    (data.index.min(), zero_end, '#7aa5d8', 0.28, 'Zero bound'),\n",
    "    (covid_start, covid_end, '#6f6f6f', 0.32, 'COVID shock'),\n",
    "    (hike_start, hike_end, '#f1c27d', 0.34, 'Rate hikes')\n",
    "]\n",
    "for ax in axes:\n",
    "    for s,e,c,a,_ in shades:\n",
    "        ax.axvspan(s, e, color=c, alpha=a, zorder=-2)\n",
    "    ax.grid(True, linestyle=':', color='#d0d4d7', alpha=0.35)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Single shared legend as horizontal line at bottom, outside plot area\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], color='#2c3e50', linewidth=2.0, label='FFR'),\n",
    "    Line2D([0], [0], color='#1f77b4', linewidth=1.6, label='BNPL returns'),\n",
    "    Line2D([0], [0], color='#ff7f0e', linewidth=1.6, label='Market returns'),\n",
    "    Line2D([0], [0], color='#2ca02c', linewidth=1.6, label='BNPL - β×Market'),\n",
    "    Patch(facecolor='#7aa5d8', alpha=0.28, edgecolor='none', label='Zero bound'),\n",
    "    Patch(facecolor='#6f6f6f', alpha=0.32, edgecolor='none', label='COVID shock'),\n",
    "    Patch(facecolor='#f1c27d', alpha=0.34, edgecolor='none', label='Rate hikes')\n",
    "]\n",
    "fig.legend(\n",
    "    handles=legend_handles,\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, 0.01),\n",
    "    frameon=True,\n",
    "    framealpha=0.92,\n",
    "    edgecolor='#cfd4d7',\n",
    "    facecolor='white',\n",
    "    ncol=7,\n",
    "    fontsize=10,\n",
    "    columnspacing=1.5\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.98])\n",
    "plt.savefig('chart_i_timeline.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    [OK] Figure 10 saved as chart_i_timeline.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a0810",
   "metadata": {},
   "source": [
    "### Timeline of Rates, BNPL vs Market, and Idiosyncratic Residual\n",
    "\n",
    "This subsection presents a three-panel timeline visualization that provides a comprehensive view of the relationship between monetary policy, BNPL returns, market returns, and the idiosyncratic component of BNPL returns over the sample period. The visualization tracks the Federal Funds Rate level, compares BNPL and market returns on the same scale, and shows BNPL returns net of beta-adjusted market exposure to isolate any interest rate sensitivity that operates independently of market-wide factors.\n",
    "\n",
    "```{figure} chart_i_timeline.png\n",
    ":name: fig_timeline\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Timeline of Rates, BNPL vs Market, and Idiosyncratic Residual\n",
    "```\n",
    "\n",
    "{numref}`fig_timeline` presents a three-panel timeline visualization. Panel A plots the Federal Funds Rate level, showing the zero lower bound period through February 2022, followed by the rapid tightening cycle from March 2022 to July 2023 when rates increased by 525 basis points. Panel B overlays BNPL and market returns on the same scale, allowing direct visual comparison of their co-movement patterns and highlighting periods when BNPL returns diverged from or converged with market returns. Panel C shows BNPL returns net of beta-adjusted market exposure, representing the idiosyncratic component of BNPL returns that cannot be explained by market movements, which is crucial for isolating any interest rate sensitivity that operates independently of market-wide factors. Shading marks key economic regimes: gray for the COVID shock period (March-June 2020), blue for the zero-bound period through February 2022, and red for the rate hike period (March 2022-July 2023). The visualization reveals that BNPL returns closely track market returns throughout the sample period, with the two series moving together during most months, which is consistent with the high market beta ($\\beta$ = approximately 2.4) estimated in the regression models. The residual panel shows limited rate-linked structure, with the beta-adjusted BNPL returns exhibiting no clear pattern that corresponds to the rate hike period, suggesting that any interest rate sensitivity operates primarily through market-wide channels rather than through BNPL-specific mechanisms. This pattern supports the regression finding that interest rate effects are statistically weak and economically dominated by market movements, and that the relationship between rates and BNPL returns, if it exists, is likely indirect and operates through market sentiment and risk appetite rather than through direct funding cost channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "564b1552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T15:57:05.174237Z",
     "iopub.status.busy": "2025-12-19T15:57:05.173982Z",
     "iopub.status.idle": "2025-12-19T15:57:05.585055Z",
     "shell.execute_reply": "2025-12-19T15:57:05.584119Z"
    },
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/1941665269.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
      "[*********************100%***********************]  3 of 3 completed\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_12210/1941665269.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Figure 11 saved as chart_l_volatility.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Figure 11: Volatility Comparison (BNPL vs peers)\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Helper to build equal-weight monthly log-return series (in %)\n",
    "def ew_monthly_log_returns(tickers, start_date, end_date):\n",
    "    px = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "    px = px.resample(\"ME\").last()\n",
    "    log_ret = np.log(px / px.shift(1)) * 100  # percent\n",
    "    return log_ret.mean(axis=1)\n",
    "\n",
    "start_dt = start_date\n",
    "end_dt = end_date\n",
    "\n",
    "credit_tickers = [\"AXP\", \"COF\", \"SYF\"]\n",
    "fintech_tickers = [\"SOFI\", \"UPST\", \"LC\"]\n",
    "\n",
    "credit_ret = ew_monthly_log_returns(credit_tickers, start_dt, end_dt)\n",
    "fintech_ret = ew_monthly_log_returns(fintech_tickers, start_dt, end_dt)\n",
    "\n",
    "# Align to main data index\n",
    "credit_ret = credit_ret.reindex(data.index).dropna()\n",
    "fintech_ret = fintech_ret.reindex(data.index).dropna()\n",
    "\n",
    "vol_data = {\n",
    "    'BNPL portfolio': data['log_returns'].std(),\n",
    "    'S&P 500 (SPY)': data['market_return'].std(),\n",
    "    'Credit cards (AXP, COF, SYF)': credit_ret.std(),\n",
    "    'FinTech lenders (SOFI, UPST, LC)': fintech_ret.std()\n",
    "}\n",
    "vol_df = pd.DataFrame(list(vol_data.items()), columns=['Series', 'Vol'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Unified palette across charts\n",
    "colors = [\n",
    "    '#1f77b4',  # BNPL\n",
    "    '#8c7aa9',  # SPY (Morandi purple)\n",
    "    '#2c9c9c',  # Credit cards\n",
    "    '#d97706'   # FinTech lenders\n",
    "]\n",
    "bars = ax.bar(vol_df['Series'], vol_df['Vol'], color=colors, alpha=0.9, edgecolor='#34495e')\n",
    "\n",
    "for idx, row in vol_df.iterrows():\n",
    "    ax.text(idx, row['Vol'] + 0.15, f\"{row['Vol']:.1f}%\", ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Monthly Volatility (Std Dev, %)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Volatility Comparison', fontsize=16, fontweight='bold')\n",
    "ax.grid(axis='y', linestyle=':', color='#d0d4d7', alpha=0.35)\n",
    "plt.xticks(rotation=20, ha='center', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_l_volatility.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    [OK] Figure 11 saved as chart_l_volatility.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d12f2",
   "metadata": {},
   "source": [
    "### Volatility Comparison\n",
    "\n",
    "```{figure} chart_l_volatility.png\n",
    ":name: fig_volatility\n",
    ":width: 4in\n",
    ":align: center\n",
    "\n",
    "Volatility Comparison\n",
    "```\n",
    "\n",
    "{numref}`fig_volatility` reveals that BNPL remains the most volatile asset class, with substantially higher return volatility than all comparison groups, followed by fintech lenders which exhibit similar but slightly lower volatility levels. Credit card issuers and the broad market are much steadier, with volatility levels that are roughly half that of BNPL stocks. The volatility spreads are wide and economically significant: BNPL volatility is approximately twice that of credit card issuers and roughly four times that of the broad market, while fintech lenders sit just below BNPL but still substantially above traditional financial services firms. This volatility gap has profound implications for statistical inference and economic interpretation. When monthly returns can swing 20-30% on headlines, earnings announcements, or sentiment shifts, detecting a 5-10% rate-induced move becomes statistically challenging, as the signal-to-noise ratio is extremely low. The high volatility also means that BNPL behaves like a high-beta, risk-on asset that experiences sharp drawdowns during tightening cycles and rapid rebounds when risk appetite returns, patterns that are driven primarily by market sentiment rather than fundamental interest rate sensitivity. For portfolio construction and risk management, BNPL exposure carries materially higher idiosyncratic and systematic risk than traditional card issuers, requiring different hedging strategies and risk tolerance. The high volatility suggests that any interest rate sensitivity is likely to be masked by this noise unless rate shocks are very large or persistent, which helps explain why the regression analysis finds statistically weak interest rate effects despite economically meaningful coefficient magnitudes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "jupytext": {
    "cell_metadata_filter": "all,-hidden,-heading_collapsed,-run_control,-trusted",
    "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
   }
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "mystnb": {
   "execution_mode": "cache",
   "render_markdown_format": "myst"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
