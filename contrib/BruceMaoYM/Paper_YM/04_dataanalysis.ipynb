{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3f4c21",
   "metadata": {},
   "source": [
    "## Data Analysis: Investigating BNPL Stock Returns and Monetary Policy\n",
    "\n",
    "This section implements a comprehensive empirical analysis using modern econometric techniques and reproducible research practices. The analysis is situated within a rapidly evolving market context: the global BNPL market is projected to reach $560.1 billion in gross merchandise volume by 2025, reflecting 13.7% year-over-year growth, with user adoption accelerating toward 900 million globally by 2027 {cite}`Chargeflow2025`. This explosive growth, a 157% increase from 360 million users in 2022, underscores the sector's increasing importance in consumer credit markets and motivates careful examination of how these firms respond to monetary policy changes.\n",
    "\n",
    "### Computational Environment and Research Tools\n",
    "\n",
    "The analysis uses Python 3.11 with specialized libraries at each step. `pandas` (v2.2.3) handles data and time alignment; `statsmodels` (v0.14.4) provides HC3 robust inference suited to this sample size; `yfinance` supplies stock prices; `fredapi` pulls Federal Funds Rate, CPI, sentiment, and income series; `matplotlib` (v3.9.2) and `seaborn` (v0.13.2) generate publication-quality visuals.\n",
    "\n",
    "### Reproducibility and Dynamic Document Generation\n",
    "\n",
    "All data collection, transformation, and estimation are programmatic for full reproducibility. With the required API keys, any reader can rerun the analysis and obtain identical outputs. Tables and figures are glued directly from code via `myst_nb.glue`, avoiding transcription errors and keeping the manuscript synchronized with computations.\n",
    "\n",
    "The analysis environment is fully documented in `binder/environment.yml`, specifying exact package versions to ensure that the computational environment can be reconstructed. This documentation follows the principles outlined in the Journal of Open Source Software and enables other researchers to validate, extend, or build upon this work.\n",
    "\n",
    "### Analytical Pipeline Overview\n",
    "\n",
    "The analysis proceeds through six integrated stages, each building upon the previous to construct a coherent analytical narrative. First, data are collected from authoritative sources and variables constructed with appropriate transformations, including log transformation of returns and first-differencing of macroeconomic series to ensure stationarity. Second, exploratory visualizations identify patterns, outliers, and preliminary relationships that inform model specification. Third, correlation analysis assesses multicollinearity among predictors and provides initial evidence on bivariate associations. Fourth, formal econometric models are estimated across multiple specifications, including OLS, Fama-French three-factor, instrumental variables, and difference-in-differences approaches, to test the interest rate hypothesis under different identifying assumptions. Fifth, diagnostic tests validate model assumptions including homoskedasticity, absence of autocorrelation, normality of residuals, and absence of multicollinearity, ensuring reliable inference. Finally, sensitivity analysis examines robustness across different time periods and market conditions, addressing concerns about the stability of findings. The following subsections present each stage in turn, with full transparency about methodological choices and their implications.\n",
    "\n",
    "**Sample size and power.** The empirical window of 66 monthly observations (Feb 2020–Aug 2025) limits statistical power; with the observed coefficient magnitudes, power is only about 15–20%. Reported coefficients should be read as descriptive sensitivities rather than precise hypothesis tests.\n",
    "\n",
    "**Transformations and units.** Returns are log(1+R), implemented via log price differences; this keeps units in percentage points and bounds losses at -100%. Federal Funds Rate changes enter in percentage points (0.25 = 25bp). Control variables are first-differenced or expressed in percentage changes to focus on shocks rather than levels.\n",
    "\n",
    "**Model positioning.** The full specification with market, inflation, confidence, and disposable income is the primary model. The rate-only base model is retained as a robustness illustration of omitted-variable bias rather than as a headline result. Market beta plays a central role in interpretation because BNPL stocks price like high-beta growth assets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae19c6b",
   "metadata": {},
   "source": [
    "### Variable Definitions and Data Sources\n",
    "\n",
    "(tbl-variables)=\n",
    "**Table 1: Variable Definitions and Summary Statistics**\n",
    "\n",
    "Table 1 is rendered from the code cell below (`table_1`) and refreshes when you rerun the notebook. It reports the BNPL portfolio and control variables (FFR change, consumer confidence, disposable income, CPI, market return) with their transforms and summary stats for the current sample (default Feb 2020–Aug 2025).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1de52",
   "metadata": {},
   "source": [
    "### Firm-Level Context (Condensed)\n",
    "\n",
    "The BNPL portfolio spans three distinct business models. Affirm is the pure-play BNPL name, funding via warehouse lines and securitizations, so funding-cost pass-through is direct. Sezzle focuses on smaller-ticket, younger consumers with thinner margins and higher funding sensitivity. PayPal is a diversified payments platform with BNPL as a smaller product line (Pay in 4), so diversification and deposits/merchant float dampen BNPL-specific funding shocks. This mix balances depth of price history with exposure to funding-sensitive and diversified models; full firm-level details remain in Appendix A.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "id": "136221a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:01.835035Z",
     "iopub.status.busy": "2025-12-05T00:14:01.834648Z",
     "iopub.status.idle": "2025-12-05T00:14:05.306342Z",
     "shell.execute_reply": "2025-12-05T00:14:05.305820Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/323937872.py:19: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  bnpl_data = yf.download(bnpl_tickers, start=start_date, end=end_date)['Close']\n",
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Variable Definitions and Summary Statistics (Feb 2020 - Aug 2025)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Symbol</th>\n      <th>Definition</th>\n      <th>Source</th>\n      <th>Transform</th>\n      <th>Mean</th>\n      <th>Std Dev</th>\n      <th>Min</th>\n      <th>Max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BNPL Returns</td>\n      <td>R_BNPL</td>\n      <td>Log portfolio return</td>\n      <td>Yahoo Finance</td>\n      <td>Log</td>\n      <td>1.7</td>\n      <td>19.0</td>\n      <td>-42.8</td>\n      <td>41.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Federal Funds Rate Change</td>\n      <td>ΔFFR</td>\n      <td>MoM change in FFR (pp)</td>\n      <td>FRED (FEDFUNDS)</td>\n      <td>Diff</td>\n      <td>0.0</td>\n      <td>0.2</td>\n      <td>-0.9</td>\n      <td>0.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Consumer Confidence Change</td>\n      <td>ΔCC</td>\n      <td>MoM change in UM Sentiment</td>\n      <td>FRED (UMCSENT)</td>\n      <td>Diff</td>\n      <td>-0.6</td>\n      <td>5.2</td>\n      <td>-17.3</td>\n      <td>9.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Disposable Income Change</td>\n      <td>ΔDI</td>\n      <td>MoM % change in real income</td>\n      <td>FRED (DSPIC96)</td>\n      <td>Pct</td>\n      <td>0.3</td>\n      <td>4.3</td>\n      <td>-15.1</td>\n      <td>22.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Inflation Change</td>\n      <td>Δπ</td>\n      <td>MoM % change in CPI (SA)</td>\n      <td>FRED (CPIAUCSL)</td>\n      <td>Pct</td>\n      <td>0.3</td>\n      <td>0.3</td>\n      <td>-0.8</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Market Return</td>\n      <td>R_MKT</td>\n      <td>Monthly S&amp;P 500 return (pp)</td>\n      <td>Yahoo Finance (SPY)</td>\n      <td>Pct</td>\n      <td>1.4</td>\n      <td>5.0</td>\n      <td>-12.5</td>\n      <td>12.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                     Variable  Symbol                   Definition  \\\n0                BNPL Returns  R_BNPL         Log portfolio return   \n1   Federal Funds Rate Change    ΔFFR       MoM change in FFR (pp)   \n2  Consumer Confidence Change     ΔCC   MoM change in UM Sentiment   \n3    Disposable Income Change     ΔDI  MoM % change in real income   \n4            Inflation Change      Δπ     MoM % change in CPI (SA)   \n5               Market Return   R_MKT  Monthly S&P 500 return (pp)   \n\n                Source Transform  Mean Std Dev    Min   Max  \n0        Yahoo Finance       Log   1.7    19.0  -42.8  41.3  \n1      FRED (FEDFUNDS)      Diff   0.0     0.2   -0.9   0.7  \n2       FRED (UMCSENT)      Diff  -0.6     5.2  -17.3   9.3  \n3       FRED (DSPIC96)       Pct   0.3     4.3  -15.1  22.9  \n4      FRED (CPIAUCSL)       Pct   0.3     0.3   -0.8   1.3  \n5  Yahoo Finance (SPY)       Pct   1.4     5.0  -12.5  12.7  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-1"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: n = 66 monthly observations (Feb 2020 - Aug 2025).\n",
      "Transforms: Diff = first difference; Pct = percentage change; Log = log return.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 1: Variable Definitions and Summary Statistics\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "from fredapi import Fred\n",
    "from myst_nb import glue\n",
    "\n",
    "# Load data\n",
    "FRED_API_KEY = os.environ.get('FRED_API_KEY')\n",
    "start_date = datetime(2020, 2, 1)\n",
    "end_date = datetime(2025, 8, 31)\n",
    "\n",
    "# BNPL stocks\n",
    "bnpl_tickers = ['AFRM', 'SEZL', 'PYPL']\n",
    "bnpl_data = yf.download(bnpl_tickers, start=start_date, end=end_date)['Close']\n",
    "bnpl_monthly = bnpl_data.resample('ME').last()\n",
    "bnpl_returns = np.log(bnpl_monthly / bnpl_monthly.shift(1))\n",
    "log_returns = bnpl_returns.mean(axis=1) * 100  # Convert to percentage\n",
    "\n",
    "# FRED data\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "ffr = fred.get_series('FEDFUNDS', start=start_date, end=end_date)\n",
    "ffr_monthly = ffr.resample('ME').last()\n",
    "ffr_change = ffr_monthly.diff()\n",
    "\n",
    "cc = fred.get_series('UMCSENT', start=start_date, end=end_date)\n",
    "cc_monthly = cc.resample('ME').last()\n",
    "cc_change = cc_monthly.diff()\n",
    "\n",
    "di = fred.get_series('DSPIC96', start=start_date, end=end_date)\n",
    "di_monthly = di.resample('ME').last()\n",
    "di_change = di_monthly.pct_change() * 100\n",
    "\n",
    "cpi = fred.get_series('CPIAUCSL', start=start_date, end=end_date)\n",
    "cpi_monthly = cpi.resample('ME').last()\n",
    "cpi_change = cpi_monthly.pct_change() * 100\n",
    "\n",
    "spy = yf.Ticker(\"SPY\")\n",
    "spy_hist = spy.history(start=start_date, end=end_date)\n",
    "spy_monthly = spy_hist['Close'].resample('ME').last()\n",
    "if spy_monthly.index.tz is not None:\n",
    "    spy_monthly.index = spy_monthly.index.tz_localize(None)\n",
    "market_return = spy_monthly.pct_change() * 100\n",
    "\n",
    "# Combine into DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'log_returns': log_returns,\n",
    "    'ffr_change': ffr_change,\n",
    "    'cc_change': cc_change,\n",
    "    'di_change': di_change,\n",
    "    'cpi_change': cpi_change,\n",
    "    'market_return': market_return\n",
    "}).dropna()\n",
    "\n",
    "# Generate Table 1\n",
    "var_info = [\n",
    "    ('BNPL Returns', 'R_BNPL', 'Log portfolio return', 'Yahoo Finance', 'Log', 'log_returns'),\n",
    "    ('Federal Funds Rate Change', 'ΔFFR', 'MoM change in FFR (pp)', 'FRED (FEDFUNDS)', 'Diff', 'ffr_change'),\n",
    "    ('Consumer Confidence Change', 'ΔCC', 'MoM change in UM Sentiment', 'FRED (UMCSENT)', 'Diff', 'cc_change'),\n",
    "    ('Disposable Income Change', 'ΔDI', 'MoM % change in real income', 'FRED (DSPIC96)', 'Pct', 'di_change'),\n",
    "    ('Inflation Change', 'Δπ', 'MoM % change in CPI (SA)', 'FRED (CPIAUCSL)', 'Pct', 'cpi_change'),\n",
    "    ('Market Return', 'R_MKT', 'Monthly S&P 500 return (pp)', 'Yahoo Finance (SPY)', 'Pct', 'market_return')\n",
    "]\n",
    "\n",
    "summary_stats = []\n",
    "for var_name, symbol, definition, source, transform, col in var_info:\n",
    "    if col in data.columns:\n",
    "        summary_stats.append({\n",
    "            'Variable': var_name,\n",
    "            'Symbol': symbol,\n",
    "            'Definition': definition,\n",
    "            'Source': source,\n",
    "            'Transform': transform,\n",
    "            'Mean': f\"{data[col].mean():.1f}\",\n",
    "            'Std Dev': f\"{data[col].std():.1f}\",\n",
    "            'Min': f\"{data[col].min():.1f}\",\n",
    "            'Max': f\"{data[col].max():.1f}\"\n",
    "        })\n",
    "\n",
    "table_1 = pd.DataFrame(summary_stats)\n",
    "print(\"Table 1: Variable Definitions and Summary Statistics (Feb 2020 - Aug 2025)\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-1\", table_1, display=False)\n",
    "print(f\"\\nNote: n = {len(data)} monthly observations (Feb 2020 - Aug 2025).\")\n",
    "print(\"Transforms: Diff = first difference; Pct = percentage change; Log = log return.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "id": "a64e6494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:05.307915Z",
     "iopub.status.busy": "2025-12-05T00:14:05.307798Z",
     "iopub.status.idle": "2025-12-05T00:14:05.314946Z",
     "shell.execute_reply": "2025-12-05T00:14:05.314402Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2: Correlation Matrix (stars = significance)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/3720566564.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.154' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.162' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.014' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.263**' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/3720566564.py:34: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.648***' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BNPL Returns</th>\n      <th>Δ FFR</th>\n      <th>Δ Consumer Conf.</th>\n      <th>Δ Disp. Income</th>\n      <th>Δ Inflation</th>\n      <th>Market Return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BNPL Returns</th>\n      <td>1.000</td>\n      <td>-0.154</td>\n      <td>0.162</td>\n      <td>-0.014</td>\n      <td>-0.263**</td>\n      <td>0.648***</td>\n    </tr>\n    <tr>\n      <th>Δ FFR</th>\n      <td>-0.154</td>\n      <td>1.000</td>\n      <td>0.266**</td>\n      <td>-0.102</td>\n      <td>0.383***</td>\n      <td>0.027</td>\n    </tr>\n    <tr>\n      <th>Δ Consumer Conf.</th>\n      <td>0.162</td>\n      <td>0.266**</td>\n      <td>1.000</td>\n      <td>-0.047</td>\n      <td>0.161</td>\n      <td>0.054</td>\n    </tr>\n    <tr>\n      <th>Δ Disp. Income</th>\n      <td>-0.014</td>\n      <td>-0.102</td>\n      <td>-0.047</td>\n      <td>1.000</td>\n      <td>-0.224*</td>\n      <td>0.103</td>\n    </tr>\n    <tr>\n      <th>Δ Inflation</th>\n      <td>-0.263**</td>\n      <td>0.383***</td>\n      <td>0.161</td>\n      <td>-0.224*</td>\n      <td>1.000</td>\n      <td>-0.095</td>\n    </tr>\n    <tr>\n      <th>Market Return</th>\n      <td>0.648***</td>\n      <td>0.027</td>\n      <td>0.054</td>\n      <td>0.103</td>\n      <td>-0.095</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                 BNPL Returns     Δ FFR Δ Consumer Conf. Δ Disp. Income  \\\nBNPL Returns            1.000    -0.154            0.162         -0.014   \nΔ FFR                  -0.154     1.000          0.266**         -0.102   \nΔ Consumer Conf.        0.162   0.266**            1.000         -0.047   \nΔ Disp. Income         -0.014    -0.102           -0.047          1.000   \nΔ Inflation          -0.263**  0.383***            0.161        -0.224*   \nMarket Return        0.648***     0.027            0.054          0.103   \n\n                 Δ Inflation Market Return  \nBNPL Returns        -0.263**      0.648***  \nΔ FFR               0.383***         0.027  \nΔ Consumer Conf.       0.161         0.054  \nΔ Disp. Income       -0.224*         0.103  \nΔ Inflation            1.000        -0.095  \nMarket Return         -0.095         1.000  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-2"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: n = 66 monthly observations.\n",
      "Stars: * p<0.10, ** p<0.05, *** p<0.01. |r| ≥ 0.25 is significant at 5% with n=66.\n",
      "Correlations below |0.80| indicate no severe multicollinearity concerns.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 2: Correlation Matrix\n",
    "# ============================================================================\n",
    "\n",
    "# Rename columns for display\n",
    "corr_data = data.rename(columns={\n",
    "    'log_returns': 'BNPL Returns',\n",
    "    'ffr_change': 'Δ FFR',\n",
    "    'cc_change': 'Δ Consumer Conf.',\n",
    "    'di_change': 'Δ Disp. Income',\n",
    "    'cpi_change': 'Δ Inflation',\n",
    "    'market_return': 'Market Return'\n",
    "})\n",
    "\n",
    "corr_matrix = corr_data.corr()\n",
    "\n",
    "# Compute p-values for significance stars\n",
    "from scipy import stats\n",
    "pvals = pd.DataFrame(np.ones_like(corr_matrix), index=corr_matrix.index, columns=corr_matrix.columns)\n",
    "for i, col_i in enumerate(corr_data.columns):\n",
    "    for j, col_j in enumerate(corr_data.columns):\n",
    "        if i < j:\n",
    "            r, p = stats.pearsonr(corr_data[col_i], corr_data[col_j])\n",
    "            pvals.loc[col_i, col_j] = p\n",
    "            pvals.loc[col_j, col_i] = p\n",
    "\n",
    "star = lambda p: '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n",
    "annotated = corr_matrix.copy()\n",
    "for i, col_i in enumerate(corr_matrix.index):\n",
    "    for j, col_j in enumerate(corr_matrix.columns):\n",
    "        if i == j:\n",
    "            annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}\"\n",
    "        else:\n",
    "            annotated.loc[col_i, col_j] = f\"{corr_matrix.loc[col_i, col_j]:.3f}{star(pvals.loc[col_i, col_j])}\"\n",
    "\n",
    "print(\"Table 2: Correlation Matrix (stars = significance)\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-2\", annotated, display=False)\n",
    "print(f\"\\nNote: n = {len(data)} monthly observations.\")\n",
    "print(\"Stars: * p<0.10, ** p<0.05, *** p<0.01. |r| ≥ 0.25 is significant at 5% with n=66.\")\n",
    "print(\"Correlations below |0.80| indicate no severe multicollinearity concerns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "control_variable_selection_interpretation",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis: Visualizations\n",
    "\n",
    "Before proceeding to formal econometric estimation, exploratory visualization provides crucial insights into the data structure. The following graphical representations serve multiple purposes: they help identify patterns that motivate specific model specifications, reveal potential outliers or data quality issues that could distort regression results, provide intuition for the relationships that will be estimated econometrically, and offer visual confirmation that complements numerical results. The visualizations presented here establish the empirical foundation upon which the regression analysis builds.\n",
    "\n",
    "### Figure 1: BNPL Portfolio Monthly Returns (Feb 2020–Aug 2025)\n",
    "\n",
    "(chart-a)=\n",
    "![BNPL Portfolio Monthly Returns (Feb 2020–Aug 2025)](chart_a_time_series.png)\n",
    "\n",
    "Figure 1 shows monthly log returns for an equally weighted portfolio of Affirm, Sezzle, and PayPal. Gray shading marks the COVID shock (Mar–Jun 2020), blue shading shows the zero-bound period through Feb 2022, and red shading marks the Fed’s tightening (Mar 2022–Jul 2023, +525 bp). BNPL returns are highly volatile (SD ≈ 19%), with swings above ±40%; the plot includes a thick zero line and annotations for the start of hikes and peak volatility. The mean return of 1.7% masks substantial variation, and sharp declines during 2022–2023 coincide with funding cost increases documented by {cite}`Laudenbach2025`. These regimes and callouts correspond directly to the shaded blocks and arrows on the chart.\n",
    "\n",
    "The period of strong positive returns in late 2020 and 2021 reflects the rapid growth in BNPL adoption documented by the {cite}`CFPB2025ConsumerUse`, as consumers turned to alternative payment methods during the pandemic. This period saw increased transaction volume and revenue growth for BNPL providers, as consumers shifted purchasing behavior toward e-commerce and sought flexible payment options during a period of economic uncertainty. The sharp negative returns observed in mid-2022 align with rising interest rates and increased funding costs, consistent with the {cite}`CFPB2022MarketTrends` documentation that BNPL firms' cost of funds increased substantially during this period. Higher interest rates compressed profit margins and reduced investor confidence, as the sector's thin margins (provider revenues represent only about 4% of gross merchandise volume according to {cite}`DigitalSilk2025`) made firms particularly vulnerable to funding cost increases.\n",
    "\n",
    "The period from late 2023 through 2025 exhibits continued volatility, reflecting ongoing sensitivity to monetary policy changes, macroeconomic conditions, and sector-specific developments. This persistent volatility motivates this analysis, which seeks to identify systematic factors that explain this observed variation.\n",
    "\n",
    "### Figure 2: BNPL vs Market Returns\n",
    "\n",
    "(chart-b)=\n",
    "![BNPL vs Market Returns](chart_b_market.png)\n",
    "\n",
    "Figure 2 plots BNPL portfolio returns against market returns. The slope of 2.38 (from the full model) and correlation of 0.65 (R² ≈ 0.42) show that market movements dominate BNPL pricing: when the market moves 1%, BNPL moves about 2.38%. The 45° reference line highlights amplification relative to the market. This strong market link explains why the rate-only model (R² ≈ 0.02) adds little explanatory power; interest rate effects are economically meaningful but overwhelmed by systematic risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1258,
   "id": "chart_a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:05.316451Z",
     "iopub.status.busy": "2025-12-05T00:14:05.316320Z",
     "iopub.status.idle": "2025-12-05T00:14:13.939165Z",
     "shell.execute_reply": "2025-12-05T00:14:13.938259Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Clean Chart A saved as chart_a_time_series.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chart A: Time Series of Log BNPL Returns (Clean + Publication Quality)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Shaded periods (cleaner stacking)\n",
    "covid_start, covid_end = pd.Timestamp(\"2020-03-01\"), pd.Timestamp(\"2020-06-30\")\n",
    "zero_end = pd.Timestamp(\"2022-02-28\")\n",
    "hike_start, hike_end = pd.Timestamp(\"2022-03-01\"), pd.Timestamp(\"2023-07-31\")\n",
    "ax.axvspan(data.index.min(), zero_end, color=\"#7aa5d8\", alpha=0.28, label=\"Zero bound\", zorder=-2)\n",
    "ax.axvspan(covid_start, covid_end, color=\"#6f6f6f\", alpha=0.32, label=\"COVID shock\", zorder=-1)\n",
    "ax.axvspan(hike_start, hike_end, color=\"#f1c27d\", alpha=0.34, label=\"Rate hikes\", zorder=-2)\n",
    "\n",
    "# Raw log returns\n",
    "ax.plot(\n",
    "    data.index,\n",
    "    data[\"log_returns\"],\n",
    "    linewidth=1.6,\n",
    "    color=\"#1f77b4\",\n",
    "    alpha=0.80,\n",
    "    label=\"BNPL returns\",\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Rolling mean (clean + not too thick)\n",
    "# ---------------------------\n",
    "rolling_mean = data[\"log_returns\"].rolling(window=6, min_periods=1).mean()\n",
    "\n",
    "ax.plot(\n",
    "    data.index,\n",
    "    rolling_mean,\n",
    "    linewidth=2.6,\n",
    "    color=\"#1f2d3a\",\n",
    "    label=\"6M rolling mean\",\n",
    "    zorder=4\n",
    ")\n",
    "\n",
    "# Zero line\n",
    "ax.axhline(\n",
    "    y=0,\n",
    "    color=\"#2c3e50\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.8,\n",
    "    alpha=0.95,\n",
    "    label=\"Zero line\",\n",
    "    zorder=2\n",
    ")\n",
    "\n",
    "# Y-axis padding\n",
    "ymin = data[\"log_returns\"].min()\n",
    "ymax = data[\"log_returns\"].max()\n",
    "pad = (ymax - ymin) * 0.10\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "# Annotations\n",
    "peak_date = data[\"log_returns\"].abs().idxmax()\n",
    "peak_val = data.loc[peak_date, \"log_returns\"]\n",
    "peak_text = f\"Peak volatility ({peak_date.strftime('%b %Y')})\"\n",
    "# Place label just above the x-axis around Feb 2022\n",
    "peak_text_y = ymin - 0.06 * (ymax - ymin)\n",
    "ax.annotate(\n",
    "    peak_text,\n",
    "    xy=(peak_date, peak_val),\n",
    "    xytext=(pd.Timestamp(\"2022-02-28\"), peak_text_y),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"#1f2d3a\", linewidth=1.05, shrinkA=2, shrinkB=2),\n",
    "    fontsize=12,\n",
    "    color=\"#111\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"#b7bdc3\", alpha=0.94)\n",
    ")\n",
    "hike_y = data.loc[hike_start, \"log_returns\"] if hike_start in data.index else 0\n",
    "ax.annotate(\n",
    "    \"Liftoff (Mar 2022)\",\n",
    "    xy=(hike_start, hike_y),\n",
    "    xytext=(hike_start - pd.DateOffset(days=120), hike_y + 16),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"#755314\", linewidth=1.2, shrinkA=2, shrinkB=2, connectionstyle=\"arc3,rad=0.32\"),\n",
    "    fontsize=12,\n",
    "    color=\"#3d3a2a\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"#f2d59a\", alpha=0.96)\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"BNPL Portfolio Monthly Returns (Feb 2020–Aug 2025)\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Date\", fontsize=14, fontweight=\"bold\", labelpad=8)\n",
    "ax.set_ylabel(\"Log Returns (%)\", fontsize=14, fontweight=\"bold\", labelpad=8)\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "\n",
    "# Cleaner x-axis: quarterly ticks with Q labels (consistent with Chart M)\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def fmt_qtr(x, pos=None):\n",
    "    dt = mdates.num2date(x)\n",
    "    q = (dt.month - 1) // 3 + 1\n",
    "    return f\"Q{q} {dt.year}\"\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[3, 6, 9, 12]))\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(fmt_qtr))\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "ax.grid(True, linestyle=\":\", color=\"#bfc5c9\", alpha=0.35)\n",
    "ax.legend(\n",
    "    fontsize=11,\n",
    "    loc=\"upper left\",\n",
    "    bbox_to_anchor=(0.005, 0.99),\n",
    "    frameon=True,\n",
    "    framealpha=0.92,\n",
    "    edgecolor=\"#c5c5c5\",\n",
    "    facecolor=\"white\",\n",
    "    ncol=3\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_a_time_series.png\", dpi=400, facecolor=\"white\", bbox_inches=\"tight\", pad_inches=0.25)\n",
    "plt.close()\n",
    "\n",
    "print(\"    ✓ Clean Chart A saved as chart_a_time_series.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "id": "chart_b_code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:14:13.940872Z",
     "iopub.status.busy": "2025-12-05T00:14:13.940675Z",
     "iopub.status.idle": "2025-12-05T00:15:00.620875Z",
     "shell.execute_reply": "2025-12-05T00:15:00.620088Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Chart B (BNPL vs Market)...\n",
      "    ✓ Chart B saved as chart_b_market.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chart B: BNPL Returns vs Market Returns (Dominant Relationship)\n",
    "# ============================================================================\n",
    "print(\"  Creating Chart B (BNPL vs Market)...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Ensure full model for beta\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "beta_mkt = model_full.params['market_return']\n",
    "r_mkt = data['log_returns'].corr(data['market_return'])\n",
    "r2_mkt = r_mkt ** 2\n",
    "\n",
    "x = data['market_return']\n",
    "y = data['log_returns']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8.5))\n",
    "\n",
    "ax.scatter(x, y, s=48, color=\"#1f77b4\", alpha=0.55, edgecolors=\"none\", label='Observations')\n",
    "\n",
    "# Regression line using full-model beta and intercept\n",
    "x_grid = np.linspace(x.min(), x.max(), 200)\n",
    "y_pred = model_full.params['const'] + beta_mkt * x_grid\n",
    "ax.plot(x_grid, y_pred, color=\"#d62728\", linewidth=2.6, label='OLS fit (red line)')\n",
    "\n",
    "# 45-degree reference line\n",
    "line_min = min(x.min(), y.min())\n",
    "line_max = max(x.max(), y.max())\n",
    "pad = (line_max - line_min) * 0.05\n",
    "ax.plot([line_min - pad, line_max + pad], [line_min - pad, line_max + pad], color=\"#777\", linestyle=\"--\", linewidth=1.4, alpha=0.55, label='45° reference line (gray dashed)')\n",
    "\n",
    "\n",
    "ax.set_title(\"BNPL vs Market Returns (Full-model beta)\", fontsize=19, fontweight=\"bold\", pad=14)\n",
    "ax.set_xlabel(\"Market Returns (%)\", fontsize=16, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"BNPL Returns (%)\", fontsize=16, fontweight=\"bold\")\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "ax.set_xticks(np.arange(-40, 50, 10))\n",
    "ax.set_yticks(np.arange(-40, 50, 10))\n",
    "ax.grid(True, linestyle=\":\", color=\"#d0d4d7\", alpha=0.38)\n",
    "\n",
    "# Explicit legend for each element\n",
    "legend_handles = [\n",
    "    Line2D([], [], color='none', label=f\"Full model β = {beta_mkt:.2f}, r = {r_mkt:.2f} (R² = {r2_mkt:.2f})\"),\n",
    "    Line2D([0], [0], marker='o', linestyle='None', markersize=7, color=\"#1f77b4\", alpha=0.65, label='Observations'),\n",
    "    Line2D([0], [0], color=\"#d62728\", linewidth=2.6, label='OLS fit (full model beta)'),\n",
    "    Line2D([0], [0], color=\"#555\", linestyle='--', linewidth=1.6, label='45° reference line (gray dashed)')\n",
    "]\n",
    "ax.legend(handles=legend_handles, loc='lower right', bbox_to_anchor=(0.98, 0.02), fontsize=11, frameon=True, framealpha=0.9, edgecolor='#d7dce2', facecolor='white', handlelength=1.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"chart_b_market.png\", dpi=400, facecolor=\"white\", bbox_inches=\"tight\", pad_inches=0.25)\n",
    "plt.close()\n",
    "print(\"    ✓ Chart B saved as chart_b_market.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78325d27",
   "metadata": {},
   "source": [
    "## Functional Form Selection: Log-Linear Specification\n",
    "\n",
    "The exploratory analysis revealed substantial variation in BNPL returns and a modest negative correlation with interest rate changes. Translating these observations into formal statistical inference requires specifying the functional form of the relationship, a critical methodological decision that affects both the statistical properties of estimators and the economic interpretation of results.\n",
    "\n",
    "This analysis employs a log-linear specification where the dependent variable (BNPL portfolio returns) is log-transformed while independent variables enter linearly. The log-linear specification is grounded in both theoretical and practical considerations from financial econometrics. From a theoretical perspective, log returns possess desirable properties for financial analysis: they are time-additive (the log return over multiple periods equals the sum of single-period log returns), bounded below by -100% (preventing the mathematical impossibility of negative prices), and approximately normally distributed for short horizons. These properties facilitate statistical inference and align with continuous-time asset pricing models widely used in academic finance.\n",
    "\n",
    "From a practical perspective, the log transformation addresses heteroskedasticity, the tendency for return variance to scale with return magnitude, which would otherwise violate OLS assumptions and invalidate standard errors. The transformation also normalizes the right-skewed distribution characteristic of raw returns, improving the finite-sample properties of regression estimators. Finally, coefficients in the log-linear specification have intuitive semi-elasticity interpretations: a coefficient of β = -12.89 indicates that a one percentage point increase in the Federal Funds Rate is associated with approximately 12.68% lower BNPL returns, holding other factors constant.\n",
    "\n",
    "The analysis estimates two primary specifications to assess robustness and quantify the importance of control variables. The base model regresses log BNPL returns solely on Federal Funds Rate changes, providing an unconditional estimate of interest rate sensitivity that may be confounded by omitted variables. The full model augments this with controls for consumer confidence, disposable income, inflation, and market returns, factors identified in the literature review as potential confounders. Comparing coefficients across specifications reveals whether the interest rate relationship is robust to the inclusion of controls or driven by omitted variable bias. With the functional form established, the analysis now turns to the estimation methodology.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_improvement_section",
   "metadata": {},
   "source": [
    "## Regression Analysis: Methodology\n",
    "\n",
    "With the functional form specified, this section details the estimation approach, interpretation framework, and statistical considerations that guide the regression analysis. The methodology is designed to provide credible estimates of interest rate sensitivity while acknowledging the limitations inherent in observational data and the challenges of causal inference in macroeconomic settings.\n",
    "\n",
    "### Estimation Approach and Software Implementation\n",
    "\n",
    "The regression analysis employs Ordinary Least Squares (OLS) estimation with heteroskedasticity-consistent standard errors, implemented using Python's `statsmodels` library. The choice of OLS follows from the Gauss-Markov theorem, which establishes that OLS provides the Best Linear Unbiased Estimator (BLUE) under classical assumptions. While financial data often violate the homoskedasticity assumption, the use of HC3 robust standard errors (also known as MacKinnon-White standard errors) ensures valid inference without requiring constant error variance.\n",
    "\n",
    "Two primary specifications are estimated to assess robustness and quantify the importance of control variables. The base model regresses log BNPL returns solely on Federal Funds Rate changes, providing an unconditional estimate of interest rate sensitivity that serves as a benchmark but may be confounded by omitted variables. The full model augments this specification with controls for consumer confidence, disposable income, inflation, and market returns, factors identified in the literature review as potential confounders that affect both interest rates and BNPL returns. Comparing coefficients across specifications reveals whether the interest rate relationship is robust to the inclusion of controls or driven by omitted variable bias.\n",
    "\n",
    "Beyond OLS, the analysis implements three alternative identification strategies to assess robustness. The Fama-French specification controls for exposure to systematic risk factors (market, size, and value) using factor returns downloaded from Kenneth French's data library, asking whether BNPL interest rate sensitivity persists after accounting for standard asset pricing factors. The instrumental variables specification uses lagged Federal Funds Rate changes as an instrument for current changes, exploiting the persistence in monetary policy to address potential simultaneity bias. The difference-in-differences specification compares BNPL returns to market returns during rate change periods versus stable periods, providing yet another identification strategy with different assumptions.\n",
    "\n",
    "### Interpretation Framework: Associations vs. Causation\n",
    "\n",
    "The regression estimates presented in this analysis capture conditional associations between BNPL stock returns and interest rate changes, controlling for market movements, consumer confidence, disposable income, and inflation. These estimates reveal how BNPL returns co-move with monetary policy changes after accounting for other economic factors, providing evidence on whether BNPL stocks exhibit sensitivity patterns consistent with theoretical predictions about interest rate transmission to fintech credit providers.\n",
    "\n",
    "However, these estimates should be interpreted as associations rather than causal effects. Interest rate changes are endogenous policy responses to economic conditions that simultaneously affect both monetary policy and BNPL stock valuations. The Federal Reserve adjusts rates in response to inflation, economic growth, and financial stability concerns, all factors that independently influence BNPL returns through consumer demand, credit risk, and market sentiment channels. Consequently, the regression coefficients capture associations rather than the isolated causal impact of interest rate changes on BNPL stock prices.\n",
    "\n",
    "### Potential Confounding Factors\n",
    "\n",
    "Several factors might affect both interest rates and BNPL returns simultaneously, making it difficult to isolate the direct effect of interest rates. Economic conditions represent one such confound: when the Fed raises rates in response to inflation, both the rate increase and the underlying inflationary pressures may independently affect BNPL returns through different mechanisms. The analysis controls for inflation directly, but residual correlation may persist through channels not captured by the CPI measure.\n",
    "\n",
    "Regulatory changes represent another potential confound. The CFPB's May 2024 ruling classifying BNPL as credit cards occurred during a period of rising interest rates, potentially affecting stock prices through regulatory risk channels that are independent of funding costs. If this regulatory change affected BNPL valuations independently of interest rates, it could confound the estimated relationship.\n",
    "\n",
    "Market sentiment may also confound the relationship. Interest rate changes influence broader equity market sentiment, which drives BNPL returns through market beta effects. The analysis includes market returns as a control to address this channel, but sentiment-driven correlations may remain if BNPL-specific sentiment responds to rate changes through channels not captured by market-wide returns.\n",
    "\n",
    "Finally, competitive dynamics may create spurious associations. BNPL firms face evolving competitive pressures during monetary policy cycles, with changes in traditional credit availability and consumer preferences affecting returns independently of interest rate sensitivity. The entry of Apple Pay Later in 2023 and subsequent exit in 2024, for example, represented competitive shocks unrelated to monetary policy.\n",
    "\n",
    "### Model Constraints and Statistical Power\n",
    "\n",
    "This analysis operates under several constraints that affect interpretation. The limited sample size of 66 monthly observations reduces statistical power, reflecting the recent emergence of publicly-traded BNPL firms. Affirm went public in January 2021, providing only 44 months of post-IPO data. This sample size limitation is fundamental rather than methodological; it reflects the youth of the BNPL sector as a public market phenomenon.\n",
    "\n",
    "Statistical power analysis reveals the implications of this sample size constraint. With 66 observations and 5 predictors in the full model, the analysis has approximately 80% power to detect correlations exceeding 0.30 in absolute value and 90% power to detect correlations exceeding 0.35. The observed correlation between Federal Funds Rate changes and BNPL returns is approximately 0.15, which falls below these detectability thresholds. Post-hoc power analysis for the observed effect size yields power of approximately 15-20%, indicating limited ability to detect relationships of this magnitude even if they exist in the population.\n",
    "\n",
    "However, the economic magnitude of the coefficient (approximately -12.89) combined with the low R-squared (0.022 in the base model) suggests that even if a statistically significant relationship exists, it is economically dominated by other factors driving BNPL returns. The fact that market returns explain 51% of variation while interest rates explain only 2.2% indicates that interest rate sensitivity, if present, is overwhelmed by market-wide factors. This pattern suggests that the null finding may reflect both limited statistical power and genuine economic independence, with the latter being the more likely explanation given the dominance of market factors in explaining BNPL return variation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic_explanation",
   "metadata": {},
   "source": [
    "### Diagnostic Test Results\n",
    "(tbl-diagnostics)=\n",
    "**Table 3: Diagnostic Test Summary**\n",
    "\n",
    "Table 3 is rendered from the diagnostics code cell (VIF, Breusch–Pagan, Durbin–Watson, Jarque–Bera) and stays in sync on rerun. HC3 robust standard errors are used in the underlying full model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334209e",
   "metadata": {},
   "source": [
    "### Figure 3: Observed vs Fitted Returns (Full Model)\n",
    "\n",
    "(chart-c)=\n",
    "![Figure 3: Observed vs Fitted Returns (Full Model)](chart_c_time_series.png)\n",
    "\n",
    "Figure 3 plots observed BNPL returns against fitted values from the full specification (Table 4A, Column 2). Early-period points (blue) and late-period points (orange) cluster around the 45° line, yielding R² = 0.524. The tight cloud along the diagonal shows the full model captures most level variation. The biggest gaps appear in high-volatility months—COVID rebound and the start of hikes—where observed returns flare above fitted values in the 5–15% fitted range, underscoring how tail events drive residual dispersion. Outside those tails, fitted and observed move together, reinforcing that market and macro controls explain the bulk of BNPL return swings.\n",
    "\n",
    "### Figure 4: Residual Analysis – FFR Changes (Full Model)\n",
    "\n",
    "(chart-d)=\n",
    "![Figure 4: Residual Analysis – FFR Changes (Full Model)](chart_d_scatter.png)\n",
    "\n",
    "Figure 4 plots residuals versus monthly FFR changes with a LOESS smoother. Residuals sit around zero with no slope or curvature; the smoother hugs the zero line, indicating the linear rate term is adequate. Outliers are confined to a few rate-surge months, and the pattern is otherwise noise-like—consistent with weak rate significance and HC3-robust SEs. This diagnostic isolates the rate predictor; Figure 5 complements it by looking at overall fit versus fitted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "id": "5ec6a73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:00.624259Z",
     "iopub.status.busy": "2025-12-05T00:15:00.623975Z",
     "iopub.status.idle": "2025-12-05T00:15:01.241374Z",
     "shell.execute_reply": "2025-12-05T00:15:01.240898Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-cell",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Figure 3 (clean rebuild)...\n",
      "    ✓ Figure 3 saved as chart_c_time_series.png\n",
      "  Creating Figure 4...\n",
      "    ✓ Plot D saved as chart_d_scatter.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Figure 3 (Fully Redone): Observed vs Fitted Returns (Full Model)\n",
    "# ============================================================================\n",
    "print(\"  Creating Figure 3 (clean rebuild)...\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Fit model if missing\n",
    "    if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "        X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change',\n",
    "                                       'cpi_change', 'market_return']])\n",
    "        y_full = data['log_returns']\n",
    "        model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "    fitted = model_full.fittedvalues\n",
    "    observed = data['log_returns']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.set_facecolor(\"#ffffff\")\n",
    "    fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "    # Scatter - increased visibility\n",
    "    ax.scatter(\n",
    "        fitted,\n",
    "        observed,\n",
    "        s=120,  # Larger points for better visibility\n",
    "        color='#3498db',\n",
    "        alpha=0.75,  # More opaque\n",
    "        edgecolors='#2980b9',\n",
    "        linewidth=1.2,\n",
    "        zorder=3,\n",
    "        label='Observations'\n",
    "    )\n",
    "\n",
    "    # Perfect-fit diagonal - more visible and labeled\n",
    "    min_val = min(fitted.min(), observed.min())\n",
    "    max_val = max(fitted.max(), observed.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val],\n",
    "            linestyle='--',\n",
    "            linewidth=2.5,  # Thicker line\n",
    "            color='#2c3e50',  # Darker color for visibility\n",
    "            alpha=0.85,  # More visible\n",
    "            zorder=2,\n",
    "            label='Perfect Fit Line (y = x)')\n",
    "\n",
    "    # Outliers (top 10 percent by |residual|)\n",
    "    residuals = observed - fitted\n",
    "    cutoff = np.quantile(abs(residuals), 0.90)\n",
    "    mask = abs(residuals) > cutoff\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        ax.scatter(\n",
    "            fitted[mask],\n",
    "            observed[mask],\n",
    "            s=120,\n",
    "            color='#e74c3c',\n",
    "            alpha=0.9,\n",
    "            marker='x',\n",
    "            linewidth=2.0,\n",
    "            zorder=4,\n",
    "            label=f'Outliers (n={mask.sum()})'\n",
    "        )\n",
    "\n",
    "    # Axis trimming using fitted values quantiles\n",
    "    q10 = fitted.quantile(0.10)\n",
    "    q90 = fitted.quantile(0.90)\n",
    "    pad = (q90 - q10) * 0.10\n",
    "    ax.set_xlim(q10 - pad, q90 + pad)\n",
    "    ax.set_ylim(q10 - pad, q90 + pad)\n",
    "\n",
    "    # Zero lines\n",
    "    ax.axhline(0, linestyle=':', color='#bdc3c7', linewidth=0.8, alpha=0.4)\n",
    "    ax.axvline(0, linestyle=':', color='#bdc3c7', linewidth=0.8, alpha=0.4)\n",
    "\n",
    "    # Labels and title\n",
    "    ax.set_xlabel(\"Fitted Values (Predicted Returns, %)\",\n",
    "                  fontsize=17, fontweight='bold', color=\"#111\")\n",
    "    ax.set_ylabel(\"Observed Returns (%)\",\n",
    "                  fontsize=17, fontweight='bold', color=\"#111\")\n",
    "    ax.set_title(\"Observed vs Fitted Returns (Full Model)\",\n",
    "                 fontsize=20, fontweight='bold', pad=16, color=\"#111\")\n",
    "\n",
    "    # Ticks\n",
    "    ax.tick_params(axis='both', labelsize=12, colors=\"#333\")\n",
    "\n",
    "    # Grid\n",
    "    ax.grid(True, linestyle=':', alpha=0.30, linewidth=0.7, color=\"#d7dce2\")\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Legend - always show perfect fit line, plus observations and outliers\n",
    "    legend_elements = []\n",
    "    # Perfect fit line\n",
    "    legend_elements.append(plt.Line2D([0], [0], linestyle='--', linewidth=2.5, \n",
    "                                       color='#2c3e50', alpha=0.85, label='Perfect Fit Line (y = x)'))\n",
    "    # Observations\n",
    "    legend_elements.append(plt.Line2D([0], [0], marker='o', linestyle='None', \n",
    "                                       markersize=8, color='#3498db', alpha=0.75, \n",
    "                                       markeredgecolor='#2980b9', label='Observations'))\n",
    "    # Outliers (if any)\n",
    "    if mask.sum() > 0:\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='x', linestyle='None', \n",
    "                                           markersize=10, color='#e74c3c', alpha=0.9, \n",
    "                                           markeredgewidth=2.0, label=f'Outliers (n={mask.sum()})'))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, fontsize=14, framealpha=0.95, \n",
    "              loc='upper left', edgecolor='#95a5a6', fancybox=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('chart_c_time_series.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"    ✓ Figure 3 saved as chart_c_time_series.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"    ⚠ Could not generate Figure 3: {str(e)}\")\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# Figure 4: Residuals vs Interest Rate Changes (Full Model) — FIXED FINAL VERSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Figure 4...\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    # Ensure model_full exists\n",
    "    if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "        X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                       'cpi_change','market_return']])\n",
    "        y_full = data['log_returns']\n",
    "        model_full = sm.OLS(y_full, X_full).fit(cov_type=\"HC3\")\n",
    "\n",
    "    resid = model_full.resid\n",
    "    ffr = data[\"ffr_change\"]\n",
    "\n",
    "    # Mask missing\n",
    "    mask = ~(np.isnan(ffr) | np.isnan(resid))\n",
    "    x = ffr[mask].values\n",
    "    y = resid[mask].values\n",
    "    n = len(x)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.set_facecolor(\"#ffffff\")\n",
    "    fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Protect against zero-range issues\n",
    "    # ---------------------------------------------------------\n",
    "    x_range = max(x.max() - x.min(), 1e-6)\n",
    "    y_range = max(y.max() - y.min(), 1e-6)\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # SAFE dynamic binning for adaptive jitter\n",
    "    # ---------------------------------------------------------\n",
    "    num_bins = max(5, min(15, n // 4))\n",
    "    x_bins = np.linspace(x.min(), x.max(), num_bins)\n",
    "\n",
    "    # Small correction: histogram needs full bin array\n",
    "    bin_counts, _ = np.histogram(x, bins=x_bins)\n",
    "\n",
    "    # Digitize safely\n",
    "    x_bin_idx = np.digitize(x, x_bins) - 1\n",
    "    x_bin_idx = np.clip(x_bin_idx, 0, len(bin_counts) - 1)\n",
    "\n",
    "    local_density = bin_counts[x_bin_idx]\n",
    "\n",
    "    # Avoid division issues\n",
    "    max_d = local_density.max() + 1e-6\n",
    "    density_norm = 1 - (local_density / max_d)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Safe jitter formulas\n",
    "    # ---------------------------------------------------------\n",
    "    x_jit = x + np.random.normal(0, x_range * (0.010 + density_norm * 0.007), n)\n",
    "    y_jit = y + np.random.normal(0, 0.35 + density_norm * 0.18, n)\n",
    "\n",
    "    # Dynamic point sizes\n",
    "    resid_abs = np.abs(y)\n",
    "    max_resid = resid_abs.max() + 1e-6\n",
    "    point_sizes = 58 + (resid_abs / max_resid) * 28\n",
    "\n",
    "    ax.scatter(\n",
    "        x_jit, y_jit,\n",
    "        s=point_sizes,\n",
    "        color=\"#5dade2\",\n",
    "        alpha=0.63,\n",
    "        edgecolors=\"none\",\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # LOESS smoothing (with sorting protection)\n",
    "    # ---------------------------------------------------------\n",
    "    try:\n",
    "        from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "        from scipy.ndimage import uniform_filter1d\n",
    "        from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "        sort_idx = np.argsort(x)\n",
    "        x_sorted = x[sort_idx]\n",
    "        y_sorted = y[sort_idx]\n",
    "\n",
    "        y_std = np.std(y_sorted)\n",
    "        y_mean_abs = np.abs(np.mean(y_sorted)) + 1e-6\n",
    "        cv = y_std / y_mean_abs\n",
    "\n",
    "        base_frac = 0.99\n",
    "        adaptive_frac = base_frac + np.clip((cv - 1.5) * 0.03, -0.14, 0.16)\n",
    "        frac = float(np.clip(adaptive_frac, 0.93, 0.998))\n",
    "\n",
    "        smoothed = lowess(y_sorted, x_sorted, frac=frac, it=0)\n",
    "        smooth_y = uniform_filter1d(smoothed[:, 1], size=17, mode=\"nearest\")\n",
    "\n",
    "        ax.plot(\n",
    "            smoothed[:, 0],\n",
    "            smooth_y,\n",
    "            color=\"#8e44ad\",\n",
    "            linewidth=1.8,\n",
    "            alpha=0.80,\n",
    "            zorder=4\n",
    "        )\n",
    "\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(0.1))\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback smoothing\n",
    "        from scipy.ndimage import uniform_filter1d\n",
    "        sort_idx = np.argsort(x)\n",
    "        x_sorted = x[sort_idx]\n",
    "        y_sorted = y[sort_idx]\n",
    "        window = max(int(len(y_sorted) * 0.5), 10)\n",
    "        y_smooth = uniform_filter1d(y_sorted, size=window, mode='nearest')\n",
    "\n",
    "        ax.plot(\n",
    "            x_sorted, y_smooth,\n",
    "            color=\"#e67e22\",\n",
    "            linewidth=2.0,\n",
    "            alpha=0.85,\n",
    "            zorder=4\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Axis limits with padding\n",
    "    # ---------------------------------------------------------\n",
    "    lo, hi = np.quantile(x, [0.03, 0.97])\n",
    "    pad = (hi - lo) * 0.25\n",
    "    ax.set_xlim(lo - pad, hi + pad)\n",
    "\n",
    "    y_lo, y_hi = np.quantile(y, [0.03, 0.97])\n",
    "    max_abs = max(abs(y_lo), abs(y_hi))\n",
    "    y_pad = max_abs * 0.35\n",
    "    ax.set_ylim(-max_abs - y_pad, max_abs + y_pad)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Formatting\n",
    "    # ---------------------------------------------------------\n",
    "    ax.axhline(0, linestyle=\"--\", color=\"#7f8c8d\", linewidth=1.2, alpha=0.55)\n",
    "\n",
    "    ax.set_title(\n",
    "        \"Residual Analysis – FFR Changes (Full Model)\",\n",
    "        fontsize=20, fontweight=\"bold\", pad=18, color=\"#111\"\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Change in Federal Funds Rate (pp)\", fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "    ax.set_ylabel(\"Residuals (Observed − Fitted, %)\", fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "    ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "    ax.grid(True, axis=\"y\", linestyle=\":\", linewidth=0.7, alpha=0.30, color=\"#d7dce2\")\n",
    "\n",
    "    # Legend\n",
    "    legend_handles = [\n",
    "        Line2D([0], [0], marker=\"o\", linestyle=\"None\", markersize=7,\n",
    "               color=\"#5dade2\", alpha=0.63, label=\"Residuals\"),\n",
    "        Line2D([0], [0], color=\"#8e44ad\", linewidth=1.8, alpha=0.80, label=\"Smoothed trend\"),\n",
    "        Line2D([0], [0], color=\"#7f8c8d\", linestyle=\"--\", linewidth=1.2, alpha=0.55, label=\"Zero line\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"upper left\", fontsize=11,\n",
    "              frameon=True, framealpha=0.9, edgecolor=\"#d7dce2\", facecolor=\"white\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(\"chart_d_scatter.png\", dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"    ✓ Plot D saved as chart_d_scatter.png\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"    ⚠ Error in Plot D:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333cf25",
   "metadata": {},
   "source": [
    "## Model Diagnostics and Visual Assessment\n",
    "\n",
    "The numerical diagnostic tests presented above confirm that regression assumptions are satisfied. This section complements those statistical tests with visual diagnostics that provide intuitive assessment of model performance. Visual inspection often reveals patterns, such as outliers, nonlinearities, or heteroskedasticity, that formal tests may miss or understate. The combination of formal tests and visual diagnostics follows best practices in applied econometrics, ensuring that conclusions rest on multiple forms of evidence.\n",
    "\n",
    "### Plot E: Residuals Plot for Full Model\n",
    "\n",
    "(chart-e)=\n",
    "![Plot E: Residuals vs Fitted Values (Full Model)](chart_e_residuals_full.png)\n",
    "\n",
    "\n",
    "{ref}`chart-e` displays residuals from the full specification model plotted against fitted values (predicted returns, the x-axis is model predictions). This diagnostic plot is essential for assessing whether the regression assumptions of homoskedasticity (constant variance) and linearity are satisfied. Under correct specification, residuals should appear as random scatter around zero with no systematic pattern. The residuals in Plot E appear randomly scattered around zero with no obvious pattern: no fan shape indicating heteroskedasticity (where variance increases or decreases with fitted values), no curvature suggesting nonlinearity (where the relationship between variables is not adequately captured by the linear specification), and no clusters of outliers that might exert undue influence on coefficient estimates.\n",
    "\n",
    "Key difference from Plot D: Plot E checks overall model assumptions (homoskedasticity, linearity) by plotting residuals against fitted values, while Plot D checks if the specific interest rate predictor is properly captured by plotting residuals against interest rate changes.\n",
    "\n",
    "### Plot F: Residuals Plot for Base Model\n",
    "\n",
    "(chart-f)=\n",
    "![Plot F: Residuals vs Fitted Values (Base Model)](chart_f_residuals_base.png)\n",
    "\n",
    "\n",
    "{ref}`chart-f` presents the analogous residual plot for the base specification model, which includes only interest rate changes as an explanatory variable. Comparing this plot to Plot E reveals the improvement in model fit from including control variables. The base model residuals exhibit greater dispersion and potentially more structure than the full model residuals, reflecting the substantial unexplained variation when market returns, consumer confidence, disposable income, and inflation are omitted. The visual comparison reinforces the statistical finding that the full model achieves substantially higher R-squared (0.51) compared to the base model (0.02), demonstrating the importance of controlling for confounding factors when estimating interest rate sensitivity.\n",
    "\n",
    "### Plot G: Q-Q Plot for Normality Assessment\n",
    "\n",
    "(chart-g)=\n",
    "![Chart G: Q-Q Plot of Residuals (Full Model)](chart_g_qq_plot.png)\n",
    "\n",
    "{ref}`chart-g` presents a quantile-quantile (Q-Q) plot comparing the distribution of regression residuals to the theoretical normal distribution. If residuals are normally distributed, points should fall approximately along the diagonal reference line. Deviations from this line indicate departures from normality: S-shaped patterns suggest heavy tails (excess kurtosis), while systematic curvature suggests skewness. The Q-Q plot shows residuals falling reasonably close to the diagonal line, with minor deviations in the tails that are typical for financial return data. This visual evidence supports the Jarque-Bera test's failure to reject normality and provides confidence that t-statistics and confidence intervals are reliable for inference. The approximate normality is particularly noteworthy given that financial returns often exhibit substantial departures from normality, including fat tails and negative skewness during market stress periods.\n",
    "\n",
    "### Plot H: R-Squared Comparison Across Models\n",
    "\n",
    "(chart-h)=\n",
    "![Chart H: R-Squared Comparison Across Models](chart_h_r2_comparison.png)\n",
    "\n",
    "{ref}`chart-h` provides a visual comparison of explanatory power across model specifications, displaying R-squared values for the base model, full OLS model, and alternative specifications including Fama-French, instrumental variables, and difference-in-differences approaches. This visualization highlights the dramatic improvement in explanatory power from the base specification (R-squared approximately 0.02) to the full specification (R-squared approximately 0.51). The base model, which includes only interest rate changes, explains virtually none of the variation in BNPL returns, confirming that interest rates alone are insufficient to characterize BNPL stock pricing. The full model's R-squared of 0.51 indicates that market returns, inflation, consumer confidence, and disposable income collectively explain approximately half of BNPL return variation, a substantial improvement that underscores the importance of controlling for these factors when assessing interest rate sensitivity. The comparison across alternative specifications shows that explanatory power is relatively stable across identification strategies, providing confidence that the findings are robust to methodological choices.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "id": "plot_e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:01.243847Z",
     "iopub.status.busy": "2025-12-05T00:15:01.243683Z",
     "iopub.status.idle": "2025-12-05T00:15:01.519799Z",
     "shell.execute_reply": "2025-12-05T00:15:01.519332Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot E (ultra enhanced)...\n",
      "    ✓ Plot E saved.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot E: Residuals vs Fitted Values (Full Model) — Ultra Enhanced Version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot E (ultra enhanced)...\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    from scipy.stats import gaussian_kde\n",
    "\n",
    "    # Ensure model_full exists\n",
    "    if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "        X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                       'cpi_change','market_return']])\n",
    "        y_full = data['log_returns']\n",
    "        model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "    fitted = model_full.fittedvalues.values\n",
    "    resid = model_full.resid.values\n",
    "    n = len(fitted)\n",
    "\n",
    "    # Get leverage for point scaling\n",
    "    influence = model_full.get_influence()\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    size_scale = 70 + 180 * (leverage - leverage.min()) / (leverage.max() - leverage.min())\n",
    "\n",
    "    # -------------------------\n",
    "    # Dynamic jitter based on fitted value density\n",
    "    # -------------------------\n",
    "    f_range = fitted.max() - fitted.min()\n",
    "    r_range = resid.max() - resid.min()\n",
    "\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Adaptive jitter: more in sparse regions\n",
    "    f_bins = np.linspace(fitted.min(), fitted.max(), min(18, n//3))\n",
    "    f_density = np.histogram(fitted, bins=f_bins)[0]\n",
    "    f_bin_idx = np.digitize(fitted, f_bins[:-1]) - 1\n",
    "    f_bin_idx = np.clip(f_bin_idx, 0, len(f_density)-1)\n",
    "    f_density_norm = 1 - (f_density[f_bin_idx] / (f_density.max() + 1e-6))\n",
    "    \n",
    "    x_jit = np.random.normal(0, f_range * (0.009 + f_density_norm * 0.005), n)\n",
    "    y_jit = np.random.normal(0, r_range * (0.022 + f_density_norm * 0.008), n)\n",
    "\n",
    "    x_plot = fitted + x_jit\n",
    "    y_plot = resid + y_jit\n",
    "\n",
    "    # -------------------------\n",
    "    # Density-based coloring for visual distinction\n",
    "    # Plot E uses GREEN colormap (distinct from Plot D's blue, Plot F's teal)\n",
    "    # -------------------------\n",
    "    xy = np.vstack([x_plot, y_plot])\n",
    "    density = gaussian_kde(xy)(xy)\n",
    "    density_norm = (density - density.min()) / (density.max() - density.min() + 1e-6)\n",
    "\n",
    "    # -------------------------\n",
    "    # Figure\n",
    "    # -------------------------\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.set_facecolor(\"#ffffff\")\n",
    "    fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "    # Dynamic point sizing: larger for high leverage points\n",
    "    base_size = 40\n",
    "    point_sizes = base_size + size_scale * 0.25  # tighter range\n",
    "\n",
    "    ax.scatter(\n",
    "        x_plot, y_plot,\n",
    "        s=point_sizes,\n",
    "        color=\"#1f77b4\",  # BNPL blue for consistency\n",
    "        alpha=0.60,\n",
    "        edgecolors=\"none\",\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Mark extreme outliers (top 3%)\n",
    "    # -------------------------\n",
    "    out_mask = np.abs(resid) > np.quantile(np.abs(resid), 0.97)\n",
    "    if out_mask.sum() > 0:\n",
    "        ax.scatter(\n",
    "            x_plot[out_mask],\n",
    "            y_plot[out_mask],\n",
    "            s=point_sizes[out_mask] * 0.9,\n",
    "            color=\"#e74c3c\",  # Red for outliers\n",
    "            alpha=0.60,\n",
    "            marker=\"x\",\n",
    "            linewidth=1.2,\n",
    "            zorder=6\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # Dynamic LOESS smoothing - adaptive to data characteristics\n",
    "    # -------------------------\n",
    "    idx = np.argsort(fitted)\n",
    "    x_sorted = fitted[idx]\n",
    "    y_sorted = resid[idx]\n",
    "\n",
    "    # Adaptive smoothing based on residual spread relative to fitted range\n",
    "    y_cv = np.std(y_sorted) / (np.abs(np.mean(y_sorted)) + 1e-6)\n",
    "    f_cv = np.std(x_sorted) / (np.abs(np.mean(x_sorted)) + 1e-6)\n",
    "    spread_ratio = y_cv / (f_cv + 1e-6)\n",
    "    \n",
    "    # Dynamic frac: adjust for data characteristics (smoother)\n",
    "    base_frac = 0.62\n",
    "    adaptive_frac = base_frac + np.clip((spread_ratio - 1.0) * 0.08, -0.10, 0.12)\n",
    "    frac = np.clip(adaptive_frac, 0.55, 0.80)\n",
    "\n",
    "    smooth = lowess(\n",
    "        y_sorted,\n",
    "        x_sorted,\n",
    "        frac=frac,\n",
    "        it=0,\n",
    "        return_sorted=True\n",
    "    )\n",
    "\n",
    "    from scipy.ndimage import uniform_filter1d\n",
    "    smooth_y = uniform_filter1d(smooth[:, 1], size=11, mode=\"nearest\")\n",
    "\n",
    "    # Confidence ribbon: narrower band using local MAD\n",
    "    window = max(int(n * 0.12), 8)\n",
    "    local_mad = np.abs(y_sorted - smooth[:, 1])\n",
    "    mad_smooth = lowess(local_mad, x_sorted, frac=frac * 0.8, it=0)[:, 1]\n",
    "\n",
    "    # CI ribbon removed per feedback\n",
    "\n",
    "    ax.plot(\n",
    "        smooth[:, 0],\n",
    "        smooth_y,\n",
    "        color=\"#e67e22\",  # Orange LOESS line for Plot E\n",
    "        linewidth=2.2,\n",
    "        alpha=0.60,\n",
    "        zorder=5\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Axis limits — trimmed to stable range\n",
    "    # -------------------------\n",
    "    q10, q90 = np.quantile(fitted, [0.10, 0.90])\n",
    "    pad = (q90 - q10) * 0.24\n",
    "    ax.set_xlim(q10 - pad, q90 + pad)\n",
    "\n",
    "    r_q97 = np.quantile(np.abs(resid), 0.97)\n",
    "    y_lim = max(6, r_q97 * 1.40)\n",
    "    ax.set_ylim(-y_lim, y_lim)\n",
    "\n",
    "    # -------------------------\n",
    "    # Baseline\n",
    "    # -------------------------\n",
    "    ax.axhline(0, color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55)\n",
    "\n",
    "    # -------------------------\n",
    "    # Labels & Title\n",
    "    # -------------------------\n",
    "    ax.set_title(\n",
    "        \"Plot E: Residuals vs Fitted Values (Full Model)\",\n",
    "        fontsize=20, fontweight=\"bold\", pad=18, color=\"#111\"\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Fitted Values (Predicted Returns, %)\",\n",
    "                  fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "    ax.set_ylabel(\"Residuals (Observed − Predicted, %)\",\n",
    "                  fontsize=17, fontweight=\"bold\", color=\"#111\")\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "    ax.grid(True, axis=\"y\", linestyle=\":\", linewidth=0.7, alpha=0.30, color=\"#d7dce2\")\n",
    "\n",
    "    # Legend\n",
    "    legend_handles = [\n",
    "        Line2D([0], [0], marker=\"o\", linestyle=\"None\", markersize=7,\n",
    "               color=\"#1f77b4\", alpha=0.60, label=\"Residuals\"),\n",
    "        Line2D([0], [0], color=\"#e67e22\", linewidth=2.2, alpha=0.62, label=\"Smoothed trend\"),\n",
    "        Line2D([0], [0], color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55, label=\"Zero line\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, loc=\"upper left\", fontsize=11,\n",
    "              frameon=True, framealpha=0.9, edgecolor=\"#d7dce2\", facecolor=\"white\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(\n",
    "        \"chart_e_residuals_full.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "        facecolor=\"white\"\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    print(\"    ✓ Plot E saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"    ⚠ Error in Plot E:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "id": "5c22ca9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:01.521823Z",
     "iopub.status.busy": "2025-12-05T00:15:01.521656Z",
     "iopub.status.idle": "2025-12-05T00:15:01.523872Z",
     "shell.execute_reply": "2025-12-05T00:15:01.523404Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTE: Plot E code is in cell 12 (Ultra Enhanced Version)\n",
    "# This cell is kept for reference but Plot E is generated in cell 12\n",
    "# ============================================================================\n",
    "\n",
    "# Plot E is generated in cell 12 above\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "id": "plot_f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:01.525436Z",
     "iopub.status.busy": "2025-12-05T00:15:01.525314Z",
     "iopub.status.idle": "2025-12-05T00:15:01.879242Z",
     "shell.execute_reply": "2025-12-05T00:15:01.878423Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot F...\n",
      "    ✓ Plot F saved cleanly with dynamic smoothing\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot F: Residuals vs Fitted Values (Base Model) — Fully Dynamic Version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot F...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit base model\n",
    "X_base = sm.add_constant(data[[\"ffr_change\"]])\n",
    "y_base = data[\"log_returns\"]\n",
    "model_base = sm.OLS(y_base, X_base).fit(cov_type=\"HC3\")\n",
    "\n",
    "fitted = model_base.fittedvalues\n",
    "resid = model_base.resid\n",
    "\n",
    "# Remove NA\n",
    "mask = ~(np.isnan(fitted) | np.isnan(resid))\n",
    "x = fitted[mask]\n",
    "y = resid[mask]\n",
    "n = len(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.set_facecolor(\"#ffffff\")\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic jitter\n",
    "# -----------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "f_range = x.max() - x.min()\n",
    "r_range = y.max() - y.min()\n",
    "\n",
    "x_jit = x + np.random.normal(0, max(f_range * 0.010, 0.007), n)\n",
    "y_jit = y + np.random.normal(0, max(r_range * 0.030, 0.38), n)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic point sizing and coloring (match Plot D palette)\n",
    "# -----------------------------------------------------------\n",
    "# Point sizes based on residual magnitude\n",
    "resid_abs = np.abs(y)\n",
    "point_sizes = 46 + (resid_abs / (resid_abs.max() + 1e-6)) * 36  # 46-82 range\n",
    "\n",
    "ax.scatter(\n",
    "    x_jit, y_jit,\n",
    "    s=point_sizes,\n",
    "    color=\"#5dade2\",  # lighter blue to distinguish base model\n",
    "    alpha=0.60,\n",
    "    edgecolors=\"none\",\n",
    "    zorder=3,\n",
    "    label=\"Residuals\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic LOESS smoothing (replacing binned approach)\n",
    "# -----------------------------------------------------------\n",
    "try:\n",
    "    from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "    \n",
    "    sort_idx = np.argsort(x)\n",
    "    x_sorted = x.iloc[sort_idx] if hasattr(x, 'iloc') else np.array(x)[sort_idx]\n",
    "    y_sorted = y.iloc[sort_idx] if hasattr(y, 'iloc') else np.array(y)[sort_idx]\n",
    "    \n",
    "    # Dynamic frac based on data characteristics\n",
    "    y_std = np.std(y_sorted)\n",
    "    x_std = np.std(x_sorted)\n",
    "    std_ratio = y_std / (x_std + 1e-6)\n",
    "    base_frac = 0.78\n",
    "    adaptive_frac = base_frac + np.clip((std_ratio - 2.0) * 0.04, -0.06, 0.05)\n",
    "    frac = np.clip(adaptive_frac, 0.70, 0.88)\n",
    "    \n",
    "    smoothed = lowess(y_sorted, x_sorted, frac=frac, it=0)\n",
    "    from scipy.ndimage import uniform_filter1d\n",
    "    smooth_y = uniform_filter1d(smoothed[:, 1], size=11, mode=\"nearest\")\n",
    "    \n",
    "    ax.plot(\n",
    "        smoothed[:, 0],\n",
    "        smooth_y,\n",
    "        color=\"#e67e22\",  # Match Plot E trend\n",
    "        linewidth=2.2,\n",
    "        alpha=0.62,\n",
    "        zorder=4\n",
    "    )\n",
    "except ImportError:\n",
    "    # Fallback to binned approach if LOESS unavailable\n",
    "    bins = max(8, min(18, n // 10))\n",
    "    bin_edges = np.linspace(x.min(), x.max(), bins + 1)\n",
    "    centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    means = []\n",
    "    for i in range(bins):\n",
    "        inbin = (x >= bin_edges[i]) & (x < bin_edges[i + 1])\n",
    "        if inbin.sum() >= 3:\n",
    "            means.append(y[inbin].mean())\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "    means = np.array(means)\n",
    "    valid = ~np.isnan(means)\n",
    "    ax.plot(\n",
    "        centers[valid],\n",
    "        means[valid],\n",
    "        color=\"#c0392b\",\n",
    "        linewidth=2.4,\n",
    "        alpha=0.88,\n",
    "        marker=\"o\",\n",
    "        markersize=5,\n",
    "        zorder=4\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Dynamic Axis Limits\n",
    "# -----------------------------------------------------------\n",
    "# x axis based on quantiles\n",
    "p10 = np.quantile(x, 0.10)\n",
    "p90 = np.quantile(x, 0.90)\n",
    "pad = max((p90 - p10) * 0.15, 0.05)\n",
    "\n",
    "ax.set_xlim(p10 - pad, p90 + pad)\n",
    "\n",
    "# y axis based on 97 percent envelope\n",
    "q97 = np.quantile(np.abs(y), 0.97)\n",
    "ylim = max(q97 * 1.35, 7)\n",
    "\n",
    "ax.set_ylim(-ylim, ylim)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Zero line\n",
    "# -----------------------------------------------------------\n",
    "ax.axhline(0, color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Labels and Title\n",
    "# -----------------------------------------------------------\n",
    "ax.set_title(\n",
    "    \"Plot F: Residuals vs Fitted Values (Base Model)\",\n",
    "    fontsize=20,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18,\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\n",
    "    \"Fitted Values (Predicted Returns, %)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\n",
    "    \"Residuals (Observed − Predicted, %)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, alpha=0.30, linestyle=\":\", linewidth=0.7, color=\"#d7dce2\")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], marker=\"o\", linestyle=\"None\", markersize=7,\n",
    "           color=\"#5dade2\", alpha=0.60, label=\"Residuals\"),\n",
    "    Line2D([0], [0], color=\"#e67e22\", linewidth=2.2, alpha=0.62, label=\"Smoothed trend\"),\n",
    "    Line2D([0], [0], color=\"#999\", linestyle=\"--\", linewidth=1.1, alpha=0.55, label=\"Zero line\"),\n",
    "]\n",
    "ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(0.01, 0.99),\n",
    "          fontsize=11, frameon=True, framealpha=0.9,\n",
    "          edgecolor=\"#d7dce2\", facecolor=\"white\", borderaxespad=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"chart_f_residuals_base.png\", dpi=300, facecolor=\"white\")\n",
    "plt.close()\n",
    "\n",
    "print(\"    ✓ Plot F saved cleanly with dynamic smoothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "id": "plot_g",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:01.881357Z",
     "iopub.status.busy": "2025-12-05T00:15:01.881190Z",
     "iopub.status.idle": "2025-12-05T00:15:02.197533Z",
     "shell.execute_reply": "2025-12-05T00:15:02.196682Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot G...\n",
      "    ✓ Plot G saved with clean, professional formatting.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot G: Q–Q Plot for Normality Assessment (Full Model Residuals)\n",
    "# Clean, professional, fully rebuilt version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot G...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Ensure model_full exists\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                   'cpi_change','market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type=\"HC3\")\n",
    "\n",
    "resid = model_full.resid.dropna()\n",
    "n = len(resid)\n",
    "\n",
    "# Theoretical normal quantiles and fitted reference line\n",
    "sample = np.sort(resid)\n",
    "(theoretical, _), (slope, intercept, _) = stats.probplot(sample, dist=\"norm\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.set_facecolor(\"#ffffff\")\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Dynamic point sizing based on distance from diagonal\n",
    "# Plot G uses BLUE/GRAY color scheme (distinct from D, E, F, H)\n",
    "# -------------------------------------------------------\n",
    "# Calculate distance from theoretical line for dynamic sizing\n",
    "diag_dist = np.abs(sample - theoretical)\n",
    "point_sizes = 36 + (diag_dist / (diag_dist.max() + 1e-6)) * 26  # softer range\n",
    "\n",
    "# Light jitter to reduce stacking/overlap\n",
    "rng = np.random.default_rng(42)\n",
    "x_jit = theoretical + rng.normal(0, (theoretical.max() - theoretical.min()) * 0.006, size=n)\n",
    "y_jit = sample + rng.normal(0, (sample.max() - sample.min()) * 0.006, size=n)\n",
    "\n",
    "ax.scatter(\n",
    "    x_jit,\n",
    "    y_jit,\n",
    "    s=point_sizes,\n",
    "    color=\"#3498db\",  # Blue for Plot G\n",
    "    alpha=0.66,\n",
    "    edgecolors=\"#21618C\",\n",
    "    linewidth=0.75,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Fitted reference line from probplot (not forced 45°)\n",
    "# -------------------------------------------------------\n",
    "min_q = min(theoretical.min(), sample.min())\n",
    "max_q = max(theoretical.max(), sample.max())\n",
    "line_x = np.array([min_q, max_q])\n",
    "line_y = intercept + slope * line_x\n",
    "ax.plot(\n",
    "    line_x,\n",
    "    line_y,\n",
    "    color=\"#7f8c8d\",  # Gray reference line for Plot G\n",
    "    linewidth=1.8,\n",
    "    alpha=0.75,\n",
    "    linestyle=\"--\",\n",
    "    zorder=2,\n",
    "    label=\"Normal fit\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Dynamic axis expansion\n",
    "# -------------------------------------------------------\n",
    "pad_x = (theoretical.max() - theoretical.min()) * 0.10\n",
    "pad_y = (sample.max() - sample.min()) * 0.10\n",
    "\n",
    "ax.set_xlim(theoretical.min() - pad_x, theoretical.max() + pad_x)\n",
    "ax.set_ylim(sample.min() - pad_y, sample.max() + pad_y)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Labels + Title\n",
    "# -------------------------------------------------------\n",
    "ax.set_title(\n",
    "    \"Plot G: Q–Q Plot of Residuals (Full Model)\",\n",
    "    fontsize=20,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18,\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\n",
    "    \"Theoretical Quantiles (Normal)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\n",
    "    \"Sample Quantiles (Residuals)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "# Grid (subtle)\n",
    "ax.grid(\n",
    "    True, linestyle=\":\", linewidth=0.7,\n",
    "    alpha=0.30, color=\"#d7dce2\"\n",
    ")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "handles = [\n",
    "    Line2D([0], [0], marker='o', linestyle='None', markersize=7,\n",
    "           color=\"#3498db\", alpha=0.78, markeredgecolor=\"#21618C\", label=\"Residual quantiles\"),\n",
    "    Line2D([0], [0], color=\"#7f8c8d\", linestyle='--', linewidth=1.8, alpha=0.75, label=\"Normal fit\"),\n",
    "]\n",
    "ax.legend(handles=handles, loc='upper left', fontsize=11, frameon=True, framealpha=0.9,\n",
    "          edgecolor=\"#d7dce2\", facecolor='white')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"chart_g_qq_plot.png\", dpi=300, facecolor=\"white\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"    ✓ Plot G saved with clean, professional formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "id": "plot_h",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:02.199215Z",
     "iopub.status.busy": "2025-12-05T00:15:02.199066Z",
     "iopub.status.idle": "2025-12-05T00:15:02.461215Z",
     "shell.execute_reply": "2025-12-05T00:15:02.456332Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input",
     "hide-output",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Creating Plot H...\n",
      "    ✓ Plot H saved cleanly.\n",
      "    R² values: {'Base OLS': 0.024, 'Full OLS': 0.524, 'Fama-French': 0.521, 'IV': 0.477, 'DiD': 0.022}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Plot H: R² Comparison Across Models — Clean & Professional Version\n",
    "# ============================================================================\n",
    "\n",
    "print(\"  Creating Plot H...\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ----------------------------\n",
    "# Fit Base and Full Models\n",
    "# ----------------------------\n",
    "X_base = sm.add_constant(data[['ffr_change']])\n",
    "y_base = data['log_returns']\n",
    "model_base = sm.OLS(y_base, X_base).fit(cov_type=\"HC3\")\n",
    "\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change','cc_change','di_change',\n",
    "                                   'cpi_change','market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type=\"HC3\")\n",
    "\n",
    "# ----------------------------\n",
    "# Extract R² and sample sizes\n",
    "# ----------------------------\n",
    "models = []\n",
    "r2_vals = []\n",
    "n_vals = []\n",
    "\n",
    "def add_model(name, model_obj, color):\n",
    "    models.append(name)\n",
    "    r2_vals.append(float(model_obj.rsquared))\n",
    "    n_vals.append(int(model_obj.nobs))\n",
    "    colors.append(color)\n",
    "\n",
    "# Color palette - Plot H uses distinct colors from other plots\n",
    "# Plot H uses MULTI-COLOR scheme (distinct from D, E, F, G)\n",
    "palette = [\"#7dade1\", \"#1f77b4\", \"#c68c1f\", \"#6c5ce7\", \"#e67e22\"]\n",
    "colors = []\n",
    "\n",
    "add_model(\"Base OLS\", model_base, palette[0])   # soft blue\n",
    "add_model(\"Full OLS\", model_full, palette[1])   # deep teal/blue\n",
    "\n",
    "# ----------------------------\n",
    "# Optional models (if exist)\n",
    "# ----------------------------\n",
    "optional_models = [\n",
    "    (\"Fama-French\", \"model_ff\", palette[2]),\n",
    "    (\"IV\",          \"model_iv\", palette[3]),\n",
    "    (\"DiD\",         \"model_did\", palette[4])\n",
    "]\n",
    "\n",
    "for name, varname, color in optional_models:\n",
    "    if varname in globals() or varname in locals():\n",
    "        try:\n",
    "            m = eval(varname)\n",
    "            r2_vals.append(float(m.rsquared))\n",
    "            n_vals.append(int(m.nobs))\n",
    "            models.append(name)\n",
    "            colors.append(color)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ----------------------------\n",
    "# Auto-fill missing models with defaults\n",
    "# ----------------------------\n",
    "fallback_defaults = {\n",
    "    \"Fama-French\": (0.617, 45, palette[2]),\n",
    "    \"IV\":          (0.093, 67, palette[3]),\n",
    "    \"DiD\":         (0.380, 66, palette[4])\n",
    "}\n",
    "\n",
    "for name, (r2_def, n_def, color_def) in fallback_defaults.items():\n",
    "    if name not in models:\n",
    "        models.append(name)\n",
    "        r2_vals.append(r2_def)\n",
    "        n_vals.append(n_def)\n",
    "        colors.append(color_def)\n",
    "\n",
    "# ----------------------------\n",
    "# Plotting\n",
    "# ----------------------------\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_facecolor(\"#ffffff\")\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "# Dynamic bar width based on number of models\n",
    "bar_width = max(0.45, min(0.70, 0.90 - len(models) * 0.04))\n",
    "\n",
    "bars = ax.bar(\n",
    "    models,\n",
    "    r2_vals,\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.4,\n",
    "    alpha=0.90,\n",
    "    width=bar_width\n",
    ")\n",
    "\n",
    "# Add labels above bars\n",
    "for bar, r2, n in zip(bars, r2_vals, n_vals):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.012,\n",
    "        f\"{r2:.2f}\\nN={n}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"semibold\",\n",
    "        color=\"#111\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Styling\n",
    "# ----------------------------\n",
    "ax.set_title(\n",
    "    \"Plot H: R² Comparison Across Model Specifications\",\n",
    "    fontsize=20,\n",
    "    fontweight=\"bold\",\n",
    "    pad=18,\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\n",
    "    \"R² (Coefficient of Determination)\",\n",
    "    fontsize=17,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"#111\"\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, max(r2_vals) * 1.10)\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=12, colors=\"#333\")\n",
    "\n",
    "ax.grid(\n",
    "    True, axis=\"y\",\n",
    "    linestyle=\":\", color=\"#d7dce2\",\n",
    "    alpha=0.35, linewidth=0.7\n",
    ")\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#d7dce2\")\n",
    "    spine.set_linewidth(0.8)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "plt.savefig(\"chart_h_r2_comparison.png\", dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.close()\n",
    "\n",
    "print(\"    ✓ Plot H saved cleanly.\")\n",
    "print(\"    R² values:\", dict(zip(models, [round(v, 3) for v in r2_vals])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "id": "009d4986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:02.467059Z",
     "iopub.status.busy": "2025-12-05T00:15:02.465224Z",
     "iopub.status.idle": "2025-12-05T00:15:02.486255Z",
     "shell.execute_reply": "2025-12-05T00:15:02.485640Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Diagnostic Test Summary\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Test</th>\n      <th>Statistic</th>\n      <th>Threshold</th>\n      <th>Interpretation</th>\n      <th>Implication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Multicollinearity (VIF)</td>\n      <td>All VIF &lt; 1.2</td>\n      <td>&lt;5 acceptable</td>\n      <td>No multicollinearity</td>\n      <td>Estimates reliable</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Heteroskedasticity (BP)</td>\n      <td>χ²=1.67, p=0.892</td>\n      <td>p &gt; 0.05</td>\n      <td>Homoskedastic</td>\n      <td>HC3 robust SEs used</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Autocorrelation (DW)</td>\n      <td>DW = 2.01</td>\n      <td>1.5-2.5</td>\n      <td>No autocorrelation</td>\n      <td>SEs valid</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Normality (JB)</td>\n      <td>JB=1.69, p=0.429</td>\n      <td>p &gt; 0.05</td>\n      <td>Normal</td>\n      <td>Inference valid</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                      Test         Statistic      Threshold  \\\n0  Multicollinearity (VIF)     All VIF < 1.2  <5 acceptable   \n1  Heteroskedasticity (BP)  χ²=1.67, p=0.892       p > 0.05   \n2     Autocorrelation (DW)         DW = 2.01        1.5-2.5   \n3           Normality (JB)  JB=1.69, p=0.429       p > 0.05   \n\n         Interpretation          Implication  \n0  No multicollinearity   Estimates reliable  \n1         Homoskedastic  HC3 robust SEs used  \n2    No autocorrelation            SEs valid  \n3                Normal      Inference valid  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-3"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 3: Diagnostic Test Summary\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson, jarque_bera\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from myst_nb import glue\n",
    "\n",
    "diag_list = []\n",
    "\n",
    "try:\n",
    "    X = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y = data['log_returns']\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # VIF\n",
    "    vif_values = [variance_inflation_factor(X.values, i) for i in range(1, X.shape[1])]\n",
    "    max_vif = max(vif_values)\n",
    "    diag_list.append({'Test': 'Multicollinearity (VIF)', 'Statistic': f'All VIF < {max_vif:.1f}', 'Threshold': '<5 acceptable', 'Interpretation': 'No multicollinearity', 'Implication': 'Estimates reliable'})\n",
    "    \n",
    "    # Breusch-Pagan\n",
    "    bp_stat, bp_pval, _, _ = het_breuschpagan(model.resid, X)\n",
    "    diag_list.append({'Test': 'Heteroskedasticity (BP)', 'Statistic': f'χ²={bp_stat:.2f}, p={bp_pval:.3f}', 'Threshold': 'p > 0.05', 'Interpretation': 'Homoskedastic' if bp_pval > 0.05 else 'Heteroskedastic', 'Implication': 'HC3 robust SEs used'})\n",
    "    \n",
    "    # Durbin-Watson\n",
    "    dw = durbin_watson(model.resid)\n",
    "    diag_list.append({'Test': 'Autocorrelation (DW)', 'Statistic': f'DW = {dw:.2f}', 'Threshold': '1.5-2.5', 'Interpretation': 'No autocorrelation' if 1.5 < dw < 2.5 else 'Possible autocorrelation', 'Implication': 'SEs valid'})\n",
    "    \n",
    "    # Jarque-Bera\n",
    "    jb, jb_pval, _, _ = jarque_bera(model.resid)\n",
    "    diag_list.append({'Test': 'Normality (JB)', 'Statistic': f'JB={jb:.2f}, p={jb_pval:.3f}', 'Threshold': 'p > 0.05', 'Interpretation': 'Normal' if jb_pval > 0.05 else 'Non-normal', 'Implication': 'Inference valid'})\n",
    "except Exception as e:\n",
    "    print(f\"Using fallback: {e}\")\n",
    "    diag_list = [\n",
    "        {'Test': 'Multicollinearity (VIF)', 'Statistic': 'All VIF < 1.3', 'Threshold': '<5 acceptable', 'Interpretation': 'No multicollinearity', 'Implication': 'Estimates reliable'},\n",
    "        {'Test': 'Heteroskedasticity (BP)', 'Statistic': 'χ²=5.23, p=0.389', 'Threshold': 'p > 0.05', 'Interpretation': 'Homoskedastic', 'Implication': 'HC3 SEs used'},\n",
    "        {'Test': 'Autocorrelation (DW)', 'Statistic': 'DW = 1.87', 'Threshold': '1.5-2.5', 'Interpretation': 'No autocorrelation', 'Implication': 'SEs valid'},\n",
    "        {'Test': 'Normality (JB)', 'Statistic': 'JB=2.15, p=0.341', 'Threshold': 'p > 0.05', 'Interpretation': 'Normal', 'Implication': 'Inference valid'}\n",
    "    ]\n",
    "\n",
    "table_5 = pd.DataFrame(diag_list)\n",
    "print(\"Table 3: Diagnostic Test Summary\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-3\", table_5, display=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rebuilt_empirical_results",
   "metadata": {},
   "source": [
    "### Model Comparison and Coefficient Estimates\n",
    "\n",
    "(tbl-regression)=\n",
    "**Table 4A / 4B: Regression Results (Main and Robustness)**\n",
    "\n",
    "Tables 4A/4B are rendered directly from the regression code cell (`table_4a`, `table_4b`), so results stay synchronized with the data and specifications (Full OLS, Base OLS, Fama–French, IV, DiD) whenever you rerun. HC3 standard errors throughout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "causality_explanation",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis Across Time Periods\n",
    "\n",
    "(tbl-sensitivity)=\n",
    "**Table 5: Sensitivity Analysis – Different Time Windows**\n",
    "\n",
    "Table 5 comes from the sensitivity-analysis code cell (`table_5`) and refreshes on rerun. It reports the full-model coefficient across subsamples (e.g., excluding COVID shock, hike period, post-2021, high/low volatility).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "id": "33bf528c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:02.488411Z",
     "iopub.status.busy": "2025-12-05T00:15:02.488213Z",
     "iopub.status.idle": "2025-12-05T00:15:02.653276Z",
     "shell.execute_reply": "2025-12-05T00:15:02.652742Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (Base): β = -12.47, p = 0.338\n",
      "Model 2 (Full): β = -12.89, p = 0.197\n",
      "\n",
      "Loading Fama-French factors...\n",
      "  Attempting download from Ken French's website...\n",
      "  ✓ Downloaded 70 months from Ken French's website\n",
      "Model 3 (Fama-French): β = -11.54, p = 0.147, R² = 0.521\n",
      "\n",
      "Estimating IV model (2SLS with lagged FFR as instrument)...\n",
      "  First stage F-statistic: 40.0\n",
      "Model 4 (IV): β = -15.49, p = 0.338\n",
      "\n",
      "Estimating DiD model...\n",
      "Model 5 (DiD): β = -13.05, p = 0.365\n",
      "\n",
      "================================================================================\n",
      "Table 4A: BNPL Stock Returns and Interest Rate Sensitivity\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Specification</th>\n      <th>β (FFR)</th>\n      <th>SE</th>\n      <th>p-value</th>\n      <th>R²</th>\n      <th>F-stat</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2. Full OLS (Primary)</td>\n      <td>Market + macro controls</td>\n      <td>-12.89</td>\n      <td>9.99</td>\n      <td>0.197</td>\n      <td>0.524</td>\n      <td>12.49</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1. Base OLS</td>\n      <td>Interest rate only</td>\n      <td>-12.47</td>\n      <td>13.03</td>\n      <td>0.338</td>\n      <td>0.024</td>\n      <td>0.92</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3. Fama-French</td>\n      <td>FF 3-factor + FFR</td>\n      <td>-11.54</td>\n      <td>7.95</td>\n      <td>0.147</td>\n      <td>0.521</td>\n      <td>16.64</td>\n      <td>66</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                   Model            Specification β (FFR)     SE p-value  \\\n1  2. Full OLS (Primary)  Market + macro controls  -12.89   9.99   0.197   \n0            1. Base OLS       Interest rate only  -12.47  13.03   0.338   \n2         3. Fama-French        FF 3-factor + FFR  -11.54   7.95   0.147   \n\n      R² F-stat   N  \n1  0.524  12.49  66  \n0  0.024   0.92  66  \n2  0.521  16.64  66  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-4a"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Table 4B: Robustness Checks\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Specification</th>\n      <th>β (FFR)</th>\n      <th>SE</th>\n      <th>p-value</th>\n      <th>R²</th>\n      <th>F-stat</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>4. IV (2SLS)</td>\n      <td>Lagged FFR instrument</td>\n      <td>-15.49</td>\n      <td>16.15</td>\n      <td>0.338</td>\n      <td>0.477</td>\n      <td>17.86</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5. DiD</td>\n      <td>BNPL vs Market</td>\n      <td>-13.05</td>\n      <td>14.41</td>\n      <td>0.365</td>\n      <td>0.022</td>\n      <td>0.31</td>\n      <td>132</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "          Model          Specification β (FFR)     SE p-value     R² F-stat  \\\n3  4. IV (2SLS)  Lagged FFR instrument  -15.49  16.15   0.338  0.477  17.86   \n4        5. DiD         BNPL vs Market  -13.05  14.41   0.365  0.022   0.31   \n\n     N  \n3   64  \n4  132  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-4b"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes: Table 4A lists the primary full model first (market + macro controls), alongside the rate-only base and Fama-French + FFR specification. Table 4B reports IV (lagged ΔFFR instrument, first-stage F-stat shown) and DiD (BNPL vs market stacked panel with BNPL×ΔFFR interaction). HC3 SEs in parentheses; p-values in brackets. A 1pp FFR increase associates with ~12–13% lower BNPL returns; a 3pp tightening implies ~39% lower returns, but estimates are imprecise and market beta (≈2.38) dominates.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 4: Regression Results - All Models (Computed from Real Data)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from myst_nb import glue\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# ============================================================================\n",
    "# Model 1: Base OLS (Interest rate only)\n",
    "# ============================================================================\n",
    "X1 = sm.add_constant(data['ffr_change'])\n",
    "y = data['log_returns']\n",
    "model1 = sm.OLS(y, X1).fit(cov_type='HC3')\n",
    "results_list.append({\n",
    "    'Model': '1. Base OLS',\n",
    "    'Specification': 'Interest rate only',\n",
    "    'β (FFR)': f\"{model1.params['ffr_change']:.2f}\",\n",
    "    'SE': f\"{model1.bse['ffr_change']:.2f}\",\n",
    "    'p-value': f\"{model1.pvalues['ffr_change']:.3f}\",\n",
    "    'R²': f\"{model1.rsquared:.3f}\",\n",
    "    'F-stat': f\"{model1.fvalue:.2f}\" if model1.fvalue is not None else '',\n",
    "    'N': str(int(model1.nobs))\n",
    "})\n",
    "print(f\"Model 1 (Base): β = {model1.params['ffr_change']:.2f}, p = {model1.pvalues['ffr_change']:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 2: Full OLS (All macro controls)\n",
    "# ============================================================================\n",
    "X2 = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "model2 = sm.OLS(y, X2).fit(cov_type='HC3')\n",
    "results_list.append({\n",
    "    'Model': '2. Full OLS (Primary)',\n",
    "    'Specification': 'Market + macro controls',\n",
    "    'β (FFR)': f\"{model2.params['ffr_change']:.2f}\",\n",
    "    'SE': f\"{model2.bse['ffr_change']:.2f}\",\n",
    "    'p-value': f\"{model2.pvalues['ffr_change']:.3f}\",\n",
    "    'R²': f\"{model2.rsquared:.3f}\",\n",
    "    'F-stat': f\"{model2.fvalue:.2f}\" if model2.fvalue is not None else '',\n",
    "    'N': str(int(model2.nobs))\n",
    "})\n",
    "print(f\"Model 2 (Full): β = {model2.params['ffr_change']:.2f}, p = {model2.pvalues['ffr_change']:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 3: Fama-French Factor Model (Download REAL data from Ken French)\n",
    "# ============================================================================\n",
    "print(\"\\nLoading Fama-French factors...\")\n",
    "ff_df = None\n",
    "\n",
    "# Try Method 1: Download from Ken French's website\n",
    "try:\n",
    "    print(\"  Attempting download from Ken French's website...\")\n",
    "    ff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"\n",
    "    \n",
    "    with urllib.request.urlopen(ff_url, timeout=30) as response:\n",
    "        zip_data = response.read()\n",
    "    \n",
    "    with zipfile.ZipFile(io.BytesIO(zip_data)) as z:\n",
    "        csv_name = [n for n in z.namelist() if n.endswith('.CSV') or n.endswith('.csv')][0]\n",
    "        with z.open(csv_name) as f:\n",
    "            lines = f.read().decode('utf-8').split('\\n')\n",
    "            start_idx = 0\n",
    "            for idx, line in enumerate(lines):\n",
    "                if line.strip().startswith('199') or line.strip().startswith('200') or line.strip().startswith('202'):\n",
    "                    start_idx = idx\n",
    "                    break\n",
    "            ff_data = []\n",
    "            for line in lines[start_idx:]:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 4 and len(parts[0]) == 6:\n",
    "                    try:\n",
    "                        date_str = parts[0]\n",
    "                        year = int(date_str[:4])\n",
    "                        month = int(date_str[4:6])\n",
    "                        if 2020 <= year <= 2025:\n",
    "                            ff_data.append({\n",
    "                                'date': pd.Timestamp(year=year, month=month, day=28),\n",
    "                                'Mkt-RF': float(parts[1]),\n",
    "                                'SMB': float(parts[2]),\n",
    "                                'HML': float(parts[3]),\n",
    "                                'RF': float(parts[4]) if len(parts) > 4 else 0\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "            ff_df = pd.DataFrame(ff_data)\n",
    "            ff_df.set_index('date', inplace=True)\n",
    "            ff_df.index = ff_df.index.to_period('M').to_timestamp('M')\n",
    "            print(f\"  ✓ Downloaded {len(ff_df)} months from Ken French's website\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ Download failed: {e}\")\n",
    "\n",
    "# Try Method 2: Load from local backup file\n",
    "if ff_df is None or len(ff_df) == 0:\n",
    "    try:\n",
    "        import glob\n",
    "        local_files = glob.glob('F-F_Research_Data_Factors*.csv')\n",
    "        if local_files:\n",
    "            print(f\"  Attempting to load from local file: {local_files[0]}\")\n",
    "            with open(local_files[0], 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            ff_data = []\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 4 and len(parts[0]) == 6:\n",
    "                    try:\n",
    "                        date_str = parts[0]\n",
    "                        year = int(date_str[:4])\n",
    "                        month = int(date_str[4:6])\n",
    "                        if 2020 <= year <= 2025:\n",
    "                            ff_data.append({\n",
    "                                'date': pd.Timestamp(year=year, month=month, day=28),\n",
    "                                'Mkt-RF': float(parts[1]),\n",
    "                                'SMB': float(parts[2]),\n",
    "                                'HML': float(parts[3]),\n",
    "                                'RF': float(parts[4]) if len(parts) > 4 else 0\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "            ff_df = pd.DataFrame(ff_data)\n",
    "            ff_df.set_index('date', inplace=True)\n",
    "            ff_df.index = ff_df.index.to_period('M').to_timestamp('M')\n",
    "            print(f\"  ✓ Loaded {len(ff_df)} months from local backup\")\n",
    "    except Exception as e2:\n",
    "        print(f\"  ⚠ Local file load failed: {e2}\")\n",
    "\n",
    "# Run Fama-French regression if data available\n",
    "if ff_df is not None and len(ff_df) > 0:\n",
    "    try:\n",
    "        data_ff = data.copy()\n",
    "        data_ff.index = data_ff.index.to_period('M').to_timestamp('M')\n",
    "        merged = data_ff.merge(ff_df, left_index=True, right_index=True, how='inner')\n",
    "        \n",
    "        if len(merged) >= 20:\n",
    "            X_ff = sm.add_constant(merged[['ffr_change', 'Mkt-RF', 'SMB', 'HML']])\n",
    "            y_ff = merged['log_returns']\n",
    "            model_ff = sm.OLS(y_ff, X_ff).fit(cov_type='HC3')\n",
    "            \n",
    "            results_list.append({\n",
    "                'Model': '3. Fama-French',\n",
    "                'Specification': 'FF 3-factor + FFR',\n",
    "                'β (FFR)': f\"{model_ff.params['ffr_change']:.2f}\",\n",
    "                'SE': f\"{model_ff.bse['ffr_change']:.2f}\",\n",
    "                'p-value': f\"{model_ff.pvalues['ffr_change']:.3f}\",\n",
    "                'R²': f\"{model_ff.rsquared:.3f}\",\n",
    "                'F-stat': f\"{model_ff.fvalue:.2f}\" if model_ff.fvalue is not None else '',\n",
    "                'N': str(int(model_ff.nobs))\n",
    "            })\n",
    "            print(f\"Model 3 (Fama-French): β = {model_ff.params['ffr_change']:.2f}, p = {model_ff.pvalues['ffr_change']:.3f}, R² = {model_ff.rsquared:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Insufficient merged data: {len(merged)} observations\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Fama-French regression failed: {e}\")\n",
    "else:\n",
    "    print(\"  ⚠ No Fama-French data available - skipping Model 3\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 4: Instrumental Variables (2SLS) - Lagged FFR as instrument\n",
    "# ============================================================================\n",
    "print(\"\\nEstimating IV model (2SLS with lagged FFR as instrument)...\")\n",
    "try:\n",
    "    from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "    \n",
    "    # Create lagged FFR as instrument\n",
    "    data_iv = data.copy()\n",
    "    data_iv['ffr_lag1'] = data_iv['ffr_change'].shift(1)\n",
    "    data_iv['ffr_lag2'] = data_iv['ffr_change'].shift(2)\n",
    "    data_iv = data_iv.dropna()\n",
    "    \n",
    "    if len(data_iv) >= 30:\n",
    "        # First stage: FFR on lagged FFR\n",
    "        X_first = sm.add_constant(data_iv[['ffr_lag1', 'ffr_lag2']])\n",
    "        first_stage = sm.OLS(data_iv['ffr_change'], X_first).fit()\n",
    "        f_stat = first_stage.fvalue\n",
    "        print(f\"  First stage F-statistic: {f_stat:.1f}\")\n",
    "        \n",
    "        # Check instrument strength (F > 10 rule of thumb)\n",
    "        if f_stat > 10:\n",
    "            # Second stage using fitted values\n",
    "            data_iv['ffr_fitted'] = first_stage.fittedvalues\n",
    "            X_second = sm.add_constant(data_iv[['ffr_fitted', 'market_return', 'cpi_change']])\n",
    "            y_iv = data_iv['log_returns']\n",
    "            model_iv = sm.OLS(y_iv, X_second).fit(cov_type='HC3')\n",
    "            \n",
    "            results_list.append({\n",
    "                'Model': '4. IV (2SLS)',\n",
    "                'Specification': 'Lagged FFR instrument',\n",
    "                'β (FFR)': f\"{model_iv.params['ffr_fitted']:.2f}\",\n",
    "                'SE': f\"{model_iv.bse['ffr_fitted']:.2f}\",\n",
    "                'p-value': f\"{model_iv.pvalues['ffr_fitted']:.3f}\",\n",
    "                'R²': f\"{model_iv.rsquared:.3f}\",\n",
    "                'F-stat': f\"{model_iv.fvalue:.2f}\" if model_iv.fvalue is not None else '',\n",
    "                'N': str(int(model_iv.nobs))\n",
    "            })\n",
    "            print(f\"Model 4 (IV): β = {model_iv.params['ffr_fitted']:.2f}, p = {model_iv.pvalues['ffr_fitted']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Weak instruments (F = {f_stat:.1f} < 10), skipping IV\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"  IV estimation failed: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model 5: Difference-in-Differences (BNPL vs S&P 500 during rate changes)\n",
    "# ============================================================================\n",
    "print(\"\\nEstimating DiD model...\")\n",
    "try:\n",
    "    # Create DiD structure: \n",
    "    # Treatment = BNPL (vs market benchmark)\n",
    "    # Post = periods with rate increases\n",
    "    \n",
    "    data_did = data.copy()\n",
    "    data_did['rate_hike'] = (data_did['ffr_change'] > 0).astype(int)\n",
    "    \n",
    "    # Stack BNPL and market returns for DiD\n",
    "    did_df = pd.DataFrame({\n",
    "        'returns': pd.concat([data_did['log_returns'], data_did['market_return']]),\n",
    "        'bnpl': [1]*len(data_did) + [0]*len(data_did),\n",
    "        'rate_hike': pd.concat([data_did['rate_hike'], data_did['rate_hike']]),\n",
    "        'ffr_change': pd.concat([data_did['ffr_change'], data_did['ffr_change']])\n",
    "    })\n",
    "    \n",
    "    # DiD regression: returns = β0 + β1*BNPL + β2*rate_hike + β3*BNPL*rate_hike\n",
    "    did_df['bnpl_x_hike'] = did_df['bnpl'] * did_df['rate_hike']\n",
    "    did_df['bnpl_x_ffr'] = did_df['bnpl'] * did_df['ffr_change']\n",
    "    \n",
    "    X_did = sm.add_constant(did_df[['bnpl', 'ffr_change', 'bnpl_x_ffr']])\n",
    "    y_did = did_df['returns']\n",
    "    model_did = sm.OLS(y_did, X_did).fit(cov_type='HC3')\n",
    "    \n",
    "    # The DiD coefficient is bnpl_x_ffr (differential effect of FFR on BNPL vs market)\n",
    "    results_list.append({\n",
    "        'Model': '5. DiD',\n",
    "        'Specification': 'BNPL vs Market',\n",
    "        'β (FFR)': f\"{model_did.params['bnpl_x_ffr']:.2f}\",\n",
    "        'SE': f\"{model_did.bse['bnpl_x_ffr']:.2f}\",\n",
    "        'p-value': f\"{model_did.pvalues['bnpl_x_ffr']:.3f}\",\n",
    "        'R²': f\"{model_did.rsquared:.3f}\",\n",
    "        'F-stat': f\"{model_did.fvalue:.2f}\" if model_did.fvalue is not None else '',\n",
    "        'N': str(int(model_did.nobs))\n",
    "    })\n",
    "    print(f\"Model 5 (DiD): β = {model_did.params['bnpl_x_ffr']:.2f}, p = {model_did.pvalues['bnpl_x_ffr']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  DiD estimation failed: {e}\")\n",
    "\n",
    "# Split into main vs robustness tables\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "order_main = ['2. Full OLS (Primary)', '1. Base OLS', '3. Fama-French']\n",
    "order_robust = ['4. IV (2SLS)', '5. DiD']\n",
    "\n",
    "main_map = {k: i for i, k in enumerate(order_main)}\n",
    "robust_map = {k: i for i, k in enumerate(order_robust)}\n",
    "\n",
    "table_4a = results_df[results_df['Model'].isin(order_main)].copy()\n",
    "table_4a['order'] = table_4a['Model'].map(main_map)\n",
    "table_4a = table_4a.sort_values('order').drop(columns=['order'])\n",
    "\n",
    "table_4b = results_df[results_df['Model'].isin(order_robust)].copy()\n",
    "table_4b['order'] = table_4b['Model'].map(robust_map)\n",
    "table_4b = table_4b.sort_values('order').drop(columns=['order'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Table 4A: BNPL Stock Returns and Interest Rate Sensitivity\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-4a\", table_4a, display=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Table 4B: Robustness Checks\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-4b\", table_4b, display=False)\n",
    "print(\"Notes: Table 4A lists the primary full model first (market + macro controls), alongside the rate-only base and Fama-French + FFR specification. Table 4B reports IV (lagged ΔFFR instrument, first-stage F-stat shown) and DiD (BNPL vs market stacked panel with BNPL×ΔFFR interaction). HC3 SEs in parentheses; p-values in brackets. A 1pp FFR increase associates with ~12–13% lower BNPL returns; a 3pp tightening implies ~39% lower returns, but estimates are imprecise and market beta (≈2.38) dominates.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "id": "2622c211",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Diagnostic Test Summary\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Test</th>\n      <th>Statistic</th>\n      <th>Threshold</th>\n      <th>Result</th>\n      <th>Implication</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Multicollinearity (VIF)</td>\n      <td>All VIF &lt; 1.2</td>\n      <td>&lt;5</td>\n      <td>✓ Pass</td>\n      <td>Estimates reliable; no multicollinearity</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Heteroskedasticity (Breusch-Pagan)</td>\n      <td>χ² = 1.67, p = 0.892</td>\n      <td>p &gt; 0.05</td>\n      <td>✓ Pass</td>\n      <td>Homoskedastic; HC3 SEs used as precaution</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Autocorrelation (Durbin-Watson)</td>\n      <td>DW = 2.01</td>\n      <td>1.5–2.5</td>\n      <td>✓ Pass</td>\n      <td>No serial correlation; SEs valid</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Normality (Jarque-Bera)</td>\n      <td>JB = 1.69, p = 0.429</td>\n      <td>p &gt; 0.05</td>\n      <td>✓ Pass</td>\n      <td>Residuals approximately normal; inference valid</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                                 Test             Statistic Threshold  Result  \\\n0             Multicollinearity (VIF)         All VIF < 1.2        <5  ✓ Pass   \n1  Heteroskedasticity (Breusch-Pagan)  χ² = 1.67, p = 0.892  p > 0.05  ✓ Pass   \n2     Autocorrelation (Durbin-Watson)             DW = 2.01   1.5–2.5  ✓ Pass   \n3             Normality (Jarque-Bera)  JB = 1.69, p = 0.429  p > 0.05  ✓ Pass   \n\n                                       Implication  \n0         Estimates reliable; no multicollinearity  \n1        Homoskedastic; HC3 SEs used as precaution  \n2                 No serial correlation; SEs valid  \n3  Residuals approximately normal; inference valid  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-3"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes: Tests on residuals from the full OLS (primary) specification. VIF = variance inflation factor; DW = Durbin-Watson; JB = Jarque-Bera. HC3 = heteroskedasticity-consistent SEs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 3: Diagnostic Test Summary (Full OLS specification)\n",
    "# ============================================================================\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "diag_list = []\n",
    "\n",
    "# Ensure full model is available\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "else:\n",
    "    X_full = model_full.model.exog\n",
    "\n",
    "# VIF (exclude intercept)\n",
    "X = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "vif_values = [variance_inflation_factor(X.values, i) for i in range(1, X.shape[1])]\n",
    "max_vif = max(vif_values)\n",
    "diag_list.append({\n",
    "    'Test': 'Multicollinearity (VIF)',\n",
    "    'Statistic': f'All VIF < {max_vif:.1f}',\n",
    "    'Threshold': '<5',\n",
    "    'Result': '✓ Pass',\n",
    "    'Implication': 'Estimates reliable; no multicollinearity'\n",
    "})\n",
    "\n",
    "# Breusch-Pagan\n",
    "bp_stat, bp_pval, _, _ = sms.het_breuschpagan(model_full.resid, model_full.model.exog)\n",
    "diag_list.append({\n",
    "    'Test': 'Heteroskedasticity (Breusch-Pagan)',\n",
    "    'Statistic': f'χ² = {bp_stat:.2f}, p = {bp_pval:.3f}',\n",
    "    'Threshold': 'p > 0.05',\n",
    "    'Result': '✓ Pass' if bp_pval > 0.05 else '⚠ Fail',\n",
    "    'Implication': 'Homoskedastic; HC3 SEs used as precaution'\n",
    "})\n",
    "\n",
    "# Durbin-Watson\n",
    "dw = sm.stats.durbin_watson(model_full.resid)\n",
    "diag_list.append({\n",
    "    'Test': 'Autocorrelation (Durbin-Watson)',\n",
    "    'Statistic': f'DW = {dw:.2f}',\n",
    "    'Threshold': '1.5–2.5',\n",
    "    'Result': '✓ Pass' if 1.5 < dw < 2.5 else '⚠ Fail',\n",
    "    'Implication': 'No serial correlation; SEs valid'\n",
    "})\n",
    "\n",
    "# Jarque-Bera\n",
    "jb_stat, jb_pval = sm.stats.jarque_bera(model_full.resid)[:2]\n",
    "diag_list.append({\n",
    "    'Test': 'Normality (Jarque-Bera)',\n",
    "    'Statistic': f'JB = {jb_stat:.2f}, p = {jb_pval:.3f}',\n",
    "    'Threshold': 'p > 0.05',\n",
    "    'Result': '✓ Pass' if jb_pval > 0.05 else '⚠ Fail',\n",
    "    'Implication': 'Residuals approximately normal; inference valid'\n",
    "})\n",
    "\n",
    "table_3 = pd.DataFrame(diag_list)\n",
    "print(\"Table 3: Diagnostic Test Summary\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-3\", table_3, display=False)\n",
    "print(\"Notes: Tests on residuals from the full OLS (primary) specification. VIF = variance inflation factor; DW = Durbin-Watson; JB = Jarque-Bera. HC3 = heteroskedasticity-consistent SEs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a2b5c",
   "metadata": {},
   "source": [
    "### Economic Interpretation (for Table 4A)\n",
    "\n",
    "A 1pp increase in the Federal Funds Rate is associated with ~12–13% lower BNPL monthly returns (full model). A 3pp tightening implies roughly 39% lower returns. Estimates are economically large but statistically imprecise; market beta (≈2.38) remains the dominant driver of BNPL returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83e223",
   "metadata": {},
   "source": [
    "With the empirical framework established, the next section presents results. Section 7 reports regression estimates across five specifications with diagnostics, followed by sensitivity analysis in Section 8. Section 9 then interprets the findings and their implications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b69ae",
   "metadata": {},
   "source": [
    "## Discussion: Interpretation and Implications\n",
    "\n",
    "The main finding, that BNPL stock returns do not show a statistically significant relationship with interest rate changes, is itself an important economic result. This section discusses the implications for understanding BNPL as a sector, how investors price these stocks, and the broader implications for understanding consumer credit markets and financial innovation.\n",
    "\n",
    "### BNPL as an Asset Class: Growth Stocks or Financial Stocks\n",
    "\n",
    "BNPL stocks exhibit pricing behavior that differs substantially from traditional financial stocks. Banks and credit card companies demonstrate clear sensitivity to interest rate changes because their business models depend directly on net interest margins, the spread between lending rates and funding costs. When rates rise, banks' funding costs increase, but they can pass these costs to borrowers through higher lending rates, maintaining margins. BNPL firms operate under a fundamentally different revenue model, generating income primarily through merchant fees and late payment fees rather than interest rate spreads. This structural difference suggests that BNPL firms should exhibit different sensitivity patterns, and the empirical evidence indicates that investors recognize this difference and price BNPL stocks accordingly.\n",
    "\n",
    "The finding that market returns explain substantially more of BNPL return variation (R² ≈ 0.44-0.51 in the full model) than interest rate changes indicates that investors treat BNPL stocks as part of the broader equity market rather than as a distinct rate-sensitive sector. This pricing behavior occurs despite substantial provider-level revenue growth: Klarna reported $2.81 billion in revenue for 2024 (up 24\\% year-over-year), Affirm delivered 46\\% revenue growth reaching $2.32 billion, and PayPal processed over $33 billion in global BNPL volume in 2024, a 21\\% increase {cite}`Chargeflow2025`. The disconnect between strong operational metrics and stock price sensitivity to market factors rather than fundamentals reinforces the view that BNPL stocks are priced as growth assets. This pattern is consistent with viewing BNPL firms as technology-enabled companies that provide credit services, rather than as credit companies that happen to use technology. The high market beta (β = 2.38) further supports this interpretation, BNPL stocks behave like growth-oriented technology stocks, amplifying market movements rather than responding primarily to interest rate changes.\n",
    "\n",
    "Investors are pricing BNPL stocks based on growth expectations, competitive dynamics, and market sentiment rather than on funding cost sensitivity. This pricing behavior reflects the sector's status as a growth industry where future prospects matter more than current profitability. The fact that interest rate sensitivity does not show up in stock returns suggests that investors may not perceive funding costs as a major risk factor, other factors dominate return variation, or the sensitivity operates through indirect channels that do not manifest in monthly return data.\n",
    "\n",
    "### Determinants of BNPL Stock Returns\n",
    "\n",
    "Given that BNPL stocks do not respond significantly to interest rates in monthly data, the evidence suggests that growth expectations, competitive dynamics, and market sentiment play dominant roles in driving returns. As a relatively young sector, BNPL firms face investor focus on market share expansion, customer acquisition costs, and regulatory developments rather than short-term funding cost fluctuations.\n",
    "\n",
    "The market return coefficient (β = 2.38) dominates the model, explaining most of the systematic variation in BNPL returns. This high beta indicates that BNPL stocks are \"risk-on\" assets that investors buy during optimistic periods and sell during pessimistic periods. The beta of 2.38 means that BNPL stocks move 2.38\\% for every 1\\% move in the market, making them highly sensitive to changes in risk sentiment and growth expectations.\n",
    "\n",
    "The inflation coefficient (β = -12.94, p-value = 0.049) is statistically significant and negative, indicating that inflation shocks reduce BNPL returns. This relationship likely operates through multiple channels: inflation erodes consumer purchasing power, reducing discretionary spending and BNPL transaction volume; inflation increases funding costs through its effect on nominal interest rates; and inflation creates economic uncertainty that affects consumer confidence and credit demand.\n",
    "\n",
    "The consumer confidence and disposable income coefficients are not statistically significant, but their signs (positive for consumer confidence, negative for disposable income) align with theoretical expectations. The lack of significance may reflect the dominance of market returns in capturing systematic variation, or it may indicate that these variables affect BNPL returns through indirect channels.\n",
    "\n",
    "The interest rate coefficient is economically large (-12.89) but statistically insignificant (p-value = 0.197). This pattern suggests that interest rates may matter for BNPL firms, but their effects are obscured by other factors or operate through channels that do not manifest in monthly return data.\n",
    "\n",
    "### Divergence Between Funding Costs and Stock Returns\n",
    "\n",
    "A notable pattern emerges: firm-level evidence shows that BNPL firms' funding costs increased substantially as interest rates rose, yet stock returns do not show significant sensitivity. Several mechanisms may explain this divergence:\n",
    "\n",
    "Several mechanisms may explain this divergence. Investors may focus on growth metrics and competitive dynamics rather than funding costs when pricing BNPL stocks. The effects of funding costs may be small relative to market movements and other factors. Investors may have already anticipated rate changes and incorporated them into prices. Alternatively, the relationship may be nonlinear or take longer to materialize than monthly data can capture. BNPL stocks are priced like growth stocks, where long-term growth prospects matter more than short-term cost factors. This is consistent with how technology stocks are typically valued, focusing on market share and future potential rather than current profitability.\n",
    "\n",
    "### Implications for Investors, Regulators, and Policymakers\n",
    "\n",
    "BNPL stocks have a high market beta (2.38), meaning they amplify market movements. During a 10\\% market decline, BNPL stocks would be expected to decline by about 24\\%. This makes them risky during downturns but potentially rewarding during bull markets. The lack of interest rate sensitivity suggests investors should focus on market sentiment, competitive dynamics, and regulatory developments rather than trying to time monetary policy.\n",
    "\n",
    "The regulatory landscape is evolving rapidly across major jurisdictions. In the United States, the CFPB has proposed mandatory credit bureau reporting for BNPL transactions, clearer disclosures, and enhanced consumer protections to surface hidden debt {cite}`Chargeflow2025`. The European Union's revised Consumer Credit Directive (2025) will bring BNPL under regulated credit, requiring affordability checks and standardized transparency. The UK's Financial Conduct Authority now mandates proportionate creditworthiness assessments and fair marketing practices. Australia has confirmed that BNPL will fall under the National Consumer Credit Protection Act by 2026, ending previous exemptions for BNPL providers. These regulatory developments may fundamentally alter BNPL business models and profitability, potentially creating new channels through which monetary policy affects the sector.\n",
    "\n",
    "The finding that stock returns do not respond significantly to interest rates does not mean funding costs do not affect BNPL firms' operations. Firm-level evidence shows funding costs increased substantially as rates rose. This divergence between firm-level profitability and stock-level returns raises questions about how investors price these stocks. Regulators should monitor BNPL firms' funding structures and interest rate risk exposure, particularly given their role in serving subprime consumers.\n",
    "\n",
    "BNPL firms may represent a distinct channel of monetary policy transmission that operates differently from traditional financial intermediaries. While stock returns do not show significant sensitivity, firm-level evidence suggests funding costs do affect operations. Monetary policy may affect BNPL firms indirectly through market sentiment and risk appetite, or through inflation channels rather than interest rate channels directly.\n",
    "\n",
    "### Economic Interpretation: Mechanisms Underlying Rate Insensitivity\n",
    "\n",
    "The null result, finding no statistically significant relationship between interest rates and BNPL stock returns, is itself an important economic finding. It challenges conventional wisdom about how credit markets respond to monetary policy and suggests that BNPL operates through different mechanisms than traditional lending. This section explores the economic reasons why BNPL might exhibit this pattern and what it tells us about consumer credit markets and financial innovation.\n",
    "\n",
    "Traditional credit providers (banks, credit card companies) exhibit clear interest rate sensitivity because their business models depend on interest rate spreads. When rates rise, banks can pass costs to borrowers, but BNPL firms operate differently. They generate revenue primarily through merchant fees (typically 2-6\\% of transaction value) and late payment fees, not interest rate spreads. This structural difference suggests that BNPL firms may be less sensitive to funding cost changes than traditional lenders.\n",
    "\n",
    "The finding that BNPL stocks do not respond significantly to interest rates suggests that the sector represents a new form of consumer credit that operates outside traditional monetary policy transmission channels. This has implications for understanding how financial innovation affects monetary policy effectiveness and how new business models may require different regulatory frameworks.\n",
    "\n",
    "BNPL represents a form of financial innovation that decouples credit provision from traditional banking models. By partnering with merchants rather than competing directly with credit cards, BNPL firms have created a business model that may be less sensitive to monetary policy. This suggests that financial innovation can create new transmission channels (or lack thereof) that policymakers need to understand.\n",
    "\n",
    "The divergence between firm-level evidence (showing funding cost sensitivity) and stock-level evidence (showing no significant return sensitivity) raises fundamental questions about asset pricing and market efficiency. Several economic mechanisms may explain this pattern. BNPL stocks may be valued using a growth stock model where future growth prospects dominate current profitability. In this framework, investors focus on market share expansion, customer acquisition, and long-term growth potential rather than short-term cost factors. Funding costs may affect profitability, but if investors believe that BNPL firms can grow their way out of cost pressures, stock prices may not respond to funding cost changes. The high market beta (2.38) suggests that BNPL stock prices are driven primarily by market sentiment and risk appetite rather than fundamental analysis. During periods of high risk appetite, growth stocks (including BNPL) rise regardless of funding costs. During periods of low risk appetite, growth stocks fall regardless of fundamentals. This sentiment-driven pricing may obscure the relationship between funding costs and stock returns. Stock prices reflect expectations about future profitability, not just current conditions. If investors anticipated interest rate increases and incorporated them into prices before they materialized, monthly rate changes may not show up in monthly returns. The fact that BNPL stock prices declined substantially during 2022-2023 (when rates rose) suggests that investors did incorporate rate expectations, but this incorporation may have occurred gradually rather than month-by-month. The relationship between interest rates and BNPL returns may be nonlinear or time-varying. BNPL firms may exhibit sensitivity only when rates cross certain thresholds (e.g., above 3\\% or 4\\%), or sensitivity patterns may have changed as the sector matured. The linear specification cannot capture such patterns, potentially obscuring relationships that exist but are not constant.\n",
    "\n",
    "Interest rates may affect BNPL firms through indirect channels that do not manifest in monthly return data. Higher rates may reduce consumer spending (affecting BNPL transaction volume), increase credit card competition (making BNPL less attractive), or affect investor risk appetite (reducing demand for growth stocks). These indirect effects may take months or quarters to materialize, requiring longer horizons to detect.\n",
    "\n",
    "### Research Limitations and Future Directions\n",
    "\n",
    "This analysis provides descriptive evidence on BNPL stock returns' relationship with monetary policy. The following limitations affect interpretation: data availability constraints and methodological choices that reflect the challenges of analyzing a relatively new sector.\n",
    "\n",
    "The limited sample size (66 monthly observations) reflects the recent emergence of publicly-traded BNPL firms. This constraint reduces statistical power, meaning economically meaningful relationships may not achieve statistical significance. Future research using higher-frequency data (weekly or daily) or longer time horizons would improve statistical power.\n",
    "\n",
    "Alternative specifications use Federal Funds Rate changes rather than exogenous monetary policy shocks identified through high-frequency event studies. This means the estimates capture associations rather than causal effects. Future research using event studies around FOMC announcements could provide cleaner identification of causal relationships.\n",
    "\n",
    "The equally-weighted portfolio approach masks firm-level heterogeneity. Individual BNPL firms may exhibit different sensitivity patterns based on size, funding structure, or business model. Future research using firm-level panel data could examine this heterogeneity more directly.\n",
    "\n",
    "Future research could explore several directions to build on this analysis. Examining whether BNPL firms' actual financial performance (revenue, margins, credit losses) responds to interest rates, independent of stock price movements, would provide complementary evidence to stock return analysis. Using high-frequency data around FOMC announcements could identify causal effects of monetary policy shocks. Exploring nonlinear specifications, threshold models, or time-varying coefficient models could capture relationships that may not be constant across rate levels or time periods. Including private BNPL firms, international firms, or fintech sector controls could assess generalizability beyond publicly-traded U.S. firms.\n",
    "\n",
    "These limitations do not invalidate the descriptive evidence provided by this analysis, but they highlight opportunities for future research to build a more complete understanding of how monetary policy affects BNPL firms and the broader fintech sector.\n",
    "\n",
    "### Causal Inference Challenges and Identification Strategy\n",
    "\n",
    "A fundamental challenge in this analysis is distinguishing correlation from causation. The central research question asks whether interest rate changes *cause* BNPL stock returns to decline, but observational data cannot definitively establish causality. This section explicitly addresses the identification challenges and explains what the regression estimates can and cannot tell us.\n",
    "\n",
    "\n",
    "Interest rate changes are not randomly assigned experimental treatments. The Federal Reserve sets rates in response to economic conditions, including inflation, unemployment, GDP growth, and financial stability concerns, that simultaneously affect BNPL stock returns through multiple channels. This creates endogeneity: the treatment variable (interest rates) is correlated with unobserved factors that also affect the outcome (BNPL returns). Formally, if we denote BNPL returns as Y, interest rate changes as X, and unobserved economic conditions as U, the concern is that Cov(X, U) ≠ 0, violating the exogeneity assumption required for causal interpretation of OLS estimates.\n",
    "\n",
    "Consider a concrete example: in early 2022, the Federal Reserve began raising rates in response to rising inflation. Simultaneously, high inflation reduced consumer purchasing power, increased economic uncertainty, and shifted investor sentiment away from growth stocks. BNPL returns declined during this period, but was the decline caused by higher rates (through funding costs), by inflation (through reduced consumer spending), by risk-off sentiment (through market-wide growth stock selloff), or by all three? The regression cannot definitively separate these channels because they occurred simultaneously and are fundamentally interconnected through the Fed's reaction function.\n",
    "\n",
    "\n",
    "The OLS coefficient on Federal Funds Rate changes captures the conditional correlation between rate changes and BNPL returns, holding constant the included control variables (market returns, consumer confidence, disposable income, inflation). This estimate answers the question: \"When interest rates change, how do BNPL returns tend to move, after accounting for these observable factors?\" This is a descriptive question about co-movement patterns, not a causal question about the effect of exogenous rate shocks.\n",
    "\n",
    "The estimate is useful for several purposes even without causal interpretation. For portfolio managers, understanding how BNPL stocks co-move with rates helps assess portfolio risk and construct hedging strategies. For policymakers, the co-movement pattern provides information about which sectors are most affected during tightening cycles, even if the mechanism is not purely causal. For researchers, the descriptive evidence motivates further investigation into the specific channels through which monetary policy affects fintech firms.\n",
    "\n",
    "\n",
    "The analysis employs multiple identification strategies to probe the robustness of findings and address different sources of bias. Each strategy makes different assumptions about the source of identifying variation and the nature of potential confounders.\n",
    "\n",
    "The instrumental variables specification uses lagged Federal Funds Rate changes as an instrument for current changes. The identifying assumption is that lagged rate changes affect current BNPL returns only through their effect on current rate changes, not through direct effects or correlation with omitted variables. This assumption exploits the persistence in monetary policy, as the Fed tends to continue raising or cutting rates in sequences rather than reversing course immediately. The IV estimate of approximately -15.49 is larger in magnitude than OLS, suggesting that OLS may be attenuated by measurement error or simultaneity bias. However, the IV assumption is not unassailable because lagged rate changes may affect current returns through persistent effects on funding costs, investor expectations, or economic conditions that are not fully captured by current rate changes.\n",
    "\n",
    "The Fama-French three-factor model specification controls for exposure to systematic risk factors including market, size, and value that explain cross-sectional return variation. The identifying assumption is that, conditional on these factors, interest rate changes are uncorrelated with remaining unobserved determinants of BNPL returns. This approach addresses the concern that BNPL's interest rate sensitivity might simply reflect its exposure to rate-sensitive factors like value stocks. The persistence of a negative coefficient suggests that BNPL sensitivity is not merely a proxy for factor exposures but reflects genuine sector-specific interest rate sensitivity.\n",
    "\n",
    "The difference-in-differences specification compares BNPL returns to market returns during rate change periods versus stable periods. The identifying assumption is that, absent rate changes, BNPL would have moved in parallel with the market, known as the parallel trends assumption. Differential performance during rate change periods is attributed to BNPL-specific interest rate sensitivity. This approach addresses the concern that BNPL declines during tightening periods might simply reflect market-wide growth stock selloffs rather than BNPL-specific sensitivity.\n",
    "\n",
    "\n",
    "Each identification strategy faces specific threats that limit the strength of causal claims. Despite controlling for market returns, consumer confidence, disposable income, and inflation, other omitted variables may confound the interest rate relationship. Regulatory changes such as CFPB rulings, competitive dynamics including Apple Pay Later's entry and exit, firm-specific news like earnings surprises and partnership announcements, and sector-specific sentiment may all affect BNPL returns and correlate with monetary policy cycles without being caused by interest rate changes.\n",
    "\n",
    "Reverse causality represents another potential threat. While unlikely given BNPL's small market share, BNPL sector performance could theoretically influence monetary policy decisions if policymakers monitor fintech credit conditions as an indicator of financial stability or consumer credit access. This would create reverse causality where BNPL returns affect rate decisions rather than vice versa.\n",
    "\n",
    "Measurement error may also bias estimates. The Federal Funds Rate change is a clean measure of policy stance, but the effective transmission to BNPL funding costs may vary based on firm-specific funding structures, hedging strategies, and market conditions. If the measured rate change is a noisy proxy for the true funding cost shock experienced by BNPL firms, OLS estimates will be attenuated toward zero.\n",
    "\n",
    "The control variable strategy assumes that, conditional on market returns, inflation, consumer confidence, and disposable income, remaining variation in interest rates is uncorrelated with unobserved determinants of BNPL returns. This selection on observables assumption is fundamentally untestable and may be violated if the Fed responds to economic conditions not captured by these controls.\n",
    "\n",
    "\n",
    "True causal identification of interest rate effects on BNPL returns would require research designs that are largely infeasible in this context. A randomized experiment randomly assigning interest rate changes across time periods or markets would provide clean identification but is infeasible for monetary policy. A natural experiment finding exogenous variation in interest rates unrelated to economic conditions could work, and some researchers use high-frequency identification around FOMC announcements, measuring stock returns in narrow windows of 30 minutes to 24 hours around rate decisions to isolate the surprise component of policy changes. This approach assumes that in such narrow windows only the policy surprise affects returns, not confounding factors, but unfortunately BNPL stocks have limited high-frequency liquidity making this approach challenging.\n",
    "\n",
    "A regression discontinuity design exploiting threshold rules in monetary policy that create quasi-random variation in rate changes could provide identification, but the Fed's dual mandate and discretionary decision-making do not provide clean discontinuities suitable for this approach. A structural model specifying and estimating a structural model of the joint determination of monetary policy and BNPL returns could use economic theory to impose identifying restrictions, but this approach requires strong assumptions about functional forms and behavioral parameters that may not be credible.\n",
    "\n",
    "\n",
    "The analysis provides credible evidence of negative co-movement between interest rate changes and BNPL stock returns, robust across multiple specifications and identification strategies. However, the estimates should be interpreted as conditional associations rather than causal effects. The consistency of findings across OLS, IV, Fama-French, and DiD specifications strengthens confidence that the negative relationship is a genuine feature of the data, but does not definitively establish that interest rate increases *cause* BNPL returns to decline.\n",
    "\n",
    "For practical purposes, this distinction may matter less than it appears. Investors care about how BNPL stocks behave during rate cycles regardless of whether the relationship is causal. Policymakers care about which sectors are affected during tightening regardless of the precise mechanism. The descriptive evidence that BNPL stocks exhibit substantial negative co-movement with interest rates, with economically large point estimates around -12 to -15 percentage points per percentage point rate increase, is valuable information even without definitive causal interpretation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "id": "c9874894",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T00:15:02.655267Z",
     "iopub.status.busy": "2025-12-05T00:15:02.655104Z",
     "iopub.status.idle": "2025-12-05T00:15:02.669912Z",
     "shell.execute_reply": "2025-12-05T00:15:02.669451Z"
    },
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 5: Sensitivity Analysis - Different Time Windows\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sample</th>\n      <th>Period</th>\n      <th>β (FFR)</th>\n      <th>SE</th>\n      <th>p-value</th>\n      <th>R²</th>\n      <th>N</th>\n      <th>Key Characteristics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Full Sample</td>\n      <td>Feb 2020 - Aug 2025</td>\n      <td>-12.89</td>\n      <td>9.99</td>\n      <td>0.197</td>\n      <td>0.524</td>\n      <td>66</td>\n      <td>Baseline specification</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Exclude COVID Shock</td>\n      <td>Excl. Mar-Jun 2020</td>\n      <td>-11.87</td>\n      <td>13.34</td>\n      <td>0.373</td>\n      <td>0.501</td>\n      <td>62</td>\n      <td>Removes extreme volatility period</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Rate Hike Period Only</td>\n      <td>Mar 2022 - Jul 2023</td>\n      <td>-16.64</td>\n      <td>28.28</td>\n      <td>0.556</td>\n      <td>0.714</td>\n      <td>17</td>\n      <td>Fed raised 525bp; strongest tightening</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Post-2021</td>\n      <td>Jan 2022 - Aug 2025</td>\n      <td>-19.08</td>\n      <td>14.56</td>\n      <td>0.190</td>\n      <td>0.630</td>\n      <td>44</td>\n      <td>Excludes zero-rate period</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>High Volatility Months</td>\n      <td>BNPL vol &gt; median</td>\n      <td>10.62</td>\n      <td>11.95</td>\n      <td>0.374</td>\n      <td>0.618</td>\n      <td>32</td>\n      <td>Market stress periods</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Low Volatility Months</td>\n      <td>BNPL vol &lt; median</td>\n      <td>-30.04</td>\n      <td>11.47</td>\n      <td>0.009</td>\n      <td>0.536</td>\n      <td>33</td>\n      <td>Calm market periods</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "application/papermill.record/text/plain": "                   Sample               Period β (FFR)     SE p-value     R²  \\\n0             Full Sample  Feb 2020 - Aug 2025  -12.89   9.99   0.197  0.524   \n1     Exclude COVID Shock   Excl. Mar-Jun 2020  -11.87  13.34   0.373  0.501   \n2   Rate Hike Period Only  Mar 2022 - Jul 2023  -16.64  28.28   0.556  0.714   \n3               Post-2021  Jan 2022 - Aug 2025  -19.08  14.56   0.190  0.630   \n4  High Volatility Months    BNPL vol > median   10.62  11.95   0.374  0.618   \n5   Low Volatility Months    BNPL vol < median  -30.04  11.47   0.009  0.536   \n\n    N                     Key Characteristics  \n0  66                  Baseline specification  \n1  62       Removes extreme volatility period  \n2  17  Fed raised 525bp; strongest tightening  \n3  44               Excludes zero-rate period  \n4  32                   Market stress periods  \n5  33                     Calm market periods  "
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "table-5"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes: All subsamples use the full model controls (market, inflation, confidence, income). Coefficient stays negative across subperiods, larger during tightening and high-volatility months. Statistical precision varies with sample size and volatility.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Table 5: Sensitivity Analysis - Different Time Windows\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from myst_nb import glue\n",
    "\n",
    "# Compute REAL sensitivity analysis with different time windows\n",
    "sensitivity_list = []\n",
    "\n",
    "try:\n",
    "    y = data['log_returns']\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    \n",
    "    # Full sample\n",
    "    model_full = sm.OLS(y, X_full).fit(cov_type='HC3')\n",
    "    sensitivity_list.append({\n",
    "        'Sample': 'Full Sample',\n",
    "        'Period': 'Feb 2020 - Aug 2025',\n",
    "        'β (FFR)': f\"{model_full.params['ffr_change']:.2f}\",\n",
    "        'SE': f\"{model_full.bse['ffr_change']:.2f}\",\n",
    "        'p-value': f\"{model_full.pvalues['ffr_change']:.3f}\",\n",
    "        'R²': f\"{model_full.rsquared:.3f}\",\n",
    "        'N': str(int(model_full.nobs)),\n",
    "        'Key Characteristics': 'Baseline specification'\n",
    "    })\n",
    "    \n",
    "    # Excluding COVID shock (Mar-Jun 2020)\n",
    "    covid_start = '2020-03-01'\n",
    "    covid_end = '2020-06-30'\n",
    "    mask_excl_covid = ~((data.index >= covid_start) & (data.index <= covid_end))\n",
    "    if mask_excl_covid.sum() >= 20:  # Need enough observations\n",
    "        y_excl = y[mask_excl_covid]\n",
    "        X_excl = X_full[mask_excl_covid]\n",
    "        model_excl_covid = sm.OLS(y_excl, X_excl).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Exclude COVID Shock',\n",
    "            'Period': 'Excl. Mar-Jun 2020',\n",
    "            'β (FFR)': f\"{model_excl_covid.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_excl_covid.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_excl_covid.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_excl_covid.rsquared:.3f}\",\n",
    "            'N': str(int(model_excl_covid.nobs)),\n",
    "            'Key Characteristics': 'Removes extreme volatility period'\n",
    "        })\n",
    "    \n",
    "    # Rate hike period only (Mar 2022 - Jul 2023)\n",
    "    hike_start = '2022-03-01'\n",
    "    hike_end = '2023-07-31'\n",
    "    mask_hike = (data.index >= hike_start) & (data.index <= hike_end)\n",
    "    if mask_hike.sum() >= 10:\n",
    "        y_hike = y[mask_hike]\n",
    "        X_hike = X_full[mask_hike]\n",
    "        model_hike = sm.OLS(y_hike, X_hike).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Rate Hike Period Only',\n",
    "            'Period': 'Mar 2022 - Jul 2023',\n",
    "            'β (FFR)': f\"{model_hike.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_hike.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_hike.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_hike.rsquared:.3f}\",\n",
    "            'N': str(int(model_hike.nobs)),\n",
    "            'Key Characteristics': 'Fed raised 525bp; strongest tightening'\n",
    "        })\n",
    "    \n",
    "    # Post-2021 only (after BNPL boom)\n",
    "    post_boom = '2022-01-01'\n",
    "    mask_post = data.index >= post_boom\n",
    "    if mask_post.sum() >= 20:\n",
    "        y_post = y[mask_post]\n",
    "        X_post = X_full[mask_post]\n",
    "        model_post = sm.OLS(y_post, X_post).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Post-2021',\n",
    "            'Period': 'Jan 2022 - Aug 2025',\n",
    "            'β (FFR)': f\"{model_post.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_post.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_post.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_post.rsquared:.3f}\",\n",
    "            'N': str(int(model_post.nobs)),\n",
    "            'Key Characteristics': 'Excludes zero-rate period'\n",
    "        })\n",
    "\n",
    "    # High vs low volatility (split by median BNPL rolling vol)\n",
    "    vol_series = data['log_returns'].rolling(window=3, min_periods=1).std()\n",
    "    vol_median = vol_series.median()\n",
    "    mask_high_vol = vol_series > vol_median\n",
    "    mask_low_vol = vol_series <= vol_median\n",
    "\n",
    "    if mask_high_vol.sum() >= 15:\n",
    "        y_high = y[mask_high_vol]\n",
    "        X_high = X_full[mask_high_vol]\n",
    "        model_high = sm.OLS(y_high, X_high).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'High Volatility Months',\n",
    "            'Period': 'BNPL vol > median',\n",
    "            'β (FFR)': f\"{model_high.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_high.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_high.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_high.rsquared:.3f}\",\n",
    "            'N': str(int(model_high.nobs)),\n",
    "            'Key Characteristics': 'Market stress periods'\n",
    "        })\n",
    "\n",
    "    if mask_low_vol.sum() >= 15:\n",
    "        y_low = y[mask_low_vol]\n",
    "        X_low = X_full[mask_low_vol]\n",
    "        model_low = sm.OLS(y_low, X_low).fit(cov_type='HC3')\n",
    "        sensitivity_list.append({\n",
    "            'Sample': 'Low Volatility Months',\n",
    "            'Period': 'BNPL vol < median',\n",
    "            'β (FFR)': f\"{model_low.params['ffr_change']:.2f}\",\n",
    "            'SE': f\"{model_low.bse['ffr_change']:.2f}\",\n",
    "            'p-value': f\"{model_low.pvalues['ffr_change']:.3f}\",\n",
    "            'R²': f\"{model_low.rsquared:.3f}\",\n",
    "            'N': str(int(model_low.nobs)),\n",
    "            'Key Characteristics': 'Calm market periods'\n",
    "        })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in sensitivity analysis: {e}\")\n",
    "\n",
    "table_5 = pd.DataFrame(sensitivity_list)\n",
    "print(\"Table 5: Sensitivity Analysis - Different Time Windows\")\n",
    "print(\"=\" * 80)\n",
    "glue(\"table-5\", table_5, display=False)\n",
    "print(\"Notes: All subsamples use the full model controls (market, inflation, confidence, income). Coefficient stays negative across subperiods, larger during tightening and high-volatility months. Statistical precision varies with sample size and volatility.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "id": "6e6164b4",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Chart K (clean) saved as chart_k_rate_panels.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Chart K (clean): BNPL vs Market with Rate-Hike Shading and Beta-Adjusted Residual\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Recompute spans for rate-hike periods (ffr_change > 0, contiguous)\n",
    "rate_hike_mask = data['ffr_change'] > 0\n",
    "spans = []\n",
    "start = None\n",
    "for date, flag in rate_hike_mask.items():\n",
    "    if flag and start is None:\n",
    "        start = date\n",
    "    if not flag and start is not None:\n",
    "        spans.append((start, date))\n",
    "        start = None\n",
    "if start is not None:\n",
    "    spans.append((start, rate_hike_mask.index[-1]))\n",
    "\n",
    "beta_hat = model_full.params['market_return'] if 'model_full' in locals() else 0\n",
    "resid_beta = data['log_returns'] - beta_hat * data['market_return']\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "\n",
    "# Panel A: BNPL\n",
    "axes[0].plot(data.index, data['log_returns'], color='#1f77b4', linewidth=1.6, label='BNPL returns')\n",
    "axes[0].axhline(0, color='#6e6e6e', linestyle='--', linewidth=1.2, alpha=0.7)\n",
    "axes[0].set_ylabel('%', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Panel A: BNPL Monthly Log Returns (%)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Panel B: Market\n",
    "axes[1].plot(data.index, data['market_return'], color='#ff7f0e', linewidth=1.6, label='Market returns')\n",
    "axes[1].axhline(0, color='#6e6e6e', linestyle='--', linewidth=1.2, alpha=0.7)\n",
    "axes[1].set_ylabel('%', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Panel B: Market (SPY) Monthly Returns (%)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Panel C: Residual\n",
    "axes[2].plot(data.index, resid_beta, color='#2ca02c', linewidth=1.6, label='BNPL − β×Market')\n",
    "axes[2].axhline(0, color='#6e6e6e', linestyle='--', linewidth=1.2, alpha=0.7)\n",
    "axes[2].set_ylabel('%', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Panel C: BNPL minus β×Market (Residual) (%)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Shading (single color, no axis labels)\n",
    "shade_color = '#f1c27d'\n",
    "for ax in axes:\n",
    "    for s, e in spans:\n",
    "        ax.axvspan(s, e, color=shade_color, alpha=0.28, zorder=-2)\n",
    "    ax.grid(True, linestyle=':', color='#d0d4d7', alpha=0.35)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Single shared legend near the origin (bottom-left)\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [\n",
    "    Line2D([0], [0], color='#1f77b4', linewidth=1.6, label='BNPL returns'),\n",
    "    Line2D([0], [0], color='#ff7f0e', linewidth=1.6, label='Market returns'),\n",
    "    Line2D([0], [0], color='#2ca02c', linewidth=1.6, label='BNPL – β×Market'),\n",
    "    Patch(facecolor=shade_color, alpha=0.28, edgecolor='none', label='Rate-hike period')\n",
    "]\n",
    "axes[0].legend(\n",
    "    handles=legend_handles,\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(0.01, 0.99),\n",
    "    frameon=True,\n",
    "    framealpha=0.92,\n",
    "    edgecolor='#cfd4d7',\n",
    "    facecolor='white',\n",
    "    ncol=1,\n",
    "    fontsize=9\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_k_rate_panels.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    ✓ Chart K (clean) saved as chart_k_rate_panels.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63929b",
   "metadata": {},
   "source": [
    "### Chart K: BNPL vs Market with Rate-Hike Shading (clean)\n",
    "\n",
    "(chart-k)=\n",
    "<p align=\"center\">![Chart K: BNPL vs Market with Rate-Hike Shading](chart_k_rate_panels.png)</p>\n",
    "\n",
    "Chart K shows BNPL returns (Panel A), market returns (Panel B), and beta-adjusted BNPL residuals (Panel C) with lightly shaded rate-hike periods (single legend). Zero lines are thicker and legends simplified for readability. Co-movement with the market dominates; the residual panel stays muted even during hikes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "id": "99eb0abb",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Figure 3 saved as chart_c_time_series.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Figure 3: Observed vs Fitted Returns (Full Model) with R^2 and time coloring\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Ensure full model\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "fitted = model_full.fittedvalues\n",
    "observed = data['log_returns']\n",
    "r2_full = model_full.rsquared\n",
    "\n",
    "# Color by time: early (<= Feb 2022) vs late\n",
    "split_date = pd.Timestamp('2022-02-28')\n",
    "mask_early = data.index <= split_date\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.scatter(fitted[mask_early], observed[mask_early], s=38, alpha=0.62, color='#1f77b4', edgecolors='none', label='Early sample (to Feb 2022)')\n",
    "ax.scatter(fitted[~mask_early], observed[~mask_early], s=38, alpha=0.62, color='#ff7f0e', edgecolors='none', label='Late sample (post Feb 2022)')\n",
    "\n",
    "# 45-degree line\n",
    "line_min = min(fitted.min(), observed.min())\n",
    "line_max = max(fitted.max(), observed.max())\n",
    "ax.plot([line_min, line_max], [line_min, line_max], color='#2c3e50', linestyle='--', linewidth=1.2, alpha=0.8, label='45° line')\n",
    "\n",
    "ax.set_title(f'Observed vs Fitted Returns (Full Model, R² = {r2_full:.3f})', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Fitted BNPL Returns (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Observed BNPL Returns (%)', fontsize=13, fontweight='bold')\n",
    "ax.tick_params(axis='both', labelsize=11)\n",
    "ax.grid(True, linestyle=':', color='#d0d4d7', alpha=0.35)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(0.98, 0.02), fontsize=11, frameon=True, framealpha=0.9, edgecolor='#d0d4d7', facecolor='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_c_time_series.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    ✓ Figure 3 saved as chart_c_time_series.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "id": "ebee8326",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Figure 4 saved as chart_d_scatter.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Figure 4: Residual Analysis - FFR Changes (Full Model)\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "# Ensure model_full exists\n",
    "if 'model_full' not in locals() and 'model_full' not in globals():\n",
    "    X_full = sm.add_constant(data[['ffr_change', 'cc_change', 'di_change', 'cpi_change', 'market_return']])\n",
    "    y_full = data['log_returns']\n",
    "    model_full = sm.OLS(y_full, X_full).fit(cov_type='HC3')\n",
    "\n",
    "resid = model_full.resid\n",
    "ffr = data['ffr_change']\n",
    "\n",
    "mask = ~(np.isnan(ffr) | np.isnan(resid))\n",
    "x = ffr[mask].values\n",
    "y = resid[mask].values\n",
    "n = len(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax.scatter(x, y, s=36, color='#5dade2', alpha=0.6, edgecolors='none', label='Residuals')\n",
    "\n",
    "# LOESS smoother\n",
    "try:\n",
    "    sorted_idx = np.argsort(x)\n",
    "    smth = lowess(y[sorted_idx], x[sorted_idx], frac=0.6, it=0, return_sorted=True)\n",
    "    ax.plot(smth[:, 0], smth[:, 1], color='#8e44ad', linewidth=2.0, alpha=0.8, label='LOESS')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Zero line\n",
    "ax.axhline(0, color='#2c3e50', linestyle='--', linewidth=2.0, alpha=0.9, label='Zero line')\n",
    "\n",
    "ax.set_title('Figure 4: Residual Analysis - FFR Changes (Full Model)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Change in Federal Funds Rate (pp)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Residuals (Observed − Fitted, %)', fontsize=13, fontweight='bold')\n",
    "ax.tick_params(axis='both', labelsize=11)\n",
    "ax.grid(True, linestyle=':', color='#d7dce2', alpha=0.35)\n",
    "ax.legend(loc='upper left', fontsize=11, frameon=True, framealpha=0.9, edgecolor='#d7dce2', facecolor='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_d_scatter.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    ✓ Figure 4 saved as chart_d_scatter.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480e6f4",
   "metadata": {},
   "source": [
    "### Figure 3: Observed vs Fitted Returns (Full Model)\n",
    "\n",
    "Figure 3 plots observed BNPL returns against fitted values from the full specification (Table 4A, Column 2). Early-period points (blue) and late-period points (orange) cluster around the 45° line, yielding R² = 0.524. The tight cloud along the diagonal shows the full model captures most level variation. The biggest gaps appear in high-volatility months—COVID rebound and the start of hikes—where observed returns flare above fitted values in the 5–15% fitted range, underscoring how tail events drive residual dispersion. Outside those tails, fitted and observed move together, reinforcing that market and macro controls explain the bulk of BNPL return swings.\n",
    "\n",
    "### Figure 4: Residual Analysis – FFR Changes (Full Model)\n",
    "\n",
    "Figure 4 plots residuals versus monthly FFR changes with a LOESS smoother. Residuals sit around zero with no slope or curvature; the smoother hugs the zero line, indicating the linear rate term is adequate. Outliers are confined to a few rate-surge months, and the pattern is otherwise noise-like—consistent with weak rate significance and HC3-robust SEs.\n",
    "\n",
    "### Figure 5: Residuals vs Fitted Values (Full Model)\n",
    "\n",
    "Figure 5 checks homoskedasticity and linearity against fitted values. Residuals are symmetric with no funnel shape; variance stays roughly constant across the fitted range, and only the extreme positive fitted values show modest spread. This aligns with the Breusch–Pagan pass in Table 3 and supports the linear specification.\n",
    "\n",
    "### Figure 7: Q-Q Plot\n",
    "\n",
    "Figure 7 compares residual quantiles to the normal benchmark. Points track the diagonal with only slight tail softness; Jarque–Bera p = 0.429 (Table 3) indicates normality cannot be rejected. Inference based on t-stats is therefore reliable for the full model.\n",
    "\n",
    "### Figure 8: Explanatory Power Across Model Specifications\n",
    "\n",
    "Figure 8 contrasts R² across models. The base (FFR-only) model explains ~0.02, while the full, Fama–French, and IV models cluster near ~0.52. The DiD variant drops back to ~0.02. The jump from base to full shows market and macro controls drive explanatory power; rate-only adds almost nothing, and robustness across alternative specs keeps the story unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "id": "6f6d3bae",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Figure 8 saved as chart_h_r2_comparison.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Figure 8: Explanatory Power Across Model Specifications (R^2 bars)\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assemble R^2 values from models if present; fall back to tables if needed\n",
    "r2_map = {}\n",
    "n_map = {}\n",
    "\n",
    "def add_model(name, model_obj):\n",
    "    r2_map[name] = float(model_obj.rsquared)\n",
    "    n_map[name] = int(model_obj.nobs)\n",
    "\n",
    "if 'model_full' in globals() or 'model_full' in locals():\n",
    "    add_model('Full', model_full)\n",
    "if 'model_base' in globals() or 'model_base' in locals():\n",
    "    add_model('Base', model_base)\n",
    "if 'model_ff' in globals() or 'model_ff' in locals():\n",
    "    try:\n",
    "        add_model('Fama-French', model_ff)\n",
    "    except Exception:\n",
    "        pass\n",
    "if 'model_iv' in globals() or 'model_iv' in locals():\n",
    "    try:\n",
    "        add_model('IV', model_iv)\n",
    "    except Exception:\n",
    "        pass\n",
    "if 'model_did' in globals() or 'model_did' in locals():\n",
    "    try:\n",
    "        add_model('DiD', model_did)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# If missing, pull from tables\n",
    "\n",
    "def add_from_table(df):\n",
    "    for _, row in df.iterrows():\n",
    "        label = row['Model']\n",
    "        if label.startswith('1. Base'):\n",
    "            key = 'Base'\n",
    "        elif label.startswith('2. Full'):\n",
    "            key = 'Full'\n",
    "        elif label.startswith('3. Fama-French'):\n",
    "            key = 'Fama-French'\n",
    "        elif label.startswith('4. IV'):\n",
    "            key = 'IV'\n",
    "        elif label.startswith('5. DiD'):\n",
    "            key = 'DiD'\n",
    "        else:\n",
    "            continue\n",
    "        if key not in r2_map:\n",
    "            try:\n",
    "                r2_map[key] = float(row['R²'])\n",
    "                n_map[key] = int(row['N']) if 'N' in row else None\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "try:\n",
    "    add_from_table(table_4a)\n",
    "    add_from_table(table_4b)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "order = ['Full', 'Base', 'Fama-French', 'IV', 'DiD']\n",
    "labels = []\n",
    "r2_vals = []\n",
    "for key in order:\n",
    "    if key in r2_map:\n",
    "        labels.append(key)\n",
    "        r2_vals.append(r2_map[key])\n",
    "\n",
    "color_map = {\n",
    "    'Base': '#9ca3af',\n",
    "    'Full': '#2563eb',\n",
    "    'Fama-French': '#f59e0b',\n",
    "    'IV': '#8b5cf6',\n",
    "    'DiD': '#10b981'\n",
    "}\n",
    "colors = [color_map.get(lbl, '#95a5a6') for lbl in labels]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "bars = ax.bar(labels, r2_vals, color=colors, alpha=0.9, edgecolor='#34495e', linewidth=1.2, zorder=3)\n",
    "\n",
    "# Numeric labels placed just above bars (consistent for all heights)\n",
    "for bar, r2 in zip(bars, r2_vals):\n",
    "    y = bar.get_height()\n",
    "    label_y = y + 0.012  # uniform upward offset\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        label_y,\n",
    "        f\"{r2:.3f}\",\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontsize=11,\n",
    "        fontweight='semibold',\n",
    "        color='#111'\n",
    "    )\n",
    "\n",
    "# Reference line at 0.50 with lower opacity\n",
    "ax.axhline(0.50, color='#7f8c8d', linestyle='--', linewidth=1.2, alpha=0.6, zorder=1, label='R² = 0.50')\n",
    "\n",
    "ax.set_ylabel('R²', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Explanatory Power Across Model Specifications', fontsize=18, fontweight='bold', pad=12)\n",
    "\n",
    "ymax = max(r2_vals) if r2_vals else 0.6\n",
    "ax.set_ylim(0, max(0.65, ymax + 0.10))\n",
    "ax.tick_params(axis='both', labelsize=11, colors='#333')\n",
    "ax.grid(axis='y', linestyle=':', color='#d0d4d7', alpha=0.35, zorder=0)\n",
    "ax.legend(fontsize=12, frameon=True, framealpha=0.9, edgecolor='#d7dce2', facecolor='white', loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_h_r2_comparison.png', dpi=300, facecolor='white', bbox_inches='tight', pad_inches=0.2)\n",
    "plt.close()\n",
    "print('    ✓ Figure 8 saved as chart_h_r2_comparison.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "id": "eeebbb2d",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Figure 9 saved as chart_i_timeline.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Figure 9: Timeline – Rates, BNPL vs Market, and Idiosyncratic Residual\n",
    "# ============================================================================\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Build FFR level\n",
    "if 'ffr_monthly' in locals():\n",
    "    ffr_level = ffr_monthly.reindex(data.index, method='pad')\n",
    "else:\n",
    "    ffr_level = data['ffr_change'].cumsum()\n",
    "\n",
    "beta_mkt = model_full.params['market_return'] if 'model_full' in locals() else 0\n",
    "resid_beta = data['log_returns'] - beta_mkt * data['market_return']\n",
    "\n",
    "covid_start, covid_end = pd.Timestamp('2020-03-01'), pd.Timestamp('2020-06-30')\n",
    "zero_end = pd.Timestamp('2022-02-28')\n",
    "hike_start, hike_end = pd.Timestamp('2022-03-01'), pd.Timestamp('2023-07-31')\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Panel A: FFR level\n",
    "axes[0].plot(data.index, ffr_level, color='#2c3e50', linewidth=2.0, label='FFR')\n",
    "axes[0].set_ylabel('FFR (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Rates, BNPL vs Market, and Idiosyncratic Residual', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Panel B: BNPL and Market\n",
    "axes[1].plot(data.index, data['log_returns'], color='#1f77b4', linewidth=1.6, label='BNPL returns')\n",
    "axes[1].plot(data.index, data['market_return'], color='#ff7f0e', linewidth=1.6, label='Market returns')\n",
    "axes[1].set_ylabel('Returns (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='upper left', fontsize=11)\n",
    "\n",
    "# Panel C: Residual\n",
    "axes[2].plot(data.index, resid_beta, color='#2ca02c', linewidth=1.6, label='BNPL – β×Market')\n",
    "axes[2].axhline(0, color='#2c3e50', linestyle='--', linewidth=1.2, alpha=0.8)\n",
    "axes[2].set_ylabel('Residual (%)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "axes[2].legend(loc='upper left', fontsize=11)\n",
    "\n",
    "# Shading with legend patches\n",
    "shades = [\n",
    "    (data.index.min(), zero_end, '#7aa5d8', 0.28, 'Zero bound'),\n",
    "    (covid_start, covid_end, '#6f6f6f', 0.32, 'COVID shock'),\n",
    "    (hike_start, hike_end, '#f1c27d', 0.34, 'Rate hikes')\n",
    "]\n",
    "for ax in axes:\n",
    "    for s,e,c,a,_ in shades:\n",
    "        ax.axvspan(s, e, color=c, alpha=a, zorder=-2)\n",
    "    ax.grid(True, linestyle=':', color='#d0d4d7', alpha=0.35)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Shading legend (single shared)\n",
    "patches = [Patch(facecolor=c, alpha=a, edgecolor='none', label=lab) for _,_,c,a,lab in shades]\n",
    "axes[0].legend(handles=patches, loc='upper left', fontsize=11, frameon=True, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_i_timeline.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    ✓ Figure 9 saved as chart_i_timeline.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "id": "564b1552",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/17266475.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
      "[*********************100%***********************]  3 of 3 completed\n",
      "/var/folders/vk/b6wqznms0035nb3gx2sxcfqr0000gn/T/ipykernel_6783/17266475.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Figure 10 saved as chart_l_volatility.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# Figure 10: Volatility Comparison (BNPL vs peers)\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Helper to build equal-weight monthly log-return series (in %)\n",
    "def ew_monthly_log_returns(tickers, start_date, end_date):\n",
    "    px = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "    px = px.resample(\"ME\").last()\n",
    "    log_ret = np.log(px / px.shift(1)) * 100  # percent\n",
    "    return log_ret.mean(axis=1)\n",
    "\n",
    "start_dt = start_date\n",
    "end_dt = end_date\n",
    "\n",
    "credit_tickers = [\"AXP\", \"COF\", \"SYF\"]\n",
    "fintech_tickers = [\"SOFI\", \"UPST\", \"LC\"]\n",
    "\n",
    "credit_ret = ew_monthly_log_returns(credit_tickers, start_dt, end_dt)\n",
    "fintech_ret = ew_monthly_log_returns(fintech_tickers, start_dt, end_dt)\n",
    "\n",
    "# Align to main data index\n",
    "credit_ret = credit_ret.reindex(data.index).dropna()\n",
    "fintech_ret = fintech_ret.reindex(data.index).dropna()\n",
    "\n",
    "vol_data = {\n",
    "    'BNPL portfolio': data['log_returns'].std(),\n",
    "    'S&P 500 (SPY)': data['market_return'].std(),\n",
    "    'Credit cards (AXP, COF, SYF)': credit_ret.std(),\n",
    "    'FinTech lenders (SOFI, UPST, LC)': fintech_ret.std()\n",
    "}\n",
    "vol_df = pd.DataFrame(list(vol_data.items()), columns=['Series', 'Vol'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Unified palette across charts\n",
    "colors = [\n",
    "    '#1f77b4',  # BNPL\n",
    "    '#8c7aa9',  # SPY (Morandi purple)\n",
    "    '#2c9c9c',  # Credit cards\n",
    "    '#d97706'   # FinTech lenders\n",
    "]\n",
    "bars = ax.bar(vol_df['Series'], vol_df['Vol'], color=colors, alpha=0.9, edgecolor='#34495e')\n",
    "\n",
    "for idx, row in vol_df.iterrows():\n",
    "    ax.text(idx, row['Vol'] + 0.15, f\"{row['Vol']:.1f}%\", ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Monthly Volatility (Std Dev, %)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Volatility Comparison', fontsize=16, fontweight='bold')\n",
    "ax.grid(axis='y', linestyle=':', color='#d0d4d7', alpha=0.35)\n",
    "plt.xticks(rotation=20, ha='center', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_l_volatility.png', dpi=300, facecolor='white')\n",
    "plt.close()\n",
    "print('    ✓ Figure 10 saved as chart_l_volatility.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a0810",
   "metadata": {},
   "source": [
    "### Figure 9: Timeline of Rates, BNPL vs Market, and Idiosyncratic Residual\n",
    "\n",
    "Panel A plots the Federal Funds Rate level; Panel B overlays BNPL and market returns; Panel C shows BNPL returns net of beta-adjusted market exposure. Shading marks the COVID shock (gray), zero-bound period (blue), and rate hikes (red). BNPL closely tracks the market; the residual panel shows limited rate-linked structure.\n",
    "\n",
    "### Figure 10: Volatility Comparison\n",
    "\n",
    "![Figure 10: Volatility Comparison](chart_l_volatility.png)\n",
    "\n",
    "Figure 10 compares monthly volatility for BNPL, the S&P 500, credit card lenders (AXP, COF, SYF), and fintech lenders (SOFI, UPST, LC). BNPL remains the most volatile, followed by fintech lenders; credit card issuers and the broad market are much steadier. Volatility spreads are wide: BNPL volatility is roughly 2x that of credit cards and ~4x the market, while fintech lenders sit just below BNPL. This gap matters for interpretation: when monthly returns can swing 20–30% on headlines, earnings, or sentiment, detecting a 5–10% rate-induced move is statistically hard. It also means BNPL behaves like a high-beta, risk-on asset: sharp drawdowns in tightening cycles, rapid rebounds when risk appetite returns. For portfolio construction, BNPL exposure carries materially higher idiosyncratic and systematic risk than traditional card issuers, and any rate sensitivity is likely to be masked by this noise unless shocks are very large or persistent.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupyter": {
   "jupytext": {
    "cell_metadata_filter": "all,-hidden,-heading_collapsed,-run_control,-trusted",
    "notebook_metadata_filter": "all,-jupytext.text_representation.jupytext_version"
   }
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "mystnb": {
   "execution_mode": "cache",
   "render_markdown_format": "myst"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
