{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3f4c21",
   "metadata": {},
   "source": [
    "## 3. Data Analysis: Interest Rate Sensitivity and Economic Determinants\n",
    "\n",
    "This study examines factors associated with Buy Now, Pay Later (BNPL) stock returns, with particular attention to the sector's sensitivity to monetary policy changes and broader macroeconomic conditions. The study employs a log-linear regression framework to estimate relationships between BNPL stock returns and a set of economic variables, including interest rate changes, consumer confidence, disposable income, inflation, and market returns. We use log-transformed BNPL returns as the dependent variable to facilitate elasticity interpretation and address common issues in financial return data (see Appendix for detailed justification).\n",
    "\n",
    "The modeling strategy employs a two-stage approach, beginning with a parsimonious base specification that examines the relationship between BNPL returns and interest rate changes, followed by a full specification model that incorporates multiple economic channels simultaneously. This sequential estimation strategy enables us to assess both direct effects of monetary policy on BNPL stock performance and the incremental explanatory power of including additional control variables. All models are estimated using Ordinary Least Squares (OLS) regression with robust standard errors (HC3) to address heteroskedasticity, which is a common feature of financial return data.\n",
    "\n",
    "The motivation for this work stems from the rapid growth and increasing economic significance of the BNPL sector. According to the Consumer Financial Protection Bureau's 2025 report, BNPL adoption has experienced substantial growth, with 21% of consumers with credit records utilizing BNPL services in 2022, representing an increase from 18% in 2021. This expansion, combined with the sector's sensitivity to funding costs and capital market conditions, makes understanding the determinants of BNPL stock returns relevant for both investors seeking to assess risk exposure and policymakers concerned with financial stability and consumer protection. The sector's reliance on short-term funding markets and its sensitivity to consumer spending patterns indicate that BNPL stock returns respond systematically to changes in monetary policy, macroeconomic conditions, and broader market movements. The extent and nature of these relationships are the focus of this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae19c6b",
   "metadata": {},
   "source": [
    "## 3.1 Data Construction and Variable Selection\n",
    "\n",
    "This section describes the data construction process and the rationale for variable selection in our econometric models. The empirical analysis requires careful construction of a balanced panel dataset that aligns stock return data with macroeconomic variables measured at different frequencies and from different sources.\n",
    "\n",
    "### 3.1.1 BNPL Portfolio Construction\n",
    "\n",
    "**Firm Selection and Sample Limitations**\n",
    "\n",
    "Our analysis includes three publicly-traded BNPL firms: PayPal Holdings Inc. (PYPL), Affirm Holdings Inc. (AFRM), and Sezzle Inc. (SEZL). These firms were selected based on criteria established in the Consumer Financial Protection Bureau's Market Trends Report, which identifies major BNPL providers in the U.S. market.\n",
    "\n",
    "Several important limitations affect this firm selection. First, the sample excludes other major BNPL providers that are not publicly traded (e.g., Klarna, Afterpay prior to acquisition, Zip) or that went public after our sample period ends (e.g., Klarna IPO in September 2025). Second, the sample excludes firms with insufficient trading history or data availability issues (e.g., Block/Square's BNPL operations are not separately traded). Third, the sample may suffer from survivorship bias, as only firms that survived and went public are included.\n",
    "\n",
    "These limitations mean our results may not generalize to the broader BNPL sector, particularly smaller providers or those operating under different business models. However, the three firms included represent substantial market coverage: PayPal's BNPL product (Pay in 4) represents 68.1% of U.S. BNPL market share, making it the largest BNPL provider. Affirm and Sezzle are pure-play BNPL providers that went public in 2020-2021, providing representative coverage of the sector's business models.\n",
    "\n",
    "**Portfolio Weighting: Equal vs. Value Weighting**\n",
    "\n",
    "We construct the BNPL portfolio using equal weighting, where each firm receives equal weight regardless of market capitalization. This approach has both advantages and limitations.\n",
    "\n",
    "Equal weighting reduces the dominance of PayPal, which has substantially larger market capitalization than Affirm or Sezzle. This ensures that pure-play BNPL firms (Affirm, Sezzle) receive equal representation in the portfolio, capturing sector-wide patterns rather than being dominated by PayPal's diversified operations. Equal weighting also reduces the influence of market capitalization changes that may be unrelated to BNPL-specific factors.\n",
    "\n",
    "However, equal weighting creates a distorted representation of the sector's economic importance. PayPal's BNPL operations represent the majority of market share, yet receive only one-third weight in the portfolio. This may bias results if PayPal exhibits different sensitivity patterns than pure-play BNPL firms. Additionally, Sezzle's small market capitalization and limited liquidity may introduce noise into the portfolio return.\n",
    "\n",
    "As a robustness check, we examine specifications excluding PayPal (Section 4.6.5) and excluding Sezzle to assess sensitivity to portfolio construction choices. Alternative portfolio constructions\u2014value-weighted portfolios, principal component analysis, or firm-level panel regressions\u2014are discussed in robustness checks but not implemented due to sample size constraints.\n",
    "\n",
    "**Return Calculation and Log Transformation**\n",
    "\n",
    "For each individual BNPL company, we calculate monthly returns as:\n",
    "\n",
    "$$R_{i,t} = \\frac{P_{i,t} - P_{i,t-1}}{P_{i,t-1}} \\times 100$$\n",
    "\n",
    "where $P_{i,t}$ is the month-end closing price for firm $i$ in month $t$. The portfolio return is then calculated as the equally-weighted average:\n",
    "\n",
    "$$R_{t}^{BNPL} = \\frac{1}{N} \\sum_{i=1}^{N} R_{i,t}$$\n",
    "\n",
    "where $N = 3$ (PYPL, AFRM, SEZL).\n",
    "\n",
    "The dependent variable uses a log transformation: $\\log(1 + R_{t}^{BNPL}/100) \\times 100$. This transformation addresses distributional skewness, stabilizes variance, and facilitates elasticity interpretation of coefficients. Detailed justification is provided in the Appendix.\n",
    "\n",
    "### 3.1.2 Variable Definitions and Data Sources\n",
    "\n",
    "**Table 3.1: Variable Definitions**\n",
    "\n",
    "| Variable | Symbol | Definition | Source | Transformation |\n",
    "|----------|--------|------------|--------|----------------|\n",
    "| BNPL Returns | $R_{t}^{BNPL}$ | Log-transformed equally-weighted portfolio return | Yahoo Finance | $\\log(1 + \\text{avg return}/100) \\times 100$ |\n",
    "| Federal Funds Rate Change | $\\Delta FFR_t$ | Month-over-month change in FFR (percentage points) | FRED (FEDFUNDS) | First difference |\n",
    "| Consumer Confidence Change | $\\Delta CC_t$ | Month-over-month change in UM Consumer Sentiment Index | FRED (UMCSENT) | First difference |\n",
    "| Disposable Income Change | $\\Delta DI_t$ | Month-over-month percentage change in real disposable personal income | FRED (DSPIC96) | Percentage change |\n",
    "| Inflation Change | $\\Delta \\pi_t$ | Month-over-month percentage change in CPI (seasonally adjusted) | FRED (CPIAUCSLSA) | Percentage change |\n",
    "| Market Return | $R_{MKT,t}$ | Monthly S&P 500 return (percentage points) | Yahoo Finance (SPY) | Percentage change |\n",
    "\n",
    "**Interest Rate Variable: Federal Funds Rate Changes**\n",
    "\n",
    "We use month-over-month changes in the Federal Funds Rate ($\\Delta FFR_t$) rather than levels. This choice addresses several concerns. First, interest rate levels may be non-stationary, while changes are typically stationary (though ADF tests show mixed results, see Section 4.3.4). Second, changes capture policy shifts more directly than levels, which may reflect long-term trends unrelated to current policy. Third, changes align with the theoretical mechanism: BNPL firms respond to funding cost changes, not absolute rate levels.\n",
    "\n",
    "However, using monthly changes creates measurement challenges. The Federal Funds Rate changes infrequently (often remaining constant for multiple months), creating many zero observations. This low-frequency variation may create attenuation bias and reduce statistical power. Alternative specifications using 2-year Treasury yield changes (Section 4.6.1) address this concern by providing higher-frequency variation.\n",
    "\n",
    "**Lag Structure and Timing Considerations**\n",
    "\n",
    "Macroeconomic variables are measured contemporaneously with BNPL returns, creating potential simultaneity concerns. Macro data is typically released during the month (e.g., CPI released mid-month), while stock returns reflect information available throughout the month. This timing mismatch may bias estimates if macro data releases affect stock prices within the same month.\n",
    "\n",
    "Ideally, we would use lagged macro variables (e.g., $\\Delta CC_{t-1}$, $\\Delta DI_{t-1}$) to ensure that macro conditions are known before stock returns are realized. However, using contemporaneous variables captures the forward-looking nature of stock prices, which incorporate expectations about future macro conditions. As a robustness check, we examine specifications with lagged macro variables (discussed in Section 4.6), though results are not substantially different.\n",
    "\n",
    "**Market Returns**\n",
    "\n",
    "We use the S&P 500 exchange-traded fund (SPY) as a proxy for broad market returns. The S&P 500 represents approximately 80% of U.S. equity market capitalization and provides a comprehensive benchmark for systematic market risk. Monthly returns are calculated as percentage changes in month-end closing prices, ensuring temporal alignment with BNPL stock returns.\n",
    "\n",
    "**Consumer Confidence**\n",
    "\n",
    "We employ the University of Michigan Consumer Sentiment Index (UMCSENT) as a measure of forward-looking consumer spending intentions. This index captures consumers' expectations about future economic conditions and their own financial situation, which should directly affect BNPL usage as consumers make purchasing decisions. We calculate month-over-month changes to capture shifts in consumer sentiment that may affect BNPL transaction volume.\n",
    "\n",
    "**Disposable Income**\n",
    "\n",
    "We use real disposable personal income (DSPIC96) from FRED, which measures inflation-adjusted personal income after taxes. This variable captures the income channel through which economic conditions affect consumer purchasing power and BNPL usage. We calculate percentage changes (month-over-month) to measure growth in disposable income, which is more economically meaningful than levels for analyzing the relationship with stock returns.\n",
    "\n",
    "**Inflation**\n",
    "\n",
    "We employ the Consumer Price Index for All Urban Consumers, Seasonally Adjusted (CPIAUCSLSA) as a measure of inflation. We use the seasonally adjusted series to remove predictable seasonal patterns (such as holiday shopping effects) that could confound our analysis. Seasonal adjustment is important for CPI because consumer prices can exhibit regular seasonal fluctuations that are unrelated to underlying inflation trends. We calculate month-over-month percentage changes to capture inflation shocks that may affect consumer purchasing power and spending patterns.\n",
    "\n",
    "**Seasonal Adjustment**\n",
    "\n",
    "We use seasonally adjusted data where available to remove predictable seasonal patterns that could confound our analysis. Real disposable personal income (DSPIC96) is obtained from FRED in seasonally adjusted form by default. Consumer Price Index (CPIAUCSLSA) is obtained as the seasonally adjusted series to remove seasonal patterns in consumer prices. Consumer sentiment (UMCSENT) and Federal Funds Rate (FEDFUNDS) do not require seasonal adjustment, as consumer sentiment is a survey-based index and interest rates do not exhibit predictable seasonal patterns. Stock returns are already in first-difference form (monthly changes) and do not require seasonal adjustment.\n",
    "\n",
    "**Data Alignment and Temporal Coverage**\n",
    "\n",
    "All variables are aligned to monthly frequency and synchronized to month-end dates to ensure temporal consistency. The sample period spans from February 2020 to August 2025, providing 67 monthly observations. This period encompasses several important macroeconomic events, including the COVID-19 pandemic, monetary policy tightening in 2022-2023, and subsequent policy normalization, providing substantial variation in both dependent and independent variables necessary for econometric inference.\n",
    "\n",
    "### 3.1.3 Interest Rate Variable Selection: Theoretical and Empirical Considerations\n",
    "\n",
    "The selection of an appropriate interest rate variable requires balancing theoretical relevance with empirical considerations. While multiple interest rate measures could potentially capture BNPL firms' funding costs, we focus on the Federal Funds Rate for several reasons. First, BNPL firms rely heavily on short-term funding markets, including warehouse credit facilities, securitization markets, and commercial paper markets, all of which are directly influenced by the Federal Funds Rate. Second, the Federal Funds Rate serves as the primary monetary policy instrument, making it the most policy-relevant measure for understanding how monetary policy affects BNPL stock returns. Third, data availability and reliability favor the Federal Funds Rate, which is published daily by the Federal Reserve and has a long historical record.\n",
    "\n",
    "Alternative interest rate measures, such as commercial paper rates or credit spreads, could theoretically provide more direct measures of BNPL firms' actual funding costs. However, these alternatives face data availability constraints and are highly correlated with the Federal Funds Rate, making the incremental benefit of using alternative measures limited. The Federal Funds Rate provides a clean, policy-relevant measure that captures the primary channel through which monetary policy affects BNPL firms' cost of capital.\n",
    "\n",
    "### 3.1.4 Model Specification: Theoretical Framework\n",
    "\n",
    "The econometric models we estimate are motivated by theoretical considerations regarding the determinants of equity returns in general and BNPL stock returns in particular. The base model focuses on interest rate sensitivity, motivated by the sector's reliance on short-term funding markets documented by the CFPB (2025). The full specification model extends this framework by incorporating additional economic channels that theory suggests should affect BNPL stock performance: consumer spending patterns (captured by consumer confidence and disposable income), purchasing power effects (captured by inflation), and systematic market risk (captured by market returns).\n",
    "\n",
    "**Base Model Specification:**\n",
    "\n",
    "$$\\log(1 + BNPL\\_Return_t/100) = \\beta_0 + \\beta_1(\\Delta Federal\\_Funds\\_Rate_t) + \\varepsilon_t$$\n",
    "\n",
    "where BNPL_Return_t is the monthly return in percentage terms. The transformation log(1 + BNPL_Return_t/100) addresses distributional skewness, truncation at -100%, and approximates continuously compounded returns. This specification tests the hypothesis that BNPL stock returns are associated with changes in short-term interest rates, which would be expected given BNPL firms' reliance on funding markets. The coefficient $\\beta_1$ measures the elasticity of BNPL returns with respect to Federal Funds Rate changes, with a negative coefficient expected if higher interest rates increase funding costs and reduce profitability.\n",
    "\n",
    "**Full Specification Model:**\n",
    "\n",
    "$$\\log(1 + BNPL\\_Return_t/100) = \\beta_0 + \\beta_1(\\Delta Federal\\_Funds\\_Rate_t) + \\beta_2(\\Delta Consumer\\_Confidence_t) + \\beta_3(\\Delta Disposable\\_Income_t) + \\beta_4(\\Delta Inflation_t) + \\beta_5(Market\\_Return_t) + \\varepsilon_t$$\n",
    "\n",
    "This specification extends the base model by incorporating control variables that capture additional economic channels affecting BNPL stock returns. The inclusion of these variables serves multiple purposes: (1) controlling for factors that may be correlated with interest rates, providing a more accurate estimate of the direct interest rate effect; (2) capturing additional economic mechanisms that theory suggests should affect BNPL performance; and (3) improving model fit and reducing omitted variable bias.\n",
    "\n",
    "The theoretical justification for each control variable stems from understanding how BNPL firms generate revenue and face costs. Consumer confidence affects forward-looking spending intentions, directly influencing BNPL transaction volume. Disposable income affects consumers' ability to make purchases and use BNPL services. Inflation affects purchasing power and may influence consumer spending patterns. Market returns capture systematic market risk, isolating BNPL-specific effects from general market movements. Together, these variables provide a comprehensive framework for understanding the multiple economic channels affecting BNPL stock performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a7902",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0a9f371b",
   "metadata": {
    "tags": [
     "remove-input",
     "hide-input",
     "hide-output",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIRM-LEVEL FINANCIAL HEALTH ANALYSIS: PayPal and Affirm\n",
    "# ============================================================================\n",
    "# Analysis Period: 2020-2025\n",
    "# Focus: Financial health trends, profitability, cash flow, operational metrics\n",
    "# ============================================================================\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# print(\"Firms: PayPal Holdings Inc. (PYPL) and Affirm Holdings Inc. (AFRM)\")\n",
    "# print(\"Analysis Period: 2020-2025\")\n",
    "\n",
    "# Define tickers\n",
    "tickers = ['PYPL', 'AFRM']\n",
    "firm_names = {'PYPL': 'PayPal Holdings Inc.', 'AFRM': 'Affirm Holdings Inc.'}\n",
    "\n",
    "# Initialize dictionaries to store financial data\n",
    "financial_data = {}\n",
    "\n",
    "# print(\"\\nFetching financial statements from Yahoo Finance...\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get financial statements (annual data)\n",
    "        income_stmt = stock.financials\n",
    "        balance_sht = stock.balance_sheet\n",
    "        cash_flow = stock.cashflow\n",
    "        \n",
    "        # Store data\n",
    "        financial_data[ticker] = {\n",
    "            'income_annual': income_stmt,\n",
    "            'balance_annual': balance_sht,\n",
    "            'cashflow_annual': cash_flow\n",
    "        }\n",
    "        \n",
    "#         print(f\"    \u2713 Successfully loaded financial data\")\n",
    "        if income_stmt is not None and not income_stmt.empty:\n",
    "            pass\n",
    "#             print(f\"    \u2713 Income statement: {income_stmt.shape[1]} periods available\")\n",
    "            pass\n",
    "        if balance_sht is not None and not balance_sht.empty:\n",
    "            pass\n",
    "#             print(f\"    \u2713 Balance sheet: {balance_sht.shape[1]} periods available\")\n",
    "            pass\n",
    "        if cash_flow is not None and not cash_flow.empty:\n",
    "            pass\n",
    "#             print(f\"    \u2713 Cash flow statement: {cash_flow.shape[1]} periods available\")\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        pass\n",
    "#         print(f\"    \u2717 Error loading {ticker}: {str(e)[:100]}\")\n",
    "        financial_data[ticker] = None\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"DATA LOADING COMPLETE\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT AND ANALYZE KEY FINANCIAL METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_financial_metrics(ticker, financial_data_dict):\n",
    "    \"\"\"Extract key financial metrics from financial statements\"\"\"\n",
    "    if ticker not in financial_data_dict or financial_data_dict[ticker] is None:\n",
    "        return None\n",
    "    \n",
    "    data = financial_data_dict[ticker]\n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # Income statement metrics (annual)\n",
    "        income = data['income_annual']\n",
    "        if income is not None and not income.empty:\n",
    "            # Get dates (columns are dates, most recent first)\n",
    "            dates = pd.to_datetime(income.columns)\n",
    "            \n",
    "            # Extract key line items - try multiple possible names\n",
    "            revenue = None\n",
    "            for rev_name in ['Total Revenue', 'Revenue', 'Net Revenue', 'Operating Revenue']:\n",
    "                if rev_name in income.index:\n",
    "                    revenue = income.loc[rev_name]\n",
    "                    break\n",
    "            \n",
    "            operating_income = None\n",
    "            for op_name in ['Operating Income', 'Operating Profit', 'Income From Operations']:\n",
    "                if op_name in income.index:\n",
    "                    operating_income = income.loc[op_name]\n",
    "                    break\n",
    "            \n",
    "            net_income = None\n",
    "            for ni_name in ['Net Income', 'Net Income Common Stockholders', 'Net Income From Continuing Operations']:\n",
    "                if ni_name in income.index:\n",
    "                    net_income = income.loc[ni_name]\n",
    "                    break\n",
    "            \n",
    "            metrics['revenue'] = revenue\n",
    "            metrics['operating_income'] = operating_income\n",
    "            metrics['net_income'] = net_income\n",
    "            \n",
    "            # Extract credit loss metrics from income statement\n",
    "            credit_loss_expense = None\n",
    "            for cl_name in ['Provision For Credit Losses', 'Provision for Credit Losses',\n",
    "                             'Credit Loss Expense', 'Credit Losses', 'Bad Debt Expense',\n",
    "                             'Allowance For Credit Losses', 'Provision For Loan Losses']:\n",
    "                if cl_name in income.index:\n",
    "                    credit_loss_expense = income.loc[cl_name]\n",
    "                    break\n",
    "            \n",
    "            metrics['credit_loss_expense'] = credit_loss_expense\n",
    "            metrics['dates'] = dates\n",
    "            \n",
    "            # Calculate margins\n",
    "            if revenue is not None:\n",
    "                if operating_income is not None:\n",
    "                    metrics['operating_margin'] = (operating_income / revenue) * 100\n",
    "                if net_income is not None:\n",
    "                    metrics['net_margin'] = (net_income / revenue) * 100\n",
    "        \n",
    "        # Balance sheet metrics\n",
    "        balance = data['balance_annual']\n",
    "        if balance is not None and not balance.empty:\n",
    "            balance_dates = pd.to_datetime(balance.columns)\n",
    "            metrics['balance_dates'] = balance_dates\n",
    "            \n",
    "            total_assets = None\n",
    "            for ta_name in ['Total Assets', 'Assets']:\n",
    "                if ta_name in balance.index:\n",
    "                    total_assets = balance.loc[ta_name]\n",
    "                    break\n",
    "            \n",
    "            total_liabilities = None\n",
    "            for tl_name in ['Total Liabilities Net Minority Interest', 'Total Liabilities', 'Liabilities']:\n",
    "                if tl_name in balance.index:\n",
    "                    total_liabilities = balance.loc[tl_name]\n",
    "                    break\n",
    "            \n",
    "            total_equity = None\n",
    "            for te_name in ['Stockholders Equity', 'Total Stockholders Equity', 'Total Equity']:\n",
    "                if te_name in balance.index:\n",
    "                    total_equity = balance.loc[te_name]\n",
    "                    break\n",
    "            \n",
    "            current_assets = None\n",
    "            for ca_name in ['Current Assets', 'Total Current Assets']:\n",
    "                if ca_name in balance.index:\n",
    "                    current_assets = balance.loc[ca_name]\n",
    "                    break\n",
    "            \n",
    "            current_liabilities = None\n",
    "            for cl_name in ['Current Liabilities', 'Total Current Liabilities']:\n",
    "                if cl_name in balance.index:\n",
    "                    current_liabilities = balance.loc[cl_name]\n",
    "                    break\n",
    "            \n",
    "            metrics['total_assets'] = total_assets\n",
    "            metrics['total_liabilities'] = total_liabilities\n",
    "            metrics['total_equity'] = total_equity\n",
    "            metrics['current_assets'] = current_assets\n",
    "            metrics['current_liabilities'] = current_liabilities\n",
    "            \n",
    "            # Extract loans receivable (for credit loss rate calculation)\n",
    "            loans_receivable = None\n",
    "            for lr_name in ['Loans Receivable', 'Total Loans Receivable', 'Consumer Loans',\n",
    "                            'Loans Held For Investment', 'Finance Receivables',\n",
    "                            'Loans And Leases Receivable']:\n",
    "                if lr_name in balance.index:\n",
    "                    loans_receivable = balance.loc[lr_name]\n",
    "                    break\n",
    "            \n",
    "            # Extract allowance for credit losses\n",
    "            allowance_credit_losses = None\n",
    "            for acl_name in ['Allowance For Credit Losses', 'Allowance for Credit Losses',\n",
    "                              'Allowance For Loan Losses', 'Allowance For Doubtful Accounts',\n",
    "                              'Reserve For Credit Losses']:\n",
    "                if acl_name in balance.index:\n",
    "                    allowance_credit_losses = balance.loc[acl_name]\n",
    "                    break\n",
    "            \n",
    "            metrics['loans_receivable'] = loans_receivable\n",
    "            metrics['allowance_credit_losses'] = allowance_credit_losses\n",
    "            \n",
    "            # Calculate credit loss rates\n",
    "            if credit_loss_expense is not None and loans_receivable is not None:\n",
    "                loans_abs = loans_receivable.abs()\n",
    "                if loans_abs.min() > 0:\n",
    "                    metrics['credit_loss_rate'] = (credit_loss_expense / loans_receivable) * 100\n",
    "            \n",
    "            # Calculate allowance coverage ratio\n",
    "            if allowance_credit_losses is not None and loans_receivable is not None:\n",
    "                loans_abs = loans_receivable.abs()\n",
    "                if loans_abs.min() > 0:\n",
    "                    metrics['allowance_coverage_ratio'] = (allowance_credit_losses / loans_receivable) * 100\n",
    "            \n",
    "            # Calculate ratios\n",
    "            if total_equity is not None and total_liabilities is not None:\n",
    "                equity_abs = total_equity.abs()\n",
    "                if equity_abs.min() > 0:\n",
    "                    metrics['debt_to_equity'] = total_liabilities / total_equity\n",
    "            if current_assets is not None and current_liabilities is not None:\n",
    "                liabilities_abs = current_liabilities.abs()\n",
    "                if liabilities_abs.min() > 0:\n",
    "                    metrics['current_ratio'] = current_assets / current_liabilities\n",
    "        \n",
    "        # Cash flow metrics\n",
    "        cashflow = data['cashflow_annual']\n",
    "        if cashflow is not None and not cashflow.empty:\n",
    "            cashflow_dates = pd.to_datetime(cashflow.columns)\n",
    "            metrics['cashflow_dates'] = cashflow_dates\n",
    "            operating_cf = None\n",
    "            for oc_name in ['Operating Cash Flow', 'Cash Flow From Continuing Operating Activities', \n",
    "                           'Total Cash From Operating Activities']:\n",
    "                if oc_name in cashflow.index:\n",
    "                    operating_cf = cashflow.loc[oc_name]\n",
    "                    break\n",
    "            \n",
    "            free_cash_flow = None\n",
    "            for fcf_name in ['Free Cash Flow', 'Capital Expenditures']:\n",
    "                if fcf_name in cashflow.index:\n",
    "                    if fcf_name == 'Free Cash Flow':\n",
    "                        free_cash_flow = cashflow.loc[fcf_name]\n",
    "                    else:\n",
    "                        # Calculate FCF as Operating CF - CapEx\n",
    "                        if operating_cf is not None:\n",
    "                            free_cash_flow = operating_cf - cashflow.loc[fcf_name]\n",
    "                    break\n",
    "            \n",
    "            metrics['operating_cashflow'] = operating_cf\n",
    "            metrics['free_cashflow'] = free_cash_flow\n",
    "            \n",
    "            # Calculate FCF margin if revenue available\n",
    "            if revenue is not None and free_cash_flow is not None:\n",
    "                metrics['fcf_margin'] = (free_cash_flow / revenue) * 100\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "#         print(f\"    Warning: Error extracting metrics for {ticker}: {str(e)[:100]}\")\n",
    "        pass\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Extract metrics for both firms\n",
    "# print(\"\\nExtracting key financial metrics...\")\n",
    "pypl_metrics = extract_financial_metrics('PYPL', financial_data)\n",
    "afrm_metrics = extract_financial_metrics('AFRM', financial_data)\n",
    "\n",
    "# Create comparison DataFrames\n",
    "def create_trend_df(metrics, firm_name):\n",
    "    \"\"\"Create a DataFrame with year-over-year trends\"\"\"\n",
    "    if metrics is None:\n",
    "        return None\n",
    "    \n",
    "    df_data = {}\n",
    "    dates = metrics.get('dates', [])\n",
    "    \n",
    "    if len(dates) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Convert dates to years for easier comparison\n",
    "    years = [d.year for d in dates]\n",
    "    \n",
    "    # Create a base index from dates\n",
    "    base_index = pd.DatetimeIndex(dates)\n",
    "    \n",
    "    # Helper function to align a series to base_index\n",
    "    def align_to_base(series, series_dates):\n",
    "        if series is None or series_dates is None or len(series_dates) == 0:\n",
    "            return None\n",
    "        try:\n",
    "            if hasattr(series, 'values'):\n",
    "                series_values = series.values\n",
    "            else:\n",
    "                series_values = series\n",
    "            series_dt = pd.DatetimeIndex(series_dates)\n",
    "            aligned_series = pd.Series(series_values, index=series_dt)\n",
    "            aligned_series = aligned_series.reindex(base_index)\n",
    "            return aligned_series.values\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    # Revenue (in billions)\n",
    "    if metrics.get('revenue') is not None:\n",
    "        aligned_values = align_to_base(metrics['revenue'], dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Revenue ($B)'] = aligned_values / 1e9\n",
    "    \n",
    "    # Net Income (in billions)\n",
    "    if metrics.get('net_income') is not None:\n",
    "        aligned_values = align_to_base(metrics['net_income'], dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Net Income ($B)'] = aligned_values / 1e9\n",
    "    \n",
    "    # Operating Margin (%)\n",
    "    if metrics.get('operating_margin') is not None:\n",
    "        aligned_values = align_to_base(metrics['operating_margin'], dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Operating Margin (%)'] = aligned_values\n",
    "    \n",
    "    # Net Margin (%)\n",
    "    if metrics.get('net_margin') is not None:\n",
    "        aligned_values = align_to_base(metrics['net_margin'], dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Net Margin (%)'] = aligned_values\n",
    "    \n",
    "    # Debt-to-Equity\n",
    "    if metrics.get('debt_to_equity') is not None:\n",
    "        balance_dates = dates\n",
    "        aligned_values = align_to_base(metrics['debt_to_equity'], balance_dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Debt-to-Equity'] = aligned_values\n",
    "    \n",
    "    # Current Ratio\n",
    "    if metrics.get('current_ratio') is not None:\n",
    "        balance_dates = dates\n",
    "        aligned_values = align_to_base(metrics['current_ratio'], balance_dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Current Ratio'] = aligned_values\n",
    "    \n",
    "    # Free Cash Flow (in billions)\n",
    "    if metrics.get('free_cashflow') is not None:\n",
    "        cashflow_dates = dates\n",
    "        aligned_values = align_to_base(metrics['free_cashflow'], cashflow_dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Free Cash Flow ($B)'] = aligned_values / 1e9\n",
    "    \n",
    "    # FCF Margin (%)\n",
    "    if metrics.get('fcf_margin') is not None:\n",
    "        cashflow_dates = dates\n",
    "        aligned_values = align_to_base(metrics['fcf_margin'], cashflow_dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['FCF Margin (%)'] = aligned_values\n",
    "    \n",
    "    # Credit Loss Expense (in billions)\n",
    "    if metrics.get('credit_loss_expense') is not None:\n",
    "        aligned_values = align_to_base(metrics['credit_loss_expense'], dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Credit Loss Expense ($B)'] = aligned_values / 1e9\n",
    "    \n",
    "    # Loans Receivable (in billions)\n",
    "    if metrics.get('loans_receivable') is not None:\n",
    "        balance_dates = dates\n",
    "        aligned_values = align_to_base(metrics['loans_receivable'], balance_dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Loans Receivable ($B)'] = aligned_values / 1e9\n",
    "    \n",
    "    # Credit Loss Rate (%)\n",
    "    if metrics.get('credit_loss_rate') is not None:\n",
    "        aligned_values = align_to_base(metrics['credit_loss_rate'], dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Credit Loss Rate (%)'] = aligned_values\n",
    "    \n",
    "    # Allowance Coverage Ratio (%)\n",
    "    if metrics.get('allowance_coverage_ratio') is not None:\n",
    "        balance_dates = dates\n",
    "        aligned_values = align_to_base(metrics['allowance_coverage_ratio'], balance_dates)\n",
    "        if aligned_values is not None and len(aligned_values) == len(years):\n",
    "            df_data['Allowance Coverage (%)'] = aligned_values\n",
    "    \n",
    "    df = pd.DataFrame(df_data, index=years)\n",
    "    df.index.name = 'Year'\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    return df\n",
    "\n",
    "pypl_trends = create_trend_df(pypl_metrics, 'PayPal')\n",
    "afrm_trends = create_trend_df(afrm_metrics, 'Affirm')\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"FINANCIAL METRICS EXTRACTION COMPLETE\")\n",
    "\n",
    "if pypl_trends is not None:\n",
    "    pass\n",
    "#     print(pypl_trends.to_string())\n",
    "    pass\n",
    "    \n",
    "if afrm_trends is not None:\n",
    "    pass\n",
    "#     print(afrm_trends.to_string())\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6db653bf",
   "metadata": {
    "tags": [
     "remove-input",
     "hide-input",
     "hide-output",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RAW DATA SUMMARY - PAYPAL AND AFFIRM (2020-2025)\n",
    "# ============================================================================\n",
    "# This cell prints all raw financial metrics for PayPal and Affirm\n",
    "# showing all years 2020-2025. Missing years will show \"N/A\"\n",
    "\n",
    "all_years = [2020, 2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "# ============================================================================\n",
    "# PAYPAL RAW DATA (2020-2025)\n",
    "# ============================================================================\n",
    "if pypl_trends is not None:\n",
    "    pass\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "    pass\n",
    "    \n",
    "    if 'Revenue ($B)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nRevenue ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Revenue ($B)']):\n",
    "                val = pypl_trends.loc[year, 'Revenue ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Net Income ($B)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nNet Income ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Net Income ($B)']):\n",
    "                val = pypl_trends.loc[year, 'Net Income ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    if 'Operating Margin (%)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nOperating Margin (%):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Operating Margin (%)']):\n",
    "                val = pypl_trends.loc[year, 'Operating Margin (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    if 'Net Margin (%)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nNet Margin (%):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Net Margin (%)']):\n",
    "                val = pypl_trends.loc[year, 'Net Margin (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    if 'Free Cash Flow ($B)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nFree Cash Flow ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Free Cash Flow ($B)']):\n",
    "                val = pypl_trends.loc[year, 'Free Cash Flow ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'FCF Margin (%)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nFCF Margin (%):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'FCF Margin (%)']):\n",
    "                val = pypl_trends.loc[year, 'FCF Margin (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Debt-to-Equity' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nDebt-to-Equity Ratio:\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Debt-to-Equity']):\n",
    "                val = pypl_trends.loc[year, 'Debt-to-Equity']\n",
    "#                 print(f\"  {year}: {val:.2f}\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Current Ratio' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nCurrent Ratio (Liquidity):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Current Ratio']):\n",
    "                val = pypl_trends.loc[year, 'Current Ratio']\n",
    "#                 print(f\"  {year}: {val:.2f}\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Credit Loss Expense ($B)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nCredit Loss Expense ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Credit Loss Expense ($B)']):\n",
    "                val = pypl_trends.loc[year, 'Credit Loss Expense ($B)']\n",
    "#                 print(f\"  {year}: ${val:.3f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Loans Receivable ($B)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nLoans Receivable ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Loans Receivable ($B)']):\n",
    "                val = pypl_trends.loc[year, 'Loans Receivable ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Credit Loss Rate (%)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nCredit Loss Rate (% of Loans):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Credit Loss Rate (%)']):\n",
    "                val = pypl_trends.loc[year, 'Credit Loss Rate (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Allowance Coverage (%)' in pypl_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nAllowance Coverage Ratio (%):\")\n",
    "        for year in all_years:\n",
    "            if year in pypl_trends.index and pd.notna(pypl_trends.loc[year, 'Allowance Coverage (%)']):\n",
    "                val = pypl_trends.loc[year, 'Allowance Coverage (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "else:\n",
    "    pass\n",
    "#     print(\"\\n\u26a0 PayPal financial data not available\")\n",
    "    pass\n",
    "\n",
    "# ============================================================================\n",
    "# AFFIRM RAW DATA (2020-2025)\n",
    "# ============================================================================\n",
    "if afrm_trends is not None:\n",
    "    pass\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    if 'Revenue ($B)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nRevenue ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Revenue ($B)']):\n",
    "                val = afrm_trends.loc[year, 'Revenue ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Net Income ($B)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nNet Income ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Net Income ($B)']):\n",
    "                val = afrm_trends.loc[year, 'Net Income ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Operating Margin (%)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nOperating Margin (%):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Operating Margin (%)']):\n",
    "                val = afrm_trends.loc[year, 'Operating Margin (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Net Margin (%)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nNet Margin (%):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Net Margin (%)']):\n",
    "                val = afrm_trends.loc[year, 'Net Margin (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Free Cash Flow ($B)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nFree Cash Flow ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Free Cash Flow ($B)']):\n",
    "                val = afrm_trends.loc[year, 'Free Cash Flow ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'FCF Margin (%)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nFCF Margin (%):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'FCF Margin (%)']):\n",
    "                val = afrm_trends.loc[year, 'FCF Margin (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Debt-to-Equity' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nDebt-to-Equity Ratio:\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Debt-to-Equity']):\n",
    "                val = afrm_trends.loc[year, 'Debt-to-Equity']\n",
    "#                 print(f\"  {year}: {val:.2f}\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Current Ratio' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nCurrent Ratio (Liquidity):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Current Ratio']):\n",
    "                val = afrm_trends.loc[year, 'Current Ratio']\n",
    "#                 print(f\"  {year}: {val:.2f}\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Credit Loss Expense ($B)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nCredit Loss Expense ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Credit Loss Expense ($B)']):\n",
    "                val = afrm_trends.loc[year, 'Credit Loss Expense ($B)']\n",
    "#                 print(f\"  {year}: ${val:.3f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Loans Receivable ($B)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nLoans Receivable ($ billions):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Loans Receivable ($B)']):\n",
    "                val = afrm_trends.loc[year, 'Loans Receivable ($B)']\n",
    "#                 print(f\"  {year}: ${val:.2f} billion\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Credit Loss Rate (%)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nCredit Loss Rate (% of Loans):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Credit Loss Rate (%)']):\n",
    "                val = afrm_trends.loc[year, 'Credit Loss Rate (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "    \n",
    "    if 'Allowance Coverage (%)' in afrm_trends.columns:\n",
    "        pass\n",
    "#         print(\"\\nAllowance Coverage Ratio (%):\")\n",
    "        for year in all_years:\n",
    "            if year in afrm_trends.index and pd.notna(afrm_trends.loc[year, 'Allowance Coverage (%)']):\n",
    "                val = afrm_trends.loc[year, 'Allowance Coverage (%)']\n",
    "#                 print(f\"  {year}: {val:.2f}%\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  {year}: N/A\")\n",
    "                pass\n",
    "else:\n",
    "    pass\n",
    "#     print(\"\\n\u26a0 Affirm financial data not available\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb234a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "control_variable_selection_interpretation",
   "metadata": {},
   "source": [
    "## 3.2 Visual Analysis: Exploratory Data Analysis and Preliminary Patterns\n",
    "\n",
    "This section presents visualizations that provide preliminary insights into the data before formal econometric analysis. These graphical representations serve multiple purposes: they help identify patterns in the data, reveal potential outliers or data quality issues, provide intuition for the relationships we estimate econometrically, and offer visual confirmation of our regression results. The visualizations complement the formal econometric analysis by making the data accessible and providing context for interpreting regression coefficients.\n",
    "### 3.2.1 Chart A: Time Series of Log BNPL Returns\n",
    "\n",
    "![Log BNPL Returns Over Time](bnpl_returns_time_series.png)\n",
    "\n",
    "Chart A displays the time series of log-transformed BNPL stock returns from February 2020 to August 2025, providing a visual representation of the dependent variable in our regression models. The log transformation, calculated as log(1 + return/100), is applied for several methodological reasons that we discuss in detail below, but the visual representation helps us understand the temporal patterns in BNPL stock performance before we begin formal econometric analysis.\n",
    "\n",
    "**Why Log Transformation?** Before discussing the patterns visible in the chart, it is worth explaining why we transform returns using the natural logarithm. Financial return data commonly exhibit heteroskedasticity, where the variance of returns changes over time\u2014typically higher during volatile periods and lower during calm periods. Log transformations help stabilize this variance structure, making the data more suitable for regression analysis. Additionally, equity returns often exhibit right-skewed distributions due to the presence of extreme positive returns, and log transformations help normalize these distributions, improving the validity of statistical inference. Finally, the log-linear specification facilitates elasticity interpretation: regression coefficients can be interpreted as percentage changes in returns per unit change in independent variables, providing intuitive economic meaning.\n",
    "\n",
    "**Temporal Patterns:** The time series reveals substantial volatility in BNPL stock returns throughout the sample period, with notable episodes of both positive and negative performance. This volatility is not random but corresponds to distinct macroeconomic and sector-specific events that inform our understanding of BNPL stock performance. The onset of the COVID-19 pandemic in early 2020 coincided with significant negative returns, reflecting initial market uncertainty regarding BNPL firms' ability to weather economic disruption. Investors were concerned about potential deterioration in consumer credit quality, reduced consumer spending, and the sector's ability to maintain transaction volume during an economic downturn.\n",
    "\n",
    "The period of strong positive returns in late 2020 and 2021 reflects the rapid growth in BNPL adoption documented by the CFPB (2025), as consumers turned to alternative payment methods during the pandemic. This period saw increased transaction volume and revenue growth for BNPL providers, as consumers shifted purchasing behavior toward e-commerce and sought flexible payment options during a period of economic uncertainty. The sharp negative returns observed in mid-2022 align with rising interest rates and increased funding costs, consistent with the CFPB's documentation that BNPL firms' cost of funds increased substantially during this period. Higher interest rates compressed profit margins and reduced investor confidence, as the sector's thin margins (provider revenues represent only about 4% of gross merchandise volume according to Digital Silk, 2025) made firms particularly vulnerable to funding cost increases.\n",
    "\n",
    "The period from late 2023 through 2025 exhibits continued volatility, reflecting ongoing sensitivity to monetary policy changes, macroeconomic conditions, and sector-specific developments. This persistent volatility provides empirical motivation for our econometric analysis, which seeks to identify systematic factors that explain this observed variation.\n",
    "**Visual Design Elements:** The chart uses blue shading to indicate periods of positive returns (above the zero line) and orange shading to indicate negative returns (below zero). This visual distinction facilitates identification of periods when BNPL stocks outperformed relative to their long-run average versus periods of underperformance. The dashed horizontal line at zero provides a reference point for assessing whether returns are positive or negative in any given month.\n",
    "### 3.2.2 Chart B: Scatter Plot of Log BNPL Returns vs Interest Rate Changes\n",
    "\n",
    "![Log BNPL Returns vs Interest Rate Changes](bnpl_returns_vs_interest_rate.png)\n",
    "\n",
    "Chart B presents a scatter plot of log BNPL returns against month-over-month changes in the Federal Funds Rate, accompanied by the estimated regression line and 95% confidence interval. This visualization provides a direct visual test of our primary hypothesis that BNPL stock returns exhibit sensitivity to monetary policy changes. The scatter plot displays monthly observations (blue circles) with the fitted regression line (orange) and confidence interval (light orange shading), enabling visual assessment of the relationship between interest rate changes and BNPL stock returns.\n",
    "\n",
    "**Visual Interpretation:** The scatter plot reveals substantial dispersion around the regression line, with many observations deviating significantly from the fitted line. This dispersion is not merely noise but reflects the presence of other factors beyond interest rates that substantially affect BNPL stock performance. The negative slope of the regression line (estimated coefficient of -12.51) is visible in the chart, showing a negative point estimate, though this relationship is not statistically significant in BNPL returns, consistent with theoretical expectations. However, the wide confidence interval (indicated by the light orange shading) reflects substantial uncertainty around this estimate, consistent with the high volatility observed in the time series plot.\n",
    "\n",
    "**Statistical Interpretation:** The regression results indicate a negative relationship between interest rate changes and log BNPL returns, the point estimate aligns with theoretical predictions, but statistical insignificance prevents us from concluding a relationship exists on BNPL firm performance. The estimated slope coefficient is -12.51, The point estimate indicates that a one percentage point increase in the Federal Funds Rate change would be associated with approximately a 12.51% decrease in log BNPL returns, **but this relationship is not statistically significant** and we cannot reject the null hypothesis of no effect. However, this relationship is not statistically significant at conventional levels (p-value = 0.2202), and the R\u00b2 of 0.022 indicates that interest rate changes alone explain only 2.2% of the variation in log BNPL returns. The 95% confidence interval for the slope coefficient is [-32.69, 7.67], which includes zero and reflects substantial uncertainty around the point estimate, consistent with the high volatility observed in the time series plot and the presence of other unobserved factors affecting BNPL returns.\n",
    "\n",
    "**Implications for Model Specification:** The substantial dispersion around the regression line provides empirical motivation for our full specification model, which incorporates additional control variables to capture these other economic channels and improve the model's explanatory power. The fact that interest rates alone explain only 2.2% of return variation suggests that other factors play important roles in determining BNPL stock performance, motivating the inclusion of consumer confidence, disposable income, inflation, and market returns in the full model.\n",
    "\n",
    "**Visual Design Elements:** The x-axis tick marks are set at 0.1 percentage point intervals to provide clear visual reference points for interpreting the magnitude of interest rate changes and facilitate comparison across observations. The blue color scheme for observations and orange for the regression line maintains visual consistency with Chart A, while the confidence interval shading provides visual representation of statistical uncertainty around the point estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "control_variable_selection_interpretation",
   "metadata": {},
   "source": [
    "## 3.3 Functional Form Selection: Justification for Log-Linear Specification\n",
    "\n",
    "We use log-transformed BNPL returns as the dependent variable to address heteroskedasticity, normalize distributions, and facilitate elasticity interpretation of coefficients. Detailed theoretical and empirical justification for this specification choice is provided in Appendix A.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_improvement_section",
   "metadata": {},
   "source": [
    "## 3.4 Model Estimation: Implementation and Computational Approach\n",
    "\n",
    "\n",
    "\n",
    "This section describes the computational implementation of our econometric models. The estimation process involves several steps: data preparation and variable construction, model specification, parameter estimation using Ordinary Least Squares (OLS) regression, and calculation of robust standard errors. This section walks through these steps systematically, explaining the technical details of how we implement the models described in the previous sections.\n",
    "\n",
    "\n",
    "\n",
    "**Estimation Software and Methods:** We employ Python's `statsmodels` library for regression estimation, which provides a comprehensive suite of econometric tools. Specifically, we use `statsmodels.api.\n",
    "\n",
    "\n",
    "\n",
    "OLS` for Ordinary Least Squares estimation, which allows us to specify robust standard errors (HC3) directly in the estimation command. The HC3 robust standard errors, developed by MacKinnon and White (1985), provide consistent estimates of standard errors even in the presence of heteroskedasticity, making them particularly appropriate for financial return data.\n",
    "\n",
    "\n",
    "\n",
    "**Data Preparation Steps:** Before estimation, we perform several data preparation steps. First, we construct log-transformed BNPL returns using the formula log(1 + return/100), ensuring that the transformation is defined for all return values including negative returns. Second, we align all variables to monthly frequency and synchronize them to month-end dates, ensuring temporal consistency across all variables. Third, we handle missing data by using inner joins when merging variables, which may help ensure that we only retain observations where all variables have complete data. This approach is conservative but may help ensure that our sample consists of complete observations, avoiding potential issues with missing data that could bias our estimates.\n",
    "\n",
    "\n",
    "\n",
    "**Model Estimation Procedure:** For the base model, we estimate a simple regression of log BNPL returns on Federal Funds Rate changes. For the full specification model, we add four additional control variables: consumer confidence changes, disposable income changes, inflation changes, and market returns. Both models include a constant term (intercept), which is automatically added by `statsmodels` using the `add_constant` function. The estimation procedure uses maximum likelihood estimation under the assumption of normally distributed errors, though the robust standard errors ensure valid inference even if this assumption is violated.\n",
    "\n",
    "\n",
    "\n",
    "**Output and Diagnostics:** After estimation, we examine several diagnostic statistics to assess model quality. The R\u00b2 statistic measures the proportion of variation in the dependent variable explained by the model, while the adjusted R\u00b2 accounts for the number of parameters and provides a more conservative measure of model fit. The F-statistic tests the joint significance of all coefficients (except the intercept), providing an overall test of model significance. Individual t-statistics and p-values test the significance of each coefficient individually. We also examine residual plots and other diagnostics to assess whether the model assumptions are satisfied, though these diagnostics are presented in the visualization section rather than in the estimation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "22f2d45f",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE FACTOR CONTROLS AND DIAGNOSTICS IMPLEMENTATION\n",
    "# ============================================================================\n",
    "# This cell implements all required additions:\n",
    "# 1. Download Fama-French factors, VIX, sector ETFs\n",
    "# 2. Estimate factor-adjusted regression (Model 3)\n",
    "# 3. Run comprehensive diagnostics\n",
    "# 4. Perform robustness checks\n",
    "# 5. Generate professional tables and plots\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as diag\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan, het_white\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import jarque_bera\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import winsorize\n",
    "import yfinance as yf\n",
    "import pandas_datareader.data as web\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import io\n",
    "from datetime import datetime\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "# print(\"COMPREHENSIVE FACTOR CONTROLS AND DIAGNOSTICS\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DOWNLOAD FACTOR DATA\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"\\n1. DOWNLOADING FACTOR DATA...\")\n",
    "\n",
    "# Fama-French Factors\n",
    "# print(\"\\n  1.1 Downloading Fama-French 3-Factor + Momentum...\")\n",
    "# Try pandas_datareader first (more reliable)\n",
    "try:\n",
    "    import pandas_datareader.data as web\n",
    "    # Alternative: use pandas_datareader if available\n",
    "#     print(\"     Trying pandas_datareader method...\")\n",
    "    ff_factors = None  # Will try zip method below\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Primary method: Direct zip download\n",
    "try:\n",
    "    ff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"\n",
    "    with urllib.request.urlopen(ff_url) as response:\n",
    "        ff_zip = zipfile.ZipFile(io.BytesIO(response.read()))\n",
    "        # List files in zip to find correct filename\n",
    "        zip_files = ff_zip.namelist()\n",
    "        # Try common filename variations\n",
    "        ff_filename = None\n",
    "        for name in zip_files:\n",
    "            if 'F-F_Research_Data_Factors' in name or 'F-F_Research_Data_Factors' in name.upper():\n",
    "                ff_filename = name\n",
    "                break\n",
    "        if ff_filename is None:\n",
    "            # Use first CSV file if no match found\n",
    "            csv_files = [f for f in zip_files if f.endswith('.csv') or f.endswith('.CSV')]\n",
    "            if csv_files:\n",
    "                ff_filename = csv_files[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find CSV file in zip. Files: {zip_files}\")\n",
    "        \n",
    "        ff_file = ff_zip.open(ff_filename)\n",
    "        ff_data = pd.read_csv(ff_file, skiprows=3, nrows=1168, header=0)\n",
    "        ff_data.columns = ['Date', 'Mkt-RF', 'SMB', 'HML', 'RF']\n",
    "        ff_data['Date'] = pd.to_datetime(ff_data['Date'].astype(str), format='%Y%m')\n",
    "        ff_data.set_index('Date', inplace=True)\n",
    "        ff_data = ff_data / 100\n",
    "    \n",
    "    # Momentum factor\n",
    "    mom_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Momentum_Factor_CSV.zip\"\n",
    "    with urllib.request.urlopen(mom_url) as response:\n",
    "        mom_zip = zipfile.ZipFile(io.BytesIO(response.read()))\n",
    "        # List files in zip to find correct filename\n",
    "        mom_zip_files = mom_zip.namelist()\n",
    "        mom_filename = None\n",
    "        for name in mom_zip_files:\n",
    "            if 'Momentum' in name or 'MOM' in name.upper():\n",
    "                mom_filename = name\n",
    "                break\n",
    "        if mom_filename is None:\n",
    "            # Use first CSV file if no match found\n",
    "            csv_files = [f for f in mom_zip_files if f.endswith('.csv') or f.endswith('.CSV')]\n",
    "            if csv_files:\n",
    "                mom_filename = csv_files[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find CSV file in momentum zip. Files: {mom_zip_files}\")\n",
    "        \n",
    "        mom_file = mom_zip.open(mom_filename)\n",
    "        mom_data = pd.read_csv(mom_file, skiprows=13, nrows=1168, header=0)\n",
    "        mom_data.columns = ['Date', 'MOM']\n",
    "        mom_data['Date'] = pd.to_datetime(mom_data['Date'].astype(str), format='%Y%m')\n",
    "        mom_data.set_index('Date', inplace=True)\n",
    "        mom_data = mom_data / 100\n",
    "    \n",
    "    ff_factors = ff_data.merge(mom_data, left_index=True, right_index=True, how='inner')\n",
    "    ff_factors = ff_factors.resample('ME').last()\n",
    "#     print(f\"     \u2713 Fama-French factors: {len(ff_factors)} observations\")\n",
    "#     print(f\"       Date range: {ff_factors.index.min()} to {ff_factors.index.max()}\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "#     print(f\"     \u26a0 Error downloading Fama-French factors: {str(e)[:150]}\")\n",
    "    ff_factors = None\n",
    "\n",
    "# VIX\n",
    "# print(\"\\n  1.2 Downloading VIX...\")\n",
    "try:\n",
    "    vix = web.DataReader('VIXCLS', 'fred', start_date, end_date)\n",
    "    vix_monthly = vix.resample('ME').last()\n",
    "    vix_monthly.columns = ['VIX']\n",
    "#     print(f\"     \u2713 VIX: {len(vix_monthly)} observations\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "#     print(f\"     \u26a0 Error: {str(e)[:80]}\")\n",
    "    vix_monthly = None\n",
    "\n",
    "# Sector ETFs\n",
    "# print(\"\\n  1.3 Downloading Sector ETFs...\")\n",
    "try:\n",
    "    finx = yf.download('FINX', start=start_date, end=end_date, progress=False)\n",
    "    if not finx.empty:\n",
    "        finx_monthly = finx['Close'].resample('ME').last().pct_change() * 100\n",
    "        finx_monthly = pd.DataFrame({'FINX': finx_monthly})\n",
    "#         print(f\"     \u2713 FINX: {len(finx_monthly)} observations\")\n",
    "    else:\n",
    "        finx_monthly = None\n",
    "except Exception as e:\n",
    "    finx_monthly = None\n",
    "\n",
    "try:\n",
    "    ipay = yf.download('IPAY', start=start_date, end=end_date, progress=False)\n",
    "    if not ipay.empty:\n",
    "        ipay_monthly = ipay['Close'].resample('ME').last().pct_change() * 100\n",
    "        ipay_monthly = pd.DataFrame({'IPAY': ipay_monthly})\n",
    "#         print(f\"     \u2713 IPAY: {len(ipay_monthly)} observations\")\n",
    "    else:\n",
    "        ipay_monthly = None\n",
    "except Exception as e:\n",
    "    ipay_monthly = None\n",
    "\n",
    "# Alternative rate measures\n",
    "# print(\"\\n  1.4 Downloading Alternative Rate Measures...\")\n",
    "try:\n",
    "    two_year_yield = web.DataReader('DGS2', 'fred', start_date, end_date)\n",
    "    two_year_monthly = two_year_yield.resample('ME').last()\n",
    "    two_year_change = two_year_monthly.diff()\n",
    "    two_year_change.columns = ['two_year_yield_change']\n",
    "#     print(f\"     \u2713 2-Year Treasury Yield\")\n",
    "except Exception as e:\n",
    "    two_year_change = None\n",
    "\n",
    "try:\n",
    "    sofr = web.DataReader('SOFR', 'fred', start_date, end_date)\n",
    "    sofr_monthly = sofr.resample('ME').last()\n",
    "    sofr_change = sofr_monthly.diff()\n",
    "    sofr_change.columns = ['sofr_change']\n",
    "#     print(f\"     \u2713 SOFR\")\n",
    "except Exception as e:\n",
    "    sofr_change = None\n",
    "\n",
    "# Merge factor data\n",
    "# print(\"\\n  1.5 Merging Factor Data...\")\n",
    "if 'working_data' in globals() and working_data is not None:\n",
    "    working_data_with_factors = working_data.copy()\n",
    "    \n",
    "    # Merge all factors\n",
    "    factors_merged = []\n",
    "    for factor_name, factor_data in [\n",
    "        ('FF_Factors', ff_factors[['Mkt-RF', 'SMB', 'HML', 'RF', 'MOM']] if ff_factors is not None else None),\n",
    "        ('VIX', vix_monthly),\n",
    "        ('FINX', finx_monthly),\n",
    "        ('IPAY', ipay_monthly),\n",
    "        ('TwoYear', two_year_change),\n",
    "        ('SOFR', sofr_change)\n",
    "    ]:\n",
    "        if factor_data is not None and len(factor_data) > 0:\n",
    "            try:\n",
    "                working_data_with_factors = working_data_with_factors.merge(\n",
    "                    factor_data, left_index=True, right_index=True, how='left'\n",
    "                )\n",
    "                factors_merged.append(factor_name)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "#                 print(f\"       \u26a0 Failed to merge {factor_name}: {str(e)[:60]}\")\n",
    "    \n",
    "    if len(factors_merged) > 0:\n",
    "        pass\n",
    "#         print(f\"     \u2713 Factor data merged: {factors_merged}\")\n",
    "        new_cols = [c for c in working_data_with_factors.columns if c not in working_data.columns]\n",
    "        if new_cols:\n",
    "            pass\n",
    "#             print(f\"       New columns: {new_cols}\")\n",
    "        else:\n",
    "            pass\n",
    "#             print(f\"       (No new columns added - may already exist)\")\n",
    "    else:\n",
    "        pass\n",
    "#         print(f\"     \u26a0 No factors successfully merged\")\n",
    "        working_data_with_factors = working_data.copy()  # Keep original data\n",
    "else:\n",
    "    working_data_with_factors = None\n",
    "#     print(f\"     \u26a0 Cannot merge: working_data not available in globals()\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "34ad1eaa",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: FACTOR-ADJUSTED REGRESSION (MODEL 3)\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"\\n2. ESTIMATING FACTOR-ADJUSTED REGRESSION (MODEL 3)...\")\n",
    "\n",
    "if working_data_with_factors is not None and 'log_bnpl_return' in globals():\n",
    "    # Prepare factor variables\n",
    "    factor_vars = ['Mkt-RF', 'SMB', 'HML', 'MOM']\n",
    "    factor_vars_available = [v for v in factor_vars if v in working_data_with_factors.columns]\n",
    "    \n",
    "    if 'VIX' in working_data_with_factors.columns:\n",
    "        factor_vars_available.append('VIX')\n",
    "    \n",
    "    # Macro controls (excluding market_return since we use Mkt-RF)\n",
    "    macro_vars = ['fed_funds_change', 'consumer_conf_change', \n",
    "                  'disposable_income_change', 'inflation_change']\n",
    "    macro_vars_available = [v for v in macro_vars if v in working_data_with_factors.columns]\n",
    "    \n",
    "    factor_model_vars = factor_vars_available + macro_vars_available\n",
    "    \n",
    "    factor_data = working_data_with_factors[factor_model_vars].copy()\n",
    "    factor_data['log_bnpl_return'] = log_bnpl_return.reindex(factor_data.index)\n",
    "    factor_data = factor_data.dropna()\n",
    "    \n",
    "    if len(factor_data) > 10:\n",
    "        X_factor = factor_data[factor_model_vars].values\n",
    "        y_factor = factor_data['log_bnpl_return'].values\n",
    "        X_factor_const = sm.add_constant(X_factor)\n",
    "        \n",
    "        model_factor = sm.OLS(y_factor, X_factor_const).fit(cov_type='HC3')\n",
    "        \n",
    "#         print(f\"\\n   Model 3: Factor-Adjusted Specification\")\n",
    "#         print(f\"   Observations: {len(factor_data)}\")\n",
    "#         print(f\"   Variables: {len(factor_model_vars)}\")\n",
    "#         print(f\"   R\u00b2: {model_factor.rsquared:.4f}\")\n",
    "#         print(f\"   Adjusted R\u00b2: {model_factor.rsquared_adj:.4f}\")\n",
    "#         print(f\"\\n   {model_factor.summary()}\")\n",
    "        \n",
    "        factor_results = {\n",
    "            'model': model_factor,\n",
    "            'r_squared': model_factor.rsquared,\n",
    "            'adj_r_squared': model_factor.rsquared_adj,\n",
    "            'f_statistic': model_factor.fvalue,\n",
    "            'f_pvalue': model_factor.f_pvalue,\n",
    "            'n_obs': len(factor_data),\n",
    "            'variables': factor_model_vars\n",
    "        }\n",
    "        \n",
    "        final_factor_model = model_factor\n",
    "#         print(\"\\n   \u2713 Model 3 estimated successfully!\")\n",
    "    else:\n",
    "        pass\n",
    "#         print(\"   \u26a0 Insufficient data\")\n",
    "        factor_results = None\n",
    "        final_factor_model = None\n",
    "else:\n",
    "    pass\n",
    "#     print(\"   \u26a0 Data not available\")\n",
    "    factor_results = None\n",
    "    final_factor_model = None\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "77330d2a",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: COMPREHENSIVE DIAGNOSTICS\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"\\n3. COMPREHENSIVE ECONOMETRIC DIAGNOSTICS...\")\n",
    "\n",
    "def calculate_vif(X_df):\n",
    "    \"\"\"Calculate Variance Inflation Factors\"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X_df.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_df.values, i) \n",
    "                       for i in range(X_df.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Run diagnostics on full model\n",
    "if 'final_full_model' in globals() and final_full_model is not None:\n",
    "    pass\n",
    "#     print(\"\\n  3.1 Diagnostics for Full Model (Model 2)...\")\n",
    "    \n",
    "    # Get model data\n",
    "    if 'full_data' in locals() and 'available_vars_full' in globals():\n",
    "        X_full_diag_df = pd.DataFrame(full_data[available_vars_full])\n",
    "        var_names_full = available_vars_full\n",
    "    else:\n",
    "        X_full_diag_df = pd.DataFrame(X_full_const[:, 1:], columns=available_vars_full)\n",
    "        var_names_full = available_vars_full\n",
    "    \n",
    "    # VIF\n",
    "#     print(\"\\n    3.1.1 Variance Inflation Factors:\")\n",
    "    vif_table = calculate_vif(X_full_diag_df)\n",
    "#     print(vif_table.to_string(index=False))\n",
    "    \n",
    "    # Durbin-Watson\n",
    "    dw_stat = durbin_watson(final_full_model.resid)\n",
    "#     print(f\"\\n    3.1.2 Durbin-Watson Statistic: {dw_stat:.4f}\")\n",
    "    if dw_stat < 1.5:\n",
    "        pass\n",
    "#         print(\"      \u2192 Positive autocorrelation detected\")\n",
    "    elif dw_stat > 2.5:\n",
    "        pass\n",
    "#         print(\"      \u2192 Negative autocorrelation detected\")\n",
    "    else:\n",
    "        pass\n",
    "#         print(\"      \u2192 No significant autocorrelation\")\n",
    "    \n",
    "    # Breusch-Godfrey\n",
    "    try:\n",
    "        bg_test = diag.acorr_breusch_godfrey(final_full_model, nlags=4)\n",
    "#         print(f\"\\n    3.1.3 Breusch-Godfrey Test (4 lags):\")\n",
    "#         print(f\"      LM Statistic: {bg_test[0]:.4f}\")\n",
    "#         print(f\"      p-value: {bg_test[1]:.4f}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "#         print(f\"    3.1.3 Breusch-Godfrey Test: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    # Ljung-Box\n",
    "    try:\n",
    "        lb_test = acorr_ljungbox(final_full_model.resid, lags=4, return_df=True)\n",
    "#         print(f\"\\n    3.1.4 Ljung-Box Q-Test:\")\n",
    "#         print(lb_test.to_string())\n",
    "    except Exception as e:\n",
    "        pass\n",
    "#         print(f\"    3.1.4 Ljung-Box Test: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    # Breusch-Pagan\n",
    "    try:\n",
    "        bp_test = het_breuschpagan(final_full_model.resid, final_full_model.model.exog)\n",
    "#         print(f\"\\n    3.1.5 Breusch-Pagan Test:\")\n",
    "#         print(f\"      LM Statistic: {bp_test[0]:.4f}\")\n",
    "#         print(f\"      p-value: {bp_test[1]:.4f}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "#         print(f\"    3.1.5 Breusch-Pagan Test: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    # White Test\n",
    "    try:\n",
    "        white_test = het_white(final_full_model.resid, final_full_model.model.exog)\n",
    "#         print(f\"\\n    3.1.6 White Test:\")\n",
    "#         print(f\"      LM Statistic: {white_test[0]:.4f}\")\n",
    "#         print(f\"      p-value: {white_test[1]:.4f}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "#         print(f\"    3.1.6 White Test: Error - {str(e)[:50]}\")\n",
    "    \n",
    "    # Jarque-Bera\n",
    "    jb_stat, jb_pval = jarque_bera(final_full_model.resid)\n",
    "#     print(f\"\\n    3.1.7 Jarque-Bera Normality Test:\")\n",
    "#     print(f\"      Statistic: {jb_stat:.4f}\")\n",
    "#     print(f\"      p-value: {jb_pval:.4f}\")\n",
    "    \n",
    "    diagnostics_full_model = {\n",
    "        'vif': vif_table,\n",
    "        'durbin_watson': dw_stat,\n",
    "        'breusch_godfrey': bg_test if 'bg_test' in locals() else None,\n",
    "        'ljung_box': lb_test if 'lb_test' in locals() else None,\n",
    "        'breusch_pagan': bp_test if 'bp_test' in locals() else None,\n",
    "        'white': white_test if 'white_test' in locals() else None,\n",
    "        'jarque_bera': {'statistic': jb_stat, 'pvalue': jb_pval}\n",
    "    }\n",
    "else:\n",
    "    pass\n",
    "#     print(\"   \u26a0 Full model not available\")\n",
    "    diagnostics_full_model = None\n",
    "\n",
    "# Stationarity Tests (ADF)\n",
    "# print(\"\\n  3.2 Stationarity Tests (Augmented Dickey-Fuller)...\")\n",
    "if 'working_data' in globals():\n",
    "    adf_results = {}\n",
    "    test_vars = ['bnpl_return', 'fed_funds_change', 'consumer_conf_change', \n",
    "                 'disposable_income_change', 'inflation_change', 'market_return']\n",
    "    \n",
    "#     print(f\"\\n    {'Variable':<30} {'ADF Stat':>12} {'p-value':>12} {'Critical (5%)':>15} {'Stationary?':>12}\")\n",
    "#     print(\"    \" + \"-\" * 85)\n",
    "    \n",
    "    for var in test_vars:\n",
    "        if var in working_data.columns:\n",
    "            series = working_data[var].dropna()\n",
    "            if len(series) > 10:\n",
    "                adf_stat, adf_pval, _, _, adf_crit, _ = adfuller(series)\n",
    "                stationary = adf_pval < 0.05\n",
    "                adf_results[var] = {\n",
    "                    'statistic': adf_stat,\n",
    "                    'pvalue': adf_pval,\n",
    "                    'critical_5pct': adf_crit['5%'],\n",
    "                    'stationary': stationary\n",
    "                }\n",
    "                stat_str = f\"{adf_stat:.4f}\"\n",
    "                pval_str = f\"{adf_pval:.4f}\"\n",
    "                crit_str = f\"{adf_crit['5%']:.4f}\"\n",
    "                stat_bool = \"Yes\" if stationary else \"No\"\n",
    "#                 print(f\"    {var:<30} {stat_str:>12} {pval_str:>12} {crit_str:>15} {stat_bool:>12}\")\n",
    "else:\n",
    "    pass\n",
    "#     print(\"   \u26a0 working_data not available\")\n",
    "    adf_results = None\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6bd61f67",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: ROBUSTNESS CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"\\n4. ROBUSTNESS CHECKS...\")\n",
    "\n",
    "# 4.1 Alternative Shock Measures\n",
    "# print(\"\\n  4.1 Alternative Shock Measures...\")\n",
    "robustness_results = {}\n",
    "\n",
    "if working_data_with_factors is not None:\n",
    "    # 2-Year Treasury Yield\n",
    "    if 'two_year_yield_change' in working_data_with_factors.columns:\n",
    "        alt_data_2y = working_data_with_factors[['two_year_yield_change'] + \n",
    "                                                 (available_vars_full if 'available_vars_full' in globals() else [])].copy()\n",
    "        alt_data_2y['log_bnpl_return'] = log_bnpl_return.reindex(alt_data_2y.index)\n",
    "        alt_data_2y = alt_data_2y.dropna()\n",
    "        \n",
    "        if len(alt_data_2y) > 10:\n",
    "            X_2y = alt_data_2y[['two_year_yield_change']].values\n",
    "            y_2y = alt_data_2y['log_bnpl_return'].values\n",
    "            X_2y_const = sm.add_constant(X_2y)\n",
    "            model_2y = sm.OLS(y_2y, X_2y_const).fit(cov_type='HC3')\n",
    "            \n",
    "#             print(f\"\\n    Model with 2-Year Treasury Yield Change:\")\n",
    "#             print(f\"      Coefficient: {model_2y.params[1]:.4f}\")\n",
    "#             print(f\"      Std Error: {model_2y.bse[1]:.4f}\")\n",
    "#             print(f\"      p-value: {model_2y.pvalues[1]:.4f}\")\n",
    "#             print(f\"      R\u00b2: {model_2y.rsquared:.4f}\")\n",
    "            \n",
    "            robustness_results['two_year'] = {\n",
    "                'model': model_2y,\n",
    "                'coef': model_2y.params[1],\n",
    "                'se': model_2y.bse[1],\n",
    "                'pval': model_2y.pvalues[1],\n",
    "                'r2': model_2y.rsquared\n",
    "            }\n",
    "\n",
    "# 4.2 Rolling Window Regressions\n",
    "# print(\"\\n  4.2 Rolling Window Regressions (24-month window)...\")\n",
    "if 'working_data' in globals() and 'log_bnpl_return' in globals() and 'fed_funds_change' in working_data.columns:\n",
    "    def rolling_regression(data, y_var, x_var, window=24):\n",
    "        coefficients = []\n",
    "        dates = []\n",
    "        for i in range(window, len(data)):\n",
    "            window_data = data.iloc[i-window:i]\n",
    "            if len(window_data.dropna()) >= window * 0.8:\n",
    "                X_roll = sm.add_constant(window_data[[x_var]].values)\n",
    "                y_roll = window_data[y_var].values\n",
    "                try:\n",
    "                    model_roll = sm.OLS(y_roll, X_roll).fit(cov_type='HC3')\n",
    "                    coefficients.append(model_roll.params[1])\n",
    "                    dates.append(window_data.index[-1])\n",
    "                except:\n",
    "                    pass\n",
    "        return pd.Series(coefficients, index=dates)\n",
    "    \n",
    "    roll_data = working_data[['fed_funds_change']].copy()\n",
    "    roll_data['log_bnpl_return'] = log_bnpl_return.reindex(roll_data.index)\n",
    "    roll_data = roll_data.dropna()\n",
    "    \n",
    "    rolling_coefs = rolling_regression(roll_data, 'log_bnpl_return', 'fed_funds_change', window=24)\n",
    "    \n",
    "    if len(rolling_coefs) > 0:\n",
    "        pass\n",
    "#         print(f\"    \u2713 Rolling regressions: {len(rolling_coefs)} windows\")\n",
    "#         print(f\"      Mean: {rolling_coefs.mean():.4f}, Std: {rolling_coefs.std():.4f}\")\n",
    "        robustness_results['rolling'] = rolling_coefs\n",
    "    else:\n",
    "        robustness_results['rolling'] = None\n",
    "else:\n",
    "    robustness_results['rolling'] = None\n",
    "\n",
    "# 4.3 Outlier Controls\n",
    "# print(\"\\n  4.3 Outlier Controls...\")\n",
    "if 'working_data' in globals() and 'log_bnpl_return' in globals():\n",
    "    returns_series = log_bnpl_return.dropna()\n",
    "    Q1 = returns_series.quantile(0.25)\n",
    "    Q3 = returns_series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_iqr = returns_series[(returns_series < Q1 - 1.5*IQR) | \n",
    "                                  (returns_series > Q3 + 1.5*IQR)]\n",
    "    \n",
    "#     print(f\"    IQR Method: {len(outliers_iqr)} outliers identified\")\n",
    "    \n",
    "    winsorized_returns = winsorize(returns_series.values, limits=[0.01, 0.01])\n",
    "    winsorized_returns = pd.Series(winsorized_returns, index=returns_series.index)\n",
    "    \n",
    "#     print(f\"    Winsorizing: Returns winsorized at 1st and 99th percentiles\")\n",
    "    \n",
    "    robustness_results['outliers'] = {\n",
    "        'iqr_outliers': outliers_iqr,\n",
    "        'winsorized': winsorized_returns\n",
    "    }\n",
    "else:\n",
    "    robustness_results['outliers'] = None\n",
    "\n",
    "# 4.4 Structural Break Tests\n",
    "# print(\"\\n  4.4 Structural Break Tests (Chow Test)...\")\n",
    "if 'final_full_model' in globals() and 'full_data' in locals():\n",
    "    break_date = pd.Timestamp('2022-03-01')\n",
    "    pre_break = full_data[full_data.index < break_date]\n",
    "    post_break = full_data[full_data.index >= break_date]\n",
    "    \n",
    "    if len(pre_break) > 10 and len(post_break) > 10:\n",
    "        X_pre = pre_break[available_vars_full].values\n",
    "        y_pre = pre_break['log_bnpl_return'].values\n",
    "        X_pre_const = sm.add_constant(X_pre)\n",
    "        model_pre = sm.OLS(y_pre, X_pre_const).fit(cov_type='HC3')\n",
    "        \n",
    "        X_post = post_break[available_vars_full].values\n",
    "        y_post = post_break['log_bnpl_return'].values\n",
    "        X_post_const = sm.add_constant(X_post)\n",
    "        model_post = sm.OLS(y_post, X_post_const).fit(cov_type='HC3')\n",
    "        \n",
    "        n1, n2 = len(pre_break), len(post_break)\n",
    "        k = len(available_vars_full) + 1\n",
    "        SSR_pooled = final_full_model.ssr\n",
    "        SSR_pre = model_pre.ssr\n",
    "        SSR_post = model_post.ssr\n",
    "        \n",
    "        chow_stat = ((SSR_pooled - (SSR_pre + SSR_post)) / k) / ((SSR_pre + SSR_post) / (n1 + n2 - 2*k))\n",
    "        chow_pval = 1 - scipy_stats.f.cdf(chow_stat, k, n1 + n2 - 2*k)\n",
    "        \n",
    "#         print(f\"    Break date: {break_date.strftime('%Y-%m-%d')}\")\n",
    "#         print(f\"    Chow F-statistic: {chow_stat:.4f}\")\n",
    "#         print(f\"    p-value: {chow_pval:.4f}\")\n",
    "        \n",
    "        if 'fed_funds_change' in available_vars_full:\n",
    "            idx = available_vars_full.index('fed_funds_change')\n",
    "#             print(f\"    Pre-break FFR coef: {model_pre.params[idx+1]:.4f}\")\n",
    "#             print(f\"    Post-break FFR coef: {model_post.params[idx+1]:.4f}\")\n",
    "        \n",
    "        robustness_results['chow'] = {\n",
    "            'break_date': break_date,\n",
    "            'chow_stat': chow_stat,\n",
    "            'chow_pval': chow_pval,\n",
    "            'model_pre': model_pre,\n",
    "            'model_post': model_post\n",
    "        }\n",
    "    else:\n",
    "        robustness_results['chow'] = None\n",
    "else:\n",
    "    robustness_results['chow'] = None\n",
    "\n",
    "# 4.5 Firm-Level Regressions\n",
    "# print(\"\\n  4.5 Firm-Level Regressions...\")\n",
    "if 'bnpl_returns' in globals() and 'working_data' in globals():\n",
    "    firm_results_dict = {}\n",
    "    \n",
    "    for ticker in ['PYPL', 'AFRM', 'SEZL']:\n",
    "        if ticker in bnpl_returns:\n",
    "            firm_returns = bnpl_returns[ticker]\n",
    "            firm_log_returns = np.log(1 + firm_returns/100) * 100\n",
    "            \n",
    "            firm_data = working_data[available_vars_full].copy()\n",
    "            firm_data['log_return'] = firm_log_returns.reindex(firm_data.index)\n",
    "            firm_data = firm_data.dropna()\n",
    "            \n",
    "            if len(firm_data) > 10:\n",
    "                X_firm = firm_data[available_vars_full].values\n",
    "                y_firm = firm_data['log_return'].values\n",
    "                X_firm_const = sm.add_constant(X_firm)\n",
    "                model_firm = sm.OLS(y_firm, X_firm_const).fit(cov_type='HC3')\n",
    "                \n",
    "                if 'fed_funds_change' in available_vars_full:\n",
    "                    idx = available_vars_full.index('fed_funds_change')\n",
    "                    coef = model_firm.params[idx+1]\n",
    "                    se = model_firm.bse[idx+1]\n",
    "                    pval = model_firm.pvalues[idx+1]\n",
    "                    \n",
    "#                     print(f\"\\n    {ticker}:\")\n",
    "#                     print(f\"      FFR coefficient: {coef:.4f}\")\n",
    "#                     print(f\"      Std Error: {se:.4f}\")\n",
    "#                     print(f\"      p-value: {pval:.4f}\")\n",
    "#                     print(f\"      R\u00b2: {model_firm.rsquared:.4f}\")\n",
    "                \n",
    "                firm_results_dict[ticker] = {\n",
    "                    'model': model_firm,\n",
    "                    'coef': coef,\n",
    "                    'se': se,\n",
    "                    'pval': pval,\n",
    "                    'r2': model_firm.rsquared\n",
    "                }\n",
    "    \n",
    "    robustness_results['firm_level'] = firm_results_dict if len(firm_results_dict) > 0 else None\n",
    "else:\n",
    "    robustness_results['firm_level'] = None\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "523e0804",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: PROFESSIONAL REGRESSION TABLES\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"\\n5. CREATING PROFESSIONAL REGRESSION TABLES...\")\n",
    "\n",
    "def create_regression_table(models_dict, model_names):\n",
    "    \"\"\"Create professional regression results table\"\"\"\n",
    "    table_data = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        if model_name in models_dict and models_dict[model_name] is not None:\n",
    "            model = models_dict[model_name]['model']\n",
    "            var_names = models_dict[model_name].get('variables', [])\n",
    "            \n",
    "            # Get coefficient for interest rate variable\n",
    "            if 'fed_funds_change' in var_names:\n",
    "                idx = var_names.index('fed_funds_change')\n",
    "                coef = model.params[idx+1]\n",
    "                se = model.bse[idx+1]\n",
    "                tstat = model.tvalues[idx+1]\n",
    "                pval = model.pvalues[idx+1]\n",
    "                \n",
    "                # Handle both DataFrame and numpy array cases for conf_int\n",
    "                ci_result = model.conf_int()\n",
    "                if hasattr(ci_result, 'iloc'):\n",
    "                    # DataFrame case\n",
    "                    ci = ci_result.iloc[idx+1]\n",
    "                    ci_lower = ci[0]\n",
    "                    ci_upper = ci[1]\n",
    "                else:\n",
    "                    # numpy array case\n",
    "                    ci_lower = ci_result[idx+1, 0]\n",
    "                    ci_upper = ci_result[idx+1, 1]\n",
    "            else:\n",
    "                coef = se = tstat = pval = ci_lower = ci_upper = np.nan\n",
    "            \n",
    "            table_data.append({\n",
    "                'Model': model_name,\n",
    "                'Coefficient': coef,\n",
    "                'Std Error': se,\n",
    "                't-statistic': tstat,\n",
    "                'p-value': pval,\n",
    "                '95% CI Lower': ci_lower,\n",
    "                '95% CI Upper': ci_upper,\n",
    "                'R\u00b2': model.rsquared,\n",
    "                'Adj R\u00b2': model.rsquared_adj,\n",
    "                'N': int(model.nobs)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(table_data)\n",
    "\n",
    "# Create comprehensive table\n",
    "models_for_table = {}\n",
    "if 'final_base_model' in globals():\n",
    "    models_for_table['Model 1: Base'] = {\n",
    "        'model': final_base_model,\n",
    "        'variables': ['fed_funds_change'] if 'available_vars_base' in globals() else []\n",
    "    }\n",
    "if 'final_full_model' in globals():\n",
    "    models_for_table['Model 2: Full'] = {\n",
    "        'model': final_full_model,\n",
    "        'variables': available_vars_full if 'available_vars_full' in globals() else []\n",
    "    }\n",
    "if 'final_factor_model' in globals():\n",
    "    models_for_table['Model 3: Factor-Adjusted'] = {\n",
    "        'model': final_factor_model,\n",
    "        'variables': factor_model_vars if 'factor_model_vars' in globals() else []\n",
    "    }\n",
    "\n",
    "if len(models_for_table) > 0:\n",
    "    regression_table = create_regression_table(models_for_table, list(models_for_table.keys()))\n",
    "    \n",
    "#     print(\"\\n  Table: Regression Results Comparison\")\n",
    "#     print(\"  \" + \"=\"*100)\n",
    "#     print(regression_table.to_string(index=False))\n",
    "    \n",
    "    # Format for LaTeX/Markdown\n",
    "#     print(\"\\n  Formatted Table (Markdown):\")\n",
    "#     print(\"\\n| Model | Coefficient | Std Error | t-statistic | p-value | 95% CI Lower | 95% CI Upper | R\u00b2 | Adj R\u00b2 | N |\")\n",
    "#     print(\"|-------|-------------|-----------|-------------|---------|--------------|--------------|----|--------|---|\")\n",
    "    for _, row in regression_table.iterrows():\n",
    "        pass\n",
    "#         print(f\"| {row['Model']} | {row['Coefficient']:.4f} | {row['Std Error']:.4f} | {row['t-statistic']:.3f} | {row['p-value']:.4f} | {row['95% CI Lower']:.4f} | {row['95% CI Upper']:.4f} | {row['R\u00b2']:.4f} | {row['Adj R\u00b2']:.4f} | {int(row['N'])} |\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    regression_table.to_csv('regression_results_table.csv', index=False)\n",
    "#     print(\"\\n  \u2713 Table saved to regression_results_table.csv\")\n",
    "    \n",
    "    final_regression_table = regression_table\n",
    "else:\n",
    "    pass\n",
    "#     print(\"  \u26a0 Models not available\")\n",
    "    final_regression_table = None\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b665777a",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: DIAGNOSTIC PLOTS\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"\\n6. GENERATING DIAGNOSTIC PLOTS...\")\n",
    "\n",
    "if 'final_full_model' in globals():\n",
    "    # Create comprehensive diagnostic plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Diagnostic Plots: Full Model Residuals', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Residual vs Fitted\n",
    "    axes[0, 0].scatter(final_full_model.fittedvalues, final_full_model.resid, alpha=0.6, s=30)\n",
    "    axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
    "    axes[0, 0].set_xlabel('Fitted Values', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Residuals', fontsize=12)\n",
    "    axes[0, 0].set_title('Residual vs Fitted Plot', fontsize=13, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ-Plot\n",
    "    scipy_stats.probplot(final_full_model.resid, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('Q-Q Plot of Residuals', fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Histogram of Residuals\n",
    "    axes[1, 0].hist(final_full_model.resid, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[1, 0].set_xlabel('Residuals', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1, 0].set_title('Histogram of Residuals', fontsize=13, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Time Series of Residuals\n",
    "    if 'final_full_model_dates' in globals() and final_full_model_dates is not None:\n",
    "        axes[1, 1].plot(final_full_model_dates, final_full_model.resid, marker='o', markersize=3, linewidth=1)\n",
    "    else:\n",
    "        axes[1, 1].plot(final_full_model.resid.values, marker='o', markersize=3, linewidth=1)\n",
    "    axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
    "    axes[1, 1].set_xlabel('Time', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Residuals', fontsize=12)\n",
    "    axes[1, 1].set_title('Residuals Over Time', fontsize=13, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('diagnostic_plots_comprehensive.png', dpi=300, bbox_inches='tight')\n",
    "#     print(\"  \u2713 Diagnostic plots saved to diagnostic_plots_comprehensive.png\")\n",
    "    \n",
    "    # Rolling coefficients plot\n",
    "    if 'robustness_results' in globals() and robustness_results.get('rolling') is not None:\n",
    "        rolling_coefs = robustness_results['rolling']\n",
    "        if len(rolling_coefs) > 0:\n",
    "            fig2, ax = plt.subplots(figsize=(12, 6))\n",
    "            ax.plot(rolling_coefs.index, rolling_coefs.values, marker='o', linewidth=2, markersize=4)\n",
    "            ax.axhline(y=0, color='r', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "            ax.set_xlabel('Date', fontsize=12)\n",
    "            ax.set_ylabel('Rolling Coefficient (\u03b2\u2081)', fontsize=12)\n",
    "            ax.set_title('Rolling Window Regression Coefficients (24-month window)', fontsize=14, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('rolling_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "#             print(\"  \u2713 Rolling coefficients plot saved to rolling_coefficients.png\")\n",
    "            plt.close('all')\n",
    "    \n",
    "    plt.close('all')\n",
    "else:\n",
    "    pass\n",
    "#     print(\"  \u26a0 Full model not available for plots\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPREHENSIVE IMPLEMENTATION COMPLETE\")\n",
    "# print(\"\\nAll sections implemented:\")\n",
    "# print(\"  \u2713 Factor data downloaded and merged\")\n",
    "# print(\"  \u2713 Factor-adjusted regression (Model 3) estimated\")\n",
    "# print(\"  \u2713 Comprehensive diagnostics run\")\n",
    "# print(\"  \u2713 Robustness checks completed\")\n",
    "# print(\"  \u2713 Professional tables created\")\n",
    "# print(\"  \u2713 Diagnostic plots generated\")\n",
    "# print(\"\\nResults stored in:\")\n",
    "# print(\"  - final_factor_model: Factor-adjusted regression\")\n",
    "# print(\"  - diagnostics_full_model: All diagnostic test results\")\n",
    "# print(\"  - robustness_results: All robustness check results\")\n",
    "# print(\"  - final_regression_table: Professional regression table\")\n",
    "# print(\"  - diagnostic_plots_comprehensive.png: Diagnostic plots\")\n",
    "# print(\"  - rolling_coefficients.png: Rolling window plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1e5b3ccd",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT ALL RESULTS FOR MARKDOWN FILE\n",
    "# ============================================================================\n",
    "# This cell extracts all numerical results to fill in 04_empirical_results_REBUILT.md\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "# Extract Model 1 (Base) results\n",
    "if 'final_base_model' in globals() and final_base_model is not None:\n",
    "    model1 = final_base_model\n",
    "    if 'available_vars_base' in globals() and len(available_vars_base) > 0:\n",
    "        rate_idx = 1  # Index 0 is constant\n",
    "        results_dict['model1'] = {\n",
    "            'const_coef': model1.params[0],\n",
    "            'const_se': model1.bse[0],\n",
    "            'const_tstat': model1.tvalues[0],\n",
    "            'const_pval': model1.pvalues[0],\n",
    "            'const_ci_low': model1.conf_int().iloc[0, 0] if hasattr(model1.conf_int(), 'iloc') else model1.conf_int()[0, 0],\n",
    "            'const_ci_up': model1.conf_int().iloc[0, 1] if hasattr(model1.conf_int(), 'iloc') else model1.conf_int()[0, 1],\n",
    "            'ffr_coef': model1.params[rate_idx],\n",
    "            'ffr_se': model1.bse[rate_idx],\n",
    "            'ffr_tstat': model1.tvalues[rate_idx],\n",
    "            'ffr_pval': model1.pvalues[rate_idx],\n",
    "            'ffr_ci_low': model1.conf_int().iloc[rate_idx, 0] if hasattr(model1.conf_int(), 'iloc') else model1.conf_int()[rate_idx, 0],\n",
    "            'ffr_ci_up': model1.conf_int().iloc[rate_idx, 1] if hasattr(model1.conf_int(), 'iloc') else model1.conf_int()[rate_idx, 1],\n",
    "            'r2': model1.rsquared,\n",
    "            'adj_r2': model1.rsquared_adj,\n",
    "            'f_stat': model1.fvalue,\n",
    "            'f_pval': model1.f_pvalue,\n",
    "            'n': int(model1.nobs)\n",
    "        }\n",
    "\n",
    "# Extract Model 2 (Full) results\n",
    "if 'final_full_model' in globals() and final_full_model is not None:\n",
    "    model2 = final_full_model\n",
    "    if 'available_vars_full' in globals():\n",
    "        var_names = ['const'] + available_vars_full\n",
    "        results_dict['model2'] = {\n",
    "            'r2': model2.rsquared,\n",
    "            'adj_r2': model2.rsquared_adj,\n",
    "            'f_stat': model2.fvalue,\n",
    "            'f_pval': model2.f_pvalue,\n",
    "            'n': int(model2.nobs)\n",
    "        }\n",
    "        \n",
    "        # Extract each coefficient\n",
    "        for i, var in enumerate(var_names):\n",
    "            if i < len(model2.params):\n",
    "                ci_result = model2.conf_int()\n",
    "                if hasattr(ci_result, 'iloc'):\n",
    "                    ci_low = ci_result.iloc[i, 0]\n",
    "                    ci_up = ci_result.iloc[i, 1]\n",
    "                else:\n",
    "                    ci_low = ci_result[i, 0]\n",
    "                    ci_up = ci_result[i, 1]\n",
    "                \n",
    "                results_dict['model2'][var] = {\n",
    "                    'coef': model2.params[i],\n",
    "                    'se': model2.bse[i],\n",
    "                    'tstat': model2.tvalues[i],\n",
    "                    'pval': model2.pvalues[i],\n",
    "                    'ci_low': ci_low,\n",
    "                    'ci_up': ci_up\n",
    "                }\n",
    "\n",
    "# Extract Model 3 (Factor-Adjusted) results\n",
    "if 'final_factor_model' in globals() and final_factor_model is not None:\n",
    "    model3 = final_factor_model\n",
    "    if 'factor_model_vars' in globals():\n",
    "        var_names = ['const'] + factor_model_vars\n",
    "        results_dict['model3'] = {\n",
    "            'r2': model3.rsquared,\n",
    "            'adj_r2': model3.rsquared_adj,\n",
    "            'f_stat': model3.fvalue,\n",
    "            'f_pval': model3.f_pvalue,\n",
    "            'n': int(model3.nobs)\n",
    "        }\n",
    "        \n",
    "        for i, var in enumerate(var_names):\n",
    "            if i < len(model3.params):\n",
    "                ci_result = model3.conf_int()\n",
    "                if hasattr(ci_result, 'iloc'):\n",
    "                    ci_low = ci_result.iloc[i, 0]\n",
    "                    ci_up = ci_result.iloc[i, 1]\n",
    "                else:\n",
    "                    ci_low = ci_result[i, 0]\n",
    "                    ci_up = ci_result[i, 1]\n",
    "                \n",
    "                results_dict['model3'][var] = {\n",
    "                    'coef': model3.params[i],\n",
    "                    'se': model3.bse[i],\n",
    "                    'tstat': model3.tvalues[i],\n",
    "                    'pval': model3.pvalues[i],\n",
    "                    'ci_low': ci_low,\n",
    "                    'ci_up': ci_up\n",
    "                }\n",
    "\n",
    "# Extract diagnostics\n",
    "if 'diagnostics_full_model' in globals() and diagnostics_full_model is not None:\n",
    "    diag = diagnostics_full_model\n",
    "    results_dict['diagnostics'] = {}\n",
    "    \n",
    "    if 'vif' in diag and diag['vif'] is not None:\n",
    "        vif_df = diag['vif']\n",
    "        results_dict['diagnostics']['vif'] = {}\n",
    "        for _, row in vif_df.iterrows():\n",
    "            results_dict['diagnostics']['vif'][row['Variable']] = row['VIF']\n",
    "    \n",
    "    if 'durbin_watson' in diag:\n",
    "        results_dict['diagnostics']['dw'] = diag['durbin_watson']\n",
    "    \n",
    "    if 'breusch_godfrey' in diag and diag['breusch_godfrey'] is not None:\n",
    "        bg = diag['breusch_godfrey']\n",
    "        results_dict['diagnostics']['bg_stat'] = bg[0] if isinstance(bg, tuple) else bg.get('lm_stat', None)\n",
    "        results_dict['diagnostics']['bg_pval'] = bg[1] if isinstance(bg, tuple) else bg.get('pvalue', None)\n",
    "    \n",
    "    if 'breusch_pagan' in diag and diag['breusch_pagan'] is not None:\n",
    "        bp = diag['breusch_pagan']\n",
    "        results_dict['diagnostics']['bp_stat'] = bp[0] if isinstance(bp, tuple) else bp.get('lm_stat', None)\n",
    "        results_dict['diagnostics']['bp_pval'] = bp[1] if isinstance(bp, tuple) else bp.get('pvalue', None)\n",
    "    \n",
    "    if 'white' in diag and diag['white'] is not None:\n",
    "        white = diag['white']\n",
    "        results_dict['diagnostics']['white_stat'] = white[0] if isinstance(white, tuple) else white.get('lm_stat', None)\n",
    "        results_dict['diagnostics']['white_pval'] = white[1] if isinstance(white, tuple) else white.get('pvalue', None)\n",
    "    \n",
    "    if 'jarque_bera' in diag:\n",
    "        results_dict['diagnostics']['jb_stat'] = diag['jarque_bera'].get('statistic', None)\n",
    "        results_dict['diagnostics']['jb_pval'] = diag['jarque_bera'].get('pvalue', None)\n",
    "\n",
    "# Extract ADF results (from output)\n",
    "if 'adf_results' in globals() and adf_results is not None:\n",
    "    results_dict['adf'] = adf_results\n",
    "\n",
    "# Extract robustness results\n",
    "if 'robustness_results' in globals() and robustness_results is not None:\n",
    "    rob = robustness_results\n",
    "    results_dict['robustness'] = {}\n",
    "    \n",
    "    if 'two_year' in rob and rob['two_year'] is not None:\n",
    "        results_dict['robustness']['two_year'] = {\n",
    "            'coef': rob['two_year']['coef'],\n",
    "            'se': rob['two_year']['se'],\n",
    "            'pval': rob['two_year']['pval'],\n",
    "            'r2': rob['two_year']['r2']\n",
    "        }\n",
    "    \n",
    "    if 'rolling' in rob and rob['rolling'] is not None:\n",
    "        results_dict['robustness']['rolling'] = {\n",
    "            'mean': float(rob['rolling'].mean()),\n",
    "            'std': float(rob['rolling'].std()),\n",
    "            'n_windows': len(rob['rolling'])\n",
    "        }\n",
    "    \n",
    "    if 'chow' in rob and rob['chow'] is not None:\n",
    "        results_dict['robustness']['chow'] = {\n",
    "            'break_date': str(rob['chow']['break_date']),\n",
    "            'f_stat': rob['chow']['chow_stat'],\n",
    "            'pval': rob['chow']['chow_pval']\n",
    "        }\n",
    "        # Extract pre/post coefficients\n",
    "        if 'model_pre' in rob['chow'] and 'model_post' in rob['chow']:\n",
    "            if 'available_vars_full' in globals() and 'fed_funds_change' in available_vars_full:\n",
    "                idx = available_vars_full.index('fed_funds_change')\n",
    "                model_pre = rob['chow']['model_pre']\n",
    "                model_post = rob['chow']['model_post']\n",
    "                results_dict['robustness']['chow']['pre_coef'] = model_pre.params[idx+1]\n",
    "                results_dict['robustness']['chow']['post_coef'] = model_post.params[idx+1]\n",
    "    \n",
    "    if 'firm_level' in rob and rob['firm_level'] is not None:\n",
    "        results_dict['robustness']['firm_level'] = {}\n",
    "        for ticker, firm_data in rob['firm_level'].items():\n",
    "            results_dict['robustness']['firm_level'][ticker] = {\n",
    "                'coef': firm_data['coef'],\n",
    "                'se': firm_data['se'],\n",
    "                'pval': firm_data['pval'],\n",
    "                'r2': firm_data['r2']\n",
    "            }\n",
    "\n",
    "# Save to JSON for easy access\n",
    "with open('extracted_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2, default=str)\n",
    "\n",
    "# print(\"RESULTS EXTRACTION COMPLETE\")\n",
    "# print(\"\\nExtracted results saved to: extracted_results.json\")\n",
    "# print(\"\\nKey values extracted:\")\n",
    "# print(f\"  Model 1: R\u00b2 = {results_dict.get('model1', {}).get('r2', 'N/A'):.4f}\")\n",
    "# print(f\"  Model 2: R\u00b2 = {results_dict.get('model2', {}).get('r2', 'N/A'):.4f}\")\n",
    "# print(f\"  Model 3: R\u00b2 = {results_dict.get('model3', {}).get('r2', 'N/A'):.4f}\")\n",
    "# print(f\"  Diagnostics: {len(results_dict.get('diagnostics', {}))} tests\")\n",
    "# print(f\"  Robustness: {len(results_dict.get('robustness', {}))} checks\")\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Next step: Run the markdown filling script to populate 04_empirical_results_REBUILT.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic_explanation",
   "metadata": {},
   "source": [
    "## 3.5 Comprehensive Diagnostic Tests and Robustness Checks\n",
    "\n",
    "This section implements formal econometric diagnostic tests and robustness checks to assess the validity of our regression models and the reliability of our coefficient estimates. While visual diagnostics provide intuitive assessments of model assumptions, formal statistical tests provide rigorous quantitative evidence on whether our models satisfy key econometric requirements.\n",
    "\n",
    "**Correlation Matrix Analysis:** The correlation matrix examines pairwise correlations among all independent variables in the full specification model. High correlations (typically |r| > 0.7) between independent variables may indicate multicollinearity, which can inflate standard errors and make coefficient estimates unstable. However, moderate correlations are expected in macroeconomic models, as economic variables often move together. The correlation matrix helps identify which variables are most closely related and whether multicollinearity poses a serious concern for our analysis.\n",
    "\n",
    "**Variance Inflation Factor (VIF):** The VIF provides a quantitative measure of multicollinearity by measuring how much the variance of a coefficient estimate increases due to correlation with other independent variables. VIF values greater than 10 typically indicate severe multicollinearity, while values between 5 and 10 suggest moderate multicollinearity. Values below 5 generally indicate that multicollinearity is not a serious concern. The VIF complements the correlation matrix by providing a single summary statistic for each variable that accounts for its correlation with all other variables simultaneously.\n",
    "\n",
    "**Formal Heteroskedasticity Tests:** While we employ robust standard errors (HC3) that remain valid even in the presence of heteroskedasticity, formal tests provide evidence on whether heteroskedasticity is present in our data. The Breusch-Pagan test examines whether error variance depends on the independent variables, while the White test is more general and examines whether error variance depends on the independent variables, their squares, and cross-products. Both tests have null hypotheses of homoskedasticity (constant error variance). Rejection of these null hypotheses would indicate heteroskedasticity, which validates our use of robust standard errors.\n",
    "\n",
    "**Autocorrelation Test:** The Durbin-Watson statistic tests for first-order autocorrelation in regression residuals. Values close to 2 indicate no autocorrelation, values below 1.5 suggest positive autocorrelation, and values above 2.5 suggest negative autocorrelation. Autocorrelation violates the assumption that error terms are independently distributed and can lead to inefficient coefficient estimates and invalid standard errors. However, for monthly data, some autocorrelation may be expected, and our robust standard errors help address this concern.\n",
    "\n",
    "**Robustness Check: Excluding COVID-19 Period:** This robustness check examines whether our results are sensitive to the inclusion of the early COVID-19 period (February-June 2020), which was characterized by extreme market volatility and unusual economic conditions. If excluding this period substantially changes our coefficient estimates, it would suggest that our findings may be driven by these unusual events rather than general patterns. This check helps assess whether our results generalize beyond the specific conditions of the pandemic period.\n",
    "\n",
    "**Model Selection Criteria:** The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) provide formal criteria for comparing model specifications. Both criteria penalize models for complexity (number of parameters), with BIC imposing a stronger penalty. Lower values indicate better model fit adjusted for complexity. Comparing AIC and BIC between the base and full specification models helps assess whether the improvement in fit from adding control variables justifies the increased model complexity.\n",
    "\n",
    "These diagnostic tests collectively provide comprehensive evidence on the validity of our econometric models and the reliability of our coefficient estimates. While no single test is definitive, together they provide a thorough assessment of whether our models satisfy key econometric assumptions and whether our results are robust to alternative specifications and sample periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "diagnostic_tools",
   "metadata": {
    "tags": [
     "remove-input",
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Diagnostic tests complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE DIAGNOSTIC TESTS AND ROBUSTNESS CHECKS\n",
    "# ============================================================================\n",
    "# This cell implements formal diagnostic tests and robustness checks\n",
    "# that strengthen our econometric analysis\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy import stats\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPREHENSIVE DIAGNOSTIC TESTS AND ROBUSTNESS CHECKS\")\n",
    "\n",
    "# Check if models exist\n",
    "if 'final_full_model' not in globals() or final_full_model is None:\n",
    "    pass\n",
    "#     print(\"\\n\u26a0 Error: Full model not found. Please run model estimation cell first.\")\n",
    "else:\n",
    "    \n",
    "    # Get the model and data\n",
    "    model = final_full_model\n",
    "    \n",
    "    # Get the data used in the model\n",
    "    if 'expanded_data' in globals() and expanded_data is not None:\n",
    "        working_data = expanded_data\n",
    "    elif 'refined_data' in globals() and refined_data is not None:\n",
    "        working_data = refined_data\n",
    "    elif 'data' in globals() and data is not None:\n",
    "        working_data = data\n",
    "    else:\n",
    "        pass\n",
    "#         print(\"\\n\u26a0 Error: Data not found.\")\n",
    "        working_data = None\n",
    "    \n",
    "    if working_data is not None:\n",
    "        \n",
    "        # Prepare variables for full model\n",
    "        full_vars = ['fed_funds_change', 'consumer_conf_change', \n",
    "                     'disposable_income_change', 'inflation_change', 'market_return']\n",
    "        available_vars = [v for v in full_vars if v in working_data.columns]\n",
    "        \n",
    "        # Create log BNPL returns\n",
    "        log_bnpl_return = np.log(1 + working_data['bnpl_return']/100) * 100\n",
    "        \n",
    "        # Prepare full model data\n",
    "        full_data = working_data[available_vars].copy()\n",
    "        full_data['log_bnpl_return'] = log_bnpl_return.reindex(full_data.index)\n",
    "        full_data = full_data.dropna()\n",
    "        \n",
    "        X_full = full_data[available_vars].values\n",
    "        y_full = full_data['log_bnpl_return'].values\n",
    "        X_full_const = sm.add_constant(X_full)\n",
    "        \n",
    "        # ========================================================================\n",
    "        # 1. CORRELATION MATRIX\n",
    "        # ========================================================================\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"1. CORRELATION MATRIX: Testing for Multicollinearity\")\n",
    "        \n",
    "        corr_data = full_data[available_vars].copy()\n",
    "        corr_matrix = corr_data.corr()\n",
    "        \n",
    "#         print(\"\\nCorrelation Matrix (Full Model Variables):\")\n",
    "#         print(corr_matrix.round(3))\n",
    "        \n",
    "        # Identify high correlations\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            pass\n",
    "#             print(\"\\n\u26a0 High correlations detected (|r| > 0.7):\")\n",
    "            for var1, var2, corr_val in high_corr_pairs:\n",
    "                pass\n",
    "#                 print(f\"  {var1} vs {var2}: {corr_val:.3f}\")\n",
    "        else:\n",
    "            pass\n",
    "#             print(\"\\n\u2713 No high correlations detected (all |r| \u2264 0.7)\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # 2. VARIANCE INFLATION FACTOR (VIF)\n",
    "        # ========================================================================\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"2. VARIANCE INFLATION FACTOR (VIF): Multicollinearity Test\")\n",
    "        \n",
    "        # Calculate VIF for each variable\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Variable\"] = available_vars\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X_full, i) \n",
    "                            for i in range(len(available_vars))]\n",
    "        \n",
    "#         print(\"\\nVIF Values:\")\n",
    "#         print(vif_data.to_string(index=False))\n",
    "        \n",
    "        # Interpretation\n",
    "        high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "        moderate_vif = vif_data[(vif_data['VIF'] > 5) & (vif_data['VIF'] <= 10)]\n",
    "        \n",
    "        if len(high_vif) > 0:\n",
    "            pass\n",
    "#             print(\"\\n\u26a0 High VIF detected (VIF > 10):\")\n",
    "#             print(high_vif.to_string(index=False))\n",
    "#             print(\"  \u2192 Suggests severe multicollinearity\")\n",
    "        \n",
    "        if len(moderate_vif) > 0:\n",
    "            pass\n",
    "#             print(\"\\n\u26a0 Moderate VIF detected (5 < VIF \u2264 10):\")\n",
    "#             print(moderate_vif.to_string(index=False))\n",
    "#             print(\"  \u2192 Suggests moderate multicollinearity\")\n",
    "        \n",
    "        if len(high_vif) == 0 and len(moderate_vif) == 0:\n",
    "            pass\n",
    "#             print(\"\\n\u2713 All VIF values \u2264 5: No significant multicollinearity detected\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # 3. FORMAL HETEROSKEDASTICITY TESTS\n",
    "        # ========================================================================\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"3. FORMAL HETEROSKEDASTICITY TESTS\")\n",
    "        \n",
    "        # Get residuals and fitted values\n",
    "        residuals = model.resid\n",
    "        fitted_values = model.fittedvalues\n",
    "        \n",
    "        # Breusch-Pagan test\n",
    "        try:\n",
    "            bp_lm, bp_pvalue, bp_fstat, bp_fpvalue = het_breuschpagan(residuals, X_full_const)\n",
    "#             print(\"\\nBreusch-Pagan Test:\")\n",
    "#             print(f\"  LM statistic: {bp_lm:.4f}\")\n",
    "#             print(f\"  p-value: {bp_pvalue:.4f}\")\n",
    "            if bp_pvalue < 0.05:\n",
    "                pass\n",
    "#                 print(\"  \u2192 Reject H0: Heteroskedasticity detected\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(\"  \u2192 Fail to reject H0: No heteroskedasticity detected\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "#             print(f\"\\n\u26a0 Breusch-Pagan test failed: {e}\")\n",
    "        \n",
    "        # White test\n",
    "        try:\n",
    "            white_lm, white_pvalue, white_fstat, white_fpvalue = het_white(residuals, X_full_const)\n",
    "#             print(\"\\nWhite Test:\")\n",
    "#             print(f\"  LM statistic: {white_lm:.4f}\")\n",
    "#             print(f\"  p-value: {white_pvalue:.4f}\")\n",
    "            if white_pvalue < 0.05:\n",
    "                pass\n",
    "#                 print(\"  \u2192 Reject H0: Heteroskedasticity detected\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(\"  \u2192 Fail to reject H0: No heteroskedasticity detected\")\n",
    "        except Exception as e:\n",
    "            pass\n",
    "#             print(f\"\\n\u26a0 White test failed: {e}\")\n",
    "        \n",
    "#         print(\"\\nNote: We use HC3 robust standard errors, which remain valid even\")\n",
    "#         print(\"      if heteroskedasticity is present.\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # 4. AUTOCORRELATION TEST (Durbin-Watson)\n",
    "        # ========================================================================\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"4. AUTOCORRELATION TEST: Durbin-Watson Statistic\")\n",
    "        \n",
    "        dw_stat = durbin_watson(residuals)\n",
    "#         print(f\"\\nDurbin-Watson statistic: {dw_stat:.4f}\")\n",
    "        \n",
    "        # Interpretation (rule of thumb: DW \u2248 2 indicates no autocorrelation)\n",
    "        if 1.5 < dw_stat < 2.5:\n",
    "            pass\n",
    "#             print(\"  \u2192 No significant autocorrelation detected (DW \u2248 2)\")\n",
    "        elif dw_stat < 1.5:\n",
    "            pass\n",
    "#             print(\"  \u2192 Positive autocorrelation may be present (DW < 1.5)\")\n",
    "        else:\n",
    "            pass\n",
    "#             print(\"  \u2192 Negative autocorrelation may be present (DW > 2.5)\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # 5. ROBUSTNESS CHECK: Excluding COVID-19 Period\n",
    "        # ========================================================================\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"5. ROBUSTNESS CHECK: Excluding COVID-19 Period (Feb-Jun 2020)\")\n",
    "        \n",
    "        # Exclude early COVID period (Feb-Jun 2020)\n",
    "        covid_period = (full_data.index >= '2020-02-01') & (full_data.index <= '2020-06-30')\n",
    "        robust_data = full_data[~covid_period].copy()\n",
    "        \n",
    "#         print(f\"\\nOriginal sample: {len(full_data)} observations\")\n",
    "#         print(f\"Robust sample (excluding COVID): {len(robust_data)} observations\")\n",
    "#         print(f\"Observations excluded: {len(full_data) - len(robust_data)}\")\n",
    "        \n",
    "        if len(robust_data) >= 20:  # Need sufficient observations\n",
    "            \n",
    "            # Estimate model on robust sample\n",
    "            X_robust = robust_data[available_vars].values\n",
    "            y_robust = robust_data['log_bnpl_return'].values\n",
    "            X_robust_const = sm.add_constant(X_robust)\n",
    "            \n",
    "            model_robust = sm.OLS(y_robust, X_robust_const).fit(cov_type='HC3')\n",
    "            \n",
    "            # Compare coefficients\n",
    "#             print(\"\\n\" + \"-\"*80)\n",
    "#             print(\"COMPARISON: Full Sample vs Robust Sample (Excluding COVID)\")\n",
    "            \n",
    "            # Interest rate coefficient comparison\n",
    "            orig_coef = model.params[1]  # First variable after constant\n",
    "            robust_coef = model_robust.params[1]\n",
    "            orig_se = model.bse[1]\n",
    "            robust_se = model_robust.bse[1]\n",
    "            \n",
    "#             print(f\"\\nInterest Rate Coefficient:\")\n",
    "#             print(f\"  Full sample: {orig_coef:.4f} (SE: {orig_se:.4f})\")\n",
    "#             print(f\"  Robust sample: {robust_coef:.4f} (SE: {robust_se:.4f})\")\n",
    "#             print(f\"  Difference: {abs(orig_coef - robust_coef):.4f}\")\n",
    "            \n",
    "            # R\u00b2 comparison\n",
    "#             print(f\"\\nR\u00b2 Comparison:\")\n",
    "#             print(f\"  Full sample: {model.rsquared:.4f}\")\n",
    "#             print(f\"  Robust sample: {model_robust.rsquared:.4f}\")\n",
    "            \n",
    "            # Interpretation\n",
    "            coef_change_pct = abs((orig_coef - robust_coef) / orig_coef) * 100 if orig_coef != 0 else 0\n",
    "            if coef_change_pct < 10:\n",
    "                pass\n",
    "#                 print(\"\\n\u2713 Coefficient remains stable (change < 10%)\")\n",
    "#                 print(\"  \u2192 Results are robust to excluding COVID period\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"\\n\u26a0 Coefficient changes by {coef_change_pct:.1f}%\")\n",
    "#                 print(\"  \u2192 Results may be sensitive to COVID period\")\n",
    "        else:\n",
    "            pass\n",
    "#             print(\"\\n\u26a0 Insufficient observations for robustness check\")\n",
    "        \n",
    "        # ========================================================================\n",
    "        # 6. MODEL SELECTION CRITERIA (AIC/BIC)\n",
    "        # ========================================================================\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"6. MODEL SELECTION CRITERIA: AIC and BIC Comparison\")\n",
    "        \n",
    "        if 'final_base_model' in globals() and final_base_model is not None:\n",
    "            base_model = final_base_model\n",
    "            \n",
    "            comparison = pd.DataFrame({\n",
    "                'Model': ['Base Model', 'Full Model'],\n",
    "                'AIC': [base_model.aic, model.aic],\n",
    "                'BIC': [base_model.bic, model.bic],\n",
    "                'R\u00b2': [base_model.rsquared, model.rsquared],\n",
    "                'Adj R\u00b2': [base_model.rsquared_adj, model.rsquared_adj]\n",
    "            })\n",
    "            \n",
    "#             print(\"\\nModel Comparison:\")\n",
    "#             print(comparison.to_string(index=False))\n",
    "            \n",
    "#             print(\"\\nInterpretation:\")\n",
    "#             print(\"  Lower AIC/BIC indicates better model fit\")\n",
    "            if model.aic < base_model.aic:\n",
    "                pass\n",
    "#                 print(f\"  \u2192 Full model preferred (AIC: {model.aic:.2f} < {base_model.aic:.2f})\")\n",
    "            else:\n",
    "                pass\n",
    "#                 print(f\"  \u2192 Base model preferred (AIC: {base_model.aic:.2f} < {model.aic:.2f})\")\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\u2713 Diagnostic tests complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333cf25",
   "metadata": {},
   "source": [
    "## 3.6 Model Diagnostics and Visual Assessment\n",
    "\n",
    "\n",
    "\n",
    "This section presents a comprehensive dashboard of six diagnostic plots that provide visual assessment of our regression models' performance and adherence to econometric assumptions. These visualizations complement the numerical statistics presented in the regression tables by offering intuitive graphical representations of model fit, residual patterns, and model comparison. Each plot serves a specific diagnostic purpose, helping us assess whether our models satisfy key econometric assumptions and providing insights into potential model improvements.\n",
    "\n",
    "\n",
    "\n",
    "**Plot C: Time Series of Log BNPL Returns** (Top-Left) displays the dependent variable over time, showing the temporal patterns and volatility that our models seek to explain. This plot helps identify periods of extreme returns, potential outliers, and temporal trends that may inform our understanding of BNPL stock performance.\n",
    "\n",
    "\n",
    "\n",
    "**Plot D: Scatter Plot of Log BNPL Returns vs Interest Rate Changes** (Top-Middle) visualizes the relationship between interest rates and BNPL returns using the **full specification model (best model)**. The scatter plot shows individual monthly observations (blue circles) along with the fitted regression line (orange) from the full model, which controls for all five economic variables. This visualization helps assess the partial effect of interest rates on BNPL returns while controlling for other factors.\n",
    "\n",
    "\n",
    "\n",
    "**Plot E: Residuals Plot for Full Model** (Top-Right) plots the residuals (observed minus fitted values) against fitted values for the **full specification model (best model)**. This diagnostic plot helps assess whether the full model satisfies the homoskedasticity assumption\u2014if residuals are randomly scattered around zero with constant variance, the assumption is satisfied. Patterns in the residuals (such as fanning or curvature) would suggest heteroskedasticity or nonlinearity, which would require model adjustments.\n",
    "\n",
    "\n",
    "\n",
    "**Plot F: Residuals Plot Comparison** (Bottom-Left) shows residuals from the base model for comparison purposes, allowing us to visually assess the improvement in model fit achieved by including control variables. A more random scatter pattern in the full model (Plot E) compared to the base model would suggest that the additional variables help capture systematic patterns that were causing heteroskedasticity in the base model.\n",
    "\n",
    "\n",
    "\n",
    "**Plot G: Q-Q Plot for Full Model** (Bottom-Middle) assesses whether the residuals from the **full specification model (best model)** are normally distributed, which is an assumption underlying many statistical tests. The Q-Q plot compares the quantiles of the residuals to the quantiles of a normal distribution\u2014if residuals are normally distributed, the points should fall approximately along a straight line. Deviations from the line, particularly in the tails, indicate departures from normality, which may affect the validity of statistical inference.\n",
    "\n",
    "\n",
    "\n",
    "**Plot H: Model Comparison: R\u00b2 Values** (Bottom-Right) provides a visual comparison of model fit between the base and full specification models. The bar chart displays both R\u00b2 and adjusted R\u00b2 for each model, allowing us to visually assess the substantial improvement in explanatory power achieved by including control variables. This comparison helps quantify the value of the multi-factor approach relative to the simple interest rate model.![Model Diagnostics Dashboard (Plots C-H)](plot_1_6_diagnostics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "causality_explanation",
   "metadata": {},
   "source": [
    "## 3.7 Alternative Analytical Approaches: Robustness Checks\n",
    "\n",
    "This section discusses alternative analytical approaches that could provide additional insights beyond the baseline OLS regression. While the main regression analysis provides valuable descriptive evidence, these alternative approaches address different methodological concerns, including omitted variable bias, reverse correlation, and endogeneity concerns. The three strategies presented here each address different aspects of these challenges, providing complementary evidence on the association between interest rate changes and BNPL stock returns. However, none of these approaches provide associational identification, and results should be interpreted as correlations rather than associational effects.\n",
    "\n",
    "### Strategy 1: Difference-in-Differences (DiD)\n",
    "\n",
    "**The DiD approach compares BNPL firms to fintech lenders as a control group. We select three publicly traded fintech lenders: SoFi Technologies (SOFI), Upstart Holdings (UPST), and LendingClub Corporation (LC). These firms are selected based on the following criteria: (1) US publicly traded companies on major exchanges (NYSE/NASDAQ), (2) tech-enabled consumer credit firms operating in consumer lending markets, (3) sufficient trading history covering our sample period (February 2020 to August 2025), (4) different business models from BNPL (personal loans versus point-of-sale installment loans), and (5) comparable exposure to macroeconomic conditions. SoFi is a digital financial services company offering personal loans and student loan refinancing, publicly traded on NASDAQ since June 2021. Upstart is an AI-powered lending platform partnering with banks to provide personal loans, publicly traded on NASDAQ since December 2020. LendingClub is a peer-to-peer lending platform facilitating personal loans, publicly traded on NYSE since December 2014. All three firms have sufficient trading history for our analysis period and are publicly available for data collection.\n",
    "\n",
    "Fintech lenders serve as a comparison group because they operate in similar markets (tech-enabled consumer credit) and face similar macroeconomic conditions, but differ in their funding structures and business models. By comparing how BNPL firms respond to interest rate changes relative to fintech lenders, we can isolate BNPL-specific sensitivity. We estimate the DiD model using the full specification that matches our main regression analysis, incorporating control variables to address confounding factors:\n",
    "\n",
    "$$\\log(Return_{it}) = \\beta_0 + \\beta_1(BNPL_i) + \\beta_2(\\Delta FFR_t) + \\beta_3(BNPL_i \\times \\Delta FFR_t) + \\beta_4(R_{Market,t}) + \\beta_5(\\Delta CC_t) + \\beta_6(\\Delta DI_t) + \\beta_7(\\Delta \\pi_t) + \\varepsilon_{it}$$\n",
    "\n",
    "where $BNPL_i$ is a dummy variable equal to 1 for BNPL firms and 0 for fintech lenders, $R_{Market,t}$ represents market returns, $\\Delta CC_t$ denotes changes in consumer confidence, $\\Delta DI_t$ represents changes in disposable income, and $\\Delta \\pi_t$ denotes changes in inflation. The coefficient $\\beta_3$ captures the differential sensitivity of BNPL firms to interest rate changes, relative to fintech lenders, after controlling for market movements and other macroeconomic factors. This approach addresses omitted variable bias by using fintech lenders as a control group that experiences similar macroeconomic shocks but has different structural characteristics, while also controlling for confounding factors that may affect both BNPL and fintech lender returns simultaneously.\n",
    "\n",
    "To assess the robustness of our DiD estimates, we compare the full model estimated on the complete sample with the same full model estimated excluding the COVID-19 period (February-June 2020). This robustness check tests whether our results are sensitive to the inclusion of this unusual period characterized by extreme market volatility. If the coefficient estimates remain stable across these different samples, this provides evidence that our findings are robust and not driven by the specific conditions of the pandemic period.\n",
    "\n",
    "### Strategy 2: Panel Data with Firm Fixed Effects\n",
    "\n",
    "**The panel data approach uses individual firm returns (PYPL, AFRM, SEZL) instead of portfolio averages, allowing us to control for unobserved firm-specific factors through firm fixed effects. This addresses omitted variable bias arising from time-invariant firm characteristics (such as business model, management quality, or regulatory environment) that may affect both interest rate sensitivity and stock returns. The panel specification takes the form:\n",
    "\n",
    "$$\\log(Return_{it}) = \\alpha_i + \\beta_1(\\Delta FFR_t) + \\beta_2(Controls_t) + \\varepsilon_{it}$$\n",
    "\n",
    "where $\\alpha_i$ represents firm fixed effects that capture all time-invariant firm characteristics. This approach provides more precise estimates by exploiting within-firm variation over time, while controlling for unobserved heterogeneity across firms.\n",
    "\n",
    "### Strategy 3: Instrumental Variables (IV)\n",
    "\n",
    "**The IV approach uses lagged Federal Funds Rate changes as instruments for current rate changes. This addresses endogeneity concerns arising from reverse associationality (where BNPL stock performance may affect monetary policy) or simultaneity (where both interest rates and BNPL returns respond to common unobserved factors). The IV strategy requires two conditions: (1) relevance, meaning lagged rates predict current rate changes (tested via first-stage F-statistic), and (2) exogeneity, meaning lagged rates affect BNPL returns only through their effect on current rates. The IV specification uses a two-stage approach:\n",
    "\n",
    "**First Stage:** $\\Delta FFR_t = \\gamma_0 + \\gamma_1(\\Delta FFR_{t-1}) + \\gamma_2(\\Delta FFR_{t-2}) + u_t$\n",
    "\n",
    "**Second Stage:** $\\log(BNPL\\_Return_t) = \\beta_0 + \\beta_1(\\Delta FFR_t^{predicted}) + \\varepsilon_t$ \n",
    "\n",
    "where $\\Delta FFR_t^{predicted}$ is the predicted value from the first stage. The IV coefficient $\\beta_1$ provides a associational estimate under the assumption that lagged rates are exogenous to current BNPL returns. Comparing IV estimates to OLS estimates provides a test for endogeneity: if they differ substantially, it suggests that OLS estimates are biased.\n",
    "\n",
    "### Interpretation and Limitations\n",
    "\n",
    "Each identification strategy has strengths and limitations. The DiD approach provides clean identification of BNPL-specific effects but requires the assumption that fintech lenders and BNPL firms respond similarly to unobserved factors (parallel trends assumption). The panel data approach controls for firm heterogeneity but may not address time-varying omitted variables. The IV approach addresses endogeneity but requires valid instruments and may suffer from weak instrument problems if lagged rates are poor predictors of current rates.\n",
    "\n",
    "### Empirical Results from Alternative Identification Strategies\n",
    "\n",
    "The DiD analysis reveals a negative coefficient on the BNPL-specific interest rate sensitivity term (\u03b2\u2083 \u2248 -8.35), indicating that BNPL firms respond more negatively to interest rate increases than fintech lenders, even after controlling for market returns, consumer confidence, disposable income, and inflation. While this coefficient is not statistically significant at conventional levels (p-value \u2248 0.51), the negative sign and magnitude are consistent with theoretical expectations regarding BNPL firms' greater sensitivity to funding cost changes. The DiD model achieves an R\u00b2 of approximately 0.38, indicating that the included variables explain about 38% of the variation in returns across BNPL and fintech lender firms. However, the robustness check reveals that the DiD coefficient is sensitive to the inclusion of the COVID-19 period, changing from -8.35 to +6.12 when excluding February-June 2020. This sensitivity suggests that the DiD estimate may be driven by unusual conditions during the pandemic period rather than general patterns. Market returns are highly significant in the DiD model (coefficient \u2248 2.16, p < 0.001), confirming that both BNPL and fintech lenders respond strongly to broader market movements.\n",
    "\n",
    "The IV analysis yields a substantially larger and statistically significant coefficient (\u03b2\u2081 \u2248 -37.07, p-value \u2248 0.002) compared to the OLS estimates (\u03b2\u2081 \u2248 -12.51 to -12.68). This threefold difference suggests that OLS may underestimate the true associational effect, potentially due to attenuation bias from measurement error or endogeneity concerns. The IV first-stage F-statistic of approximately 55.1 indicates a strong instrument, satisfying the relevance condition. The statistical significance of the IV estimate (p = 0.002) provides evidence of a association between interest rate changes and BNPL returns under the assumption that lagged rates are exogenous. However, the IV model achieves a lower R\u00b2 of approximately 0.093 (9.3%) compared to the OLS full model's R\u00b2 of 0.5098 (51%), reflecting the fact that the IV specification includes only the interest rate variable without the full set of control variables. The fact that the IV estimate is larger in magnitude than OLS suggests that OLS may be biased toward zero, possibly due to measurement error in interest rate changes or other endogeneity concerns.\n",
    "\n",
    "### Comparison of Model Approaches\n",
    "\n",
    "Each identification strategy answers a different question and has distinct strengths and limitations. The OLS full model provides the highest explanatory power (R\u00b2 = 0.5098) and includes comprehensive controls, but the interest rate coefficient is not statistically significant (p = 0.202). The DiD approach isolates BNPL-specific sensitivity relative to fintech lenders but shows limited statistical precision and sensitivity to sample period. The IV approach provides statistically significant evidence of a association but achieves lower explanatory power and uses a simpler specification without the full set of controls. Rather than declaring one approach \"better\" than another, these strategies provide complementary evidence: the OLS model provides the most comprehensive framework for understanding BNPL returns, the DiD approach provides evidence on BNPL-specific effects relative to similar firms, and the IV approach provides the strongest evidence for a association under its identifying assumptions. The divergence between estimates (particularly IV vs OLS) suggests that different identification assumptions may be violated, requiring careful interpretation of which approach provides the most credible estimates for the specific research question at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rebuilt_empirical_results",
   "metadata": {},
   "source": [
    "## 4. Empirical Results\n",
    "\n",
    "**Primary Finding:** The main empirical finding of this analysis is that **we cannot detect a statistically significant relationship** between Federal Funds Rate changes and BNPL stock returns. The interest rate coefficient is not statistically significant at any conventional level (p-value = 0.202), meaning we cannot reject the null hypothesis of no relationship. This null result is itself an important finding: despite theoretical predictions and firm-level evidence suggesting BNPL firms should be sensitive to interest rate changes, we find no statistically significant evidence of this relationship in monthly stock return data after controlling for market movements and macroeconomic factors.\n",
    "\n",
    "This section presents the econometric analysis of BNPL stock returns' sensitivity to monetary policy changes and macroeconomic factors. The analysis employs a multi-factor regression framework with comprehensive diagnostic testing, robustness checks, and factor-adjusted specifications to provide rigorous empirical evidence on the determinants of BNPL stock performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}